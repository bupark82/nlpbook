{
 "cells": [
  {
   "cell_type": "raw",
   "id": "literary-debate",
   "metadata": {},
   "source": [
    "[[ch11]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regulated-rally",
   "metadata": {},
   "source": [
    "# Productionization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "80a1c152",
   "metadata": {},
   "source": [
    "The difficulty of the move from prototyping to production is where many companies fail is one of the main reasons many companies derive such a low return on investment on machine learning initiatives they launch. In the previous chapter, we discussed how to productionize machine learning as a web app. However, primary way for companies to productionize machine learning and truly unlock the value of these models in a production setting is not via a simple wep app; it is via APIs and automated pipelines, both of which we will cover in this chapter. We will also discuss the various roles that are involved in deploying, maintaining, and monitoring machine learning models in production, and explore Databricks, one of the current market-leading platforms to perform data science and machine learning work in the enterprise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developing-evolution",
   "metadata": {},
   "source": [
    "## Data Scientists, Engineers, and Analysts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ca077d0b",
   "metadata": {},
   "source": [
    "Before we dive into how to productionize machine learning models, let's review the different individuals who will be invovled during the entire machine learning development and deployment cycle. Understanding the roles of these individuals and their preferences for programming language and programming environment is important because we want to reduce the friction in moving from prototyping models to deploying them in production; in other words, we need to consider ease of collaboration to ensure success in running machine learning in production. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "republican-young",
   "metadata": {},
   "source": [
    "### Prototyping, Deployment, and Maintenance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e619c28a",
   "metadata": {},
   "source": [
    "There are three distinct technical stages in the machine learning cycle: prototyping, deployment, and ongoing monitoring and maintenance. In the ptototyping stage, data scientists take into consideration the objectives of the business (unsually informed by a product manager) and prepare data, perform feature engineering, choose algorithms to test, define cost fuctions, train and evaluate multiple models, and select the best-performing models as the winner-all of which you are very familiar with.\n",
    "\n",
    "During this prototyping stage, data engineers may help with some of the extraction, transform, and load (ETL) work required to consolidate the data from multiple sources into one centralized location and make it available to data scientists for machine learning develoment. Data analysts may perform data exploration and preparation to assit the data scientists and may help evaluate the results of the machine learning models. But, data scientists are largely the primary players during this model development phase, while engineers and analysts perform support roles.\n",
    "\n",
    "During the model deployment stage, data and machine learning engineers become the primary players, supported by the data scientists who developed the machine learning models during the prototyping stage. The engineers typically refactor the code developed by the data scientists so that the model is performant (i.e, can scale to large datasets) and robust (i.e, can handle errors and edge cases gracefully). The engineers also need to position the model in the company's software architectures so the model does what it needs to in the larger workflow. These data and ML engineers are the crucial players in getting models from prototype to production.\n",
    "\n",
    "Data scientists support the engineers byh pair programming, working alongside each other as the engineers write a more performant, robust version of the data scientists's original code. The data scientist explains how the model works and answers any ohter quesitons the engineers may have.\n",
    "\n",
    "During model deployment, data analysts have a very limited role to play. However, once the model has been deployed, data analysts take on the primary role of interpreting the model's outputs and interfacing with the nontechnical consumers of the model, both internally in the orgaization an potentianlly externally with clients.\n",
    "\n",
    "The data and ML engineers and analysts are also the first line of defense in case the model behaves poorly. Data and ML engineers will monitor the model to insure it has near-100% uptime, is scaling well to large volumnes of data, and is generating successful responses instead of errors. Data analysts also help identify when there are errors in the model's outputs and flag when the model's performance deteriorates, which will happen over time as new data flows through the model that perhaps is not well-represented by the training data used to develop the model.\n",
    "\n",
    "If the issues with the model are engineering-related, the engineers will resolve the situation themselves. But, if the inssues are model-related, the engineers and analysts will engage the data scientists to dig deeper into why the model is performing poorly. One common resolution is model retraining: the data scientists will have to periodically retain the model on new data that is representative of the data the model is performing inference on in production. \n",
    "\n",
    "Once the data scientists finish retraining the model, the data and ML engineers will deploy it in production, and the data analysts will interpret the results and confirm that the model is indeed performing better. And, the cycle goes on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extraordinary-father",
   "metadata": {},
   "source": [
    "### Notebooks and Scripts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7d6d80e6",
   "metadata": {},
   "source": [
    "A common point of contention is using Jupyter Notebooks for production work. As a consequence of history, notebooks have gotten a bad rep for encouraging poor software engineering practices and nonreporoducible code. For the most part, a lot of these concerns have now been mitigated, but it's still important to think about where it might be useful to use notebooks versus scripts.\n",
    "\n",
    "During the prototyping stage, most data scientists develop models in notebook-based environment, such as Jupyter Notebook, JupyterLab, IPython, Google Colab, and even a VS Code extension. Notebook-based environments are great for prototyping and experimentation. They really do bring us close to the \"code at the speed of thought\" adage.\n",
    "\n",
    "However, your \"thoughts\" may not necessarily have unit tests or PEP compliance, and they generally don't need to work with a distributed version control system with frequnt force pushes to main...\n",
    "\n",
    "So after the data scientists are finished developing the model in these notebook-based environments, engineers typically refactor the code into scripts, writing functions and organizing them into classes to make it easier to reproduce results, experiment at sacle, and debug in production.\n",
    "\n",
    "Data scientists prefer notebooks because it allows them to experiment and iterate quickly. Engineers prefer scripts because they work with the broader set of tools rquired deployment. Once the models are in production, the data and ML engineers still need to monitor the models for performance and robustness, and the data scientists still need to maintain the quality of the model by occasionally retraining it, so the desire to go back and forth between notebooks and scripts still exists.\n",
    "\n",
    "Data analysts, who typically interface more with business functions and are less involved in developing and deploying machine learning models, will need to engage with the model's outputs and interpret the results. The needs of these data analysts will also need to be accommondated.\n",
    "\n",
    "Companies that want to do machine learning in production need to be aware of these various roles-data scientists, data and ML engineers, and data analysts-and how to establish a unified platform for work across all three functions. Note that different organzaitons may refer to these roles by different names. There is no standardized convertion, and many times a single person may wear many hats. But at the end of the day, the crucial tasks can be split into building new models, intergrating them into a production pipeline, and analyzing/interpreting results to produce valuable insights to the organization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "endangered-origin",
   "metadata": {},
   "source": [
    "## Databricks: Your Unified Data Analytics Platform"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aa7f38fe",
   "metadata": {},
   "source": [
    "Fortunately there is a platform today that accommondates these varing needs and makes it easier to push machine learning models into production: Databricks. Databricks is the industry leader for collaborative data science and machine learning work for large-scale production use cases today. It accomondates data scientists, data and ML engineers, and data analysts and supports the entire ML life cycle from model development to testing and deployment to monitoring and maintenance.\n",
    "\n",
    "Let's explore Databricks and use it to deploy one of the machine learning models we developed earlier in the book.\n",
    "\n",
    "> While there are alternatives to Databricks, such as Amazon SageMaker and Saturn Cloud (discussed later in the chapter), Databricks is by far the market leader today for several reasons:\n",
    "\n",
    "> - It has been around the longest, which means that many practitioners are already familiar with the technology.\n",
    ">- It currently has the most mature offering for organizations, including the security and compliance features most companies will need.\n",
    "> - It is bauilt for big data and continues to innovate on this front; in fact, the creators of Databricks were the original creators of the most popular big data processing framework today(Spark, which we'll cover in the next section)\n",
    "\n",
    "> We expect the landscape to become more competitive in the coming years, so spend time exploring alternatives to Databricks, too, as you advance in your career."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floppy-inflation",
   "metadata": {},
   "source": [
    "### Support for Big Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4f41080a",
   "metadata": {},
   "source": [
    "When developing machine learning models, many data scientists do not fully consider how to make them performant on very large datasets or how to handle streaming use cases (e.g., where data is flowing through the model for real-time interence). Data scientists typically work on small to medium-size datasets that are prepared for model training. But, once in production, these models may need to perform interence quickly on orders of magnitude larger datasets.\n",
    "\n",
    "To perform machine learning at scale, it is impracical to use a single machine to perform inference. Rather, many machines are required; these machines, when linked together for a task, are often referred to as a cluster of machines.\n",
    "\n",
    "This is also where big data technologies such as Hadoop and Spark come into play. Hadoop is the original big data technology, starting out as a Yahoo project in 2006. Hadoop allows users to perform data operations in parallel across many machines in a cluster and string them back together as an output; this mechanism to farm out data operations and generate results is known as MapReduce. For example, if a user wanted to add a scalar to every element in a 10-billion row dataset, the user could use a 10-machine cluster to perform the task nearly 10 times faster than if the task had been performed on a single machine.\n",
    "\n",
    "Spark, the newer big data technology, was developed in 2012 at the AMPLab at UC Berkeley. It is simliar to Hadoop in many ways; Spark processes data in parallel across a cluster, just like Hadoop. However, Spark performs these data operations in memory(also known as RAM), where Hadoop reads and writes files to its filesystem format(Hadoop Distributed File System, or HDFS), which is on disk.\n",
    "\n",
    "While Hadoop is still popular, Spark is the clear winner and rising star in big data. Spark can run 100 times faster in-memory and 10 times faster on disk compared to Hadoop. Spark is also much faster on machine learning applications and supports abractions that are preferred by data scientists(Spark DataFrames, which are similar to Python's pandas and R packages) and by data analysts (Spark SQL, which is similar to SQL tables in relation data stores).\n",
    "\n",
    "Databricks is built on Spark, and one of Databricks' cofounders, Matei Zaharia, is also the founder of Spark. Although Spark is an open source framesork and supported by the Apache Software Foundation, the Databricks version of Spark is a commercially focursed and futher optimized instance of Spark and yet anohter reason Databricks is the preferred platform for data and machine learning work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extensive-mineral",
   "metadata": {},
   "source": [
    "### Support For Multiple Programming Languages"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0bfba2e6",
   "metadata": {},
   "source": [
    "One of the major challenges for data scientists, engineers, and analysts is the disconnet in programming languages. Data scientists typically code in Python (and, R, although decreasingly so), data and machine learning engineers prefer Scala or PySpark (both of which leverage Spark), and data analysts prefer SQL.\n",
    "\n",
    "Databricks makes it easy to transition from one programming language to another(from Python to Scala to SQL) and supports a notebook-based environment (which data scientists typically prefer) as well as scripts, programmatic access, and APIs (which engineers will need to deploy, monitor, and maintain systems in production efficiently).\n",
    "\n",
    "The beauty of this multilanguage approach is that data scientists, engineers, and analysts can all work on a single platform. Once the models are developed, Databricks makes it easy to deploy these models fast; no need to switch from one platform to another to take a model from prototype to production. Databricks dissolves the organizational and technological silos taht data scientists, egineers, and analystst traditionally encounter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enhanced-association",
   "metadata": {},
   "source": [
    "### Support For ML Frameworks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "72541403",
   "metadata": {},
   "source": [
    "Figure 11-1 shows how Databricks describes the challenges and their solutions. The challenges include the need to support multiple programming languages, multiple machine learning frameworks, and multiple DevOps tools as well as the need to reduce friction among teams that are preparing data, building models, and moving the models into production. And, security and compliance are crucial considerations for any mature organization.\n",
    "\n",
    "Figure 11-1. Databricks platform (courtesy of Databricks(https://oreil.ly/iX7SW))\n",
    "\n",
    "Databricks's solution to these challenges in a ready-to-use, optimized, and scalable machine learning environment with built-in support for the most common data science programming languages and many of the most popular machine learning frameworks, such as PyTorch, TensorFlow, and Scikit-learn. This environment is available for use with just a few clicks (after some initial setup), allowing data scientists to quickly spin up clusters of machines to perform data science and develop machine learning models and then spin the clusters down once the work is done. These environments are also customizable, allowing data science and ML teams to import libraries and run init scripts, as necessary.\n",
    "\n",
    "Spark also comes with its own Spark-optimzied machine learning library, MLlib, which is available for use on Databricks. MLlib supports the more common machine learning algorithms such as calssification, rgression, clustering, and collaborative filtering, and allows data scientists to develop more performant machine learning pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stable-paint",
   "metadata": {},
   "source": [
    "### Support for Model Repository, Access Control, Data Lineage, and Versioning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bf045aed",
   "metadata": {},
   "source": [
    "Finally, Databricks provides the ability to track experiments, register and version machine learning models, grant access and permissions based on roles, and track how data flows through the data pipeline. Databricks comes with many of the compliance features larger enterprises will need as they scale their machine learning operations.\n",
    "\n",
    "To sum up, see Figure 11-2, which shows the ML life cycle on Databricks. This figure shows how data from multiple data sources (e.g, flat files, big data, and streaming data) flows into Databricks, which has multiple layers to support the work required (everything from collaborative workspaces, AutoML features, feature engineering stores, experiment tracking, model registry, model deployment options, and security and compliance support). The work done on Databricks powers both analytical use cases such as reports and dashboards as well as operational AI via APIs, batch scoring, and edge device deployment.\n",
    "\n",
    "Figure 11-2. Machine learning life cycle (courtesy of Databricks (https://oreil.ly/iMuq7).)\n",
    "\n",
    "Databricks provides a unitied platform for data scientists, engineers, and analysts to develop and deploy machine learning models at scale, work in multiple programming languages, acess a variety of ML framworks an libraries with a few clicks, and collbarorate on every stage from model prototype to production."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "earned-premium",
   "metadata": {},
   "source": [
    "## Databricks Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spectacular-democrat",
   "metadata": {},
   "source": [
    "### Set Up Access to S3 Bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hollow-howard",
   "metadata": {},
   "source": [
    "### Set Up Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "separated-oasis",
   "metadata": {},
   "source": [
    "### Create Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sorted-ordering",
   "metadata": {},
   "source": [
    "### Create Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "psychological-racing",
   "metadata": {},
   "source": [
    "```python\n",
    "# Make directory in DBFS\n",
    "dbutils.fs.mkdirs(\"dbfs:/databricks/models/spacy\")\n",
    "\n",
    "# Copy files from S3 to DBFS\n",
    "dbutils.fs.cp(\"s3a://nlp-demo/models/spacy/\",\n",
    "              \"dbfs:/databricks/models/spacy/\", True)\n",
    "\n",
    "# Confirm files in DBFS\n",
    "display(dbutils.fs.ls(\"dbfs:/databricks/models/spacy/\"))\n",
    "\n",
    "# Make directory in DBFS\n",
    "dbutils.fs.mkdirs(\"dbfs:/databricks/scripts/\")\n",
    "\n",
    "# Put script in DBFS\n",
    "dbutils.fs.put(\"dbfs:/databricks/scripts/spacy_with_models.sh\", \\\n",
    "\"\"\"pip install /dbfs/databricks/models/spacy/en_core_web_lg-2.3.1.tar.gz \\\n",
    "pip install /dbfs/databricks/models/spacy/en_ner_base_V3-0.0.0.tar.gz \\\n",
    "pip install /dbfs/databricks/models/spacy/\\\n",
    "en_textcat_prodigy_V3_base_full-0.0.0.tar.gz\"\"\", True)\n",
    "\n",
    "# Confirm file in DBFS\n",
    "display(dbutils.fs.ls(\"dbfs:/databricks/scripts/spacy_with_models.sh\"))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unnecessary-encoding",
   "metadata": {},
   "source": [
    "### Enable Init Script and Restart Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floating-christian",
   "metadata": {},
   "source": [
    "### Run Speed Test - Inference on NER using SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "objective-doctor",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "# Python\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# PySpark\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "associate-antarctica",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "inputPath = \"s3a://nlp-demo/ag_dataset/prepared/train_prepared.csv\" \\\n",
    " # path to your S3 bucket\n",
    "df = spark.read.format('csv').options(header='true', inferSchema='true', \\\n",
    " quote=\"\\\"\", escape= \"\\\"\").load(inputPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finnish-emission",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Cache\n",
    "df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "standard-knock",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# View shape of data\n",
    "print((df.count(), len(df.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "formed-stanley",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Define schema\n",
    "schema = ArrayType(StructType([\n",
    "    StructField(\"text\", StringType(), False),\n",
    "    StructField(\"start_char\", IntegerType(), False),\n",
    "    StructField(\"end_char\", IntegerType(), False),\n",
    "    StructField(\"label\", StringType(), False)\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorporate-independence",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Define Function to Get Entities\n",
    "def get_entities(text):\n",
    "    global nlp\n",
    "    try:\n",
    "        doc = nlp(str(text))\n",
    "    except:\n",
    "        nlp = spacy.load('en_core_web_lg')\n",
    "        doc = nlp(str(text))\n",
    "    return [[e.text, e.start_char, e.end_char, e.label_] for e in doc.ents]\n",
    "\n",
    "get_entities_udf = udf(get_entities, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "referenced-afternoon",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Get Entities\n",
    "documents_df = df.withColumn('entities', get_entities_udf('description'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improved-pathology",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Write parquet\n",
    "documents_df.write.parquet(\\\n",
    " \"s3a://nlp-demo/ag_dataset/prepared/write_test.parquet\", \\\n",
    " mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dried-balance",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "'''Main Libraries'''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Connect to Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "write_path = '/content/drive/My Drive/Applied-NLP-in-the-Enterprise'\n",
    "\n",
    "# Install SpaCy\n",
    "!pip install -U spacy\n",
    "!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "super-evening",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    " \n",
    "# Start timer\n",
    "start_time = time.time()\n",
    " \n",
    "# Define function to read data\n",
    "def read_data(file):\n",
    "    read_path = '/content/drive/My Drive/Applied-NLP-in-the-Enterprise'\n",
    "    data = pd.read_csv(read_path+file)\n",
    "    return data\n",
    " \n",
    "# Read data\n",
    "data = read_data('/data/ag_dataset/prepared/train_prepared.csv')\n",
    " \n",
    "# Load model\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    " \n",
    "# Load time\n",
    "load_time = time.time()\n",
    "print(\"Time to load data and model: \", np.round(load_time-start_time,2))\n",
    " \n",
    "# Apply NLP model\n",
    "data[\"entities\"] = data[\"description\"].apply(lambda x: \\\n",
    " [(e.text, e.start_char, e.end_char, e.label_) for e in nlp(x).ents])\n",
    " \n",
    "# End timer\n",
    "end_time = time.time()\n",
    "print(\"Time to perform NER: \", np.round(end_time-load_time,2))\n",
    "print(\"Total time: \", np.round(time.time()-start_time,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "associate-blogger",
   "metadata": {},
   "source": [
    "## Machine Learning Jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polish-object",
   "metadata": {},
   "source": [
    "### Production Pipeline Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extensive-patch",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Load Libraries\n",
    "# Python\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# PySpark\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType, ArrayType\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "honest-surface",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Load Data\n",
    "inputPath = getArgument(\"inputPath\", \"default\")\n",
    "df = spark.read.format('csv').options(header='true', inferSchema='true', \\\n",
    " quote=\"\\\"\", escape= \"\\\"\").load(inputPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specific-rebecca",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Define Schema\n",
    "schema = ArrayType(StructType([\n",
    "    StructField(\"text\", StringType(), False),\n",
    "    StructField(\"start_char\", IntegerType(), False),\n",
    "    StructField(\"end_char\", IntegerType(), False),\n",
    "    StructField(\"label\", StringType(), False)\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radical-advocate",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Define Function to Get Entities\n",
    "def get_entities(text):\n",
    "    global nlp\n",
    "    try:\n",
    "        doc = nlp(text)\n",
    "    except:\n",
    "        nlp = spacy.load('en_ner_base_V3')\n",
    "        doc = nlp(text)\n",
    "    return [[e.text, e.start_char, e.end_char, e.label_] for e in doc.ents]\n",
    "\n",
    "get_entities_udf = udf(lambda x: get_entities(x), schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "failing-cable",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Get Entities\n",
    "documents_df = df.withColumn('entities', get_entities_udf('description'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wired-macedonia",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Write Parquet\n",
    "outPath = getArgument(\"outputPath\", \"default\")\n",
    "documents_df.write.format(\"parquet\").mode(\"overwrite\").save(outPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collect-linux",
   "metadata": {},
   "source": [
    "### Scheduled Machine Learning Jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sunset-railway",
   "metadata": {},
   "source": [
    "### Event-Driven Machine Learning Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "postal-newcastle",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "const https = require(\"https\");\n",
    "\n",
    "exports.handler = (event, context, callback) => {\n",
    "  var data = JSON.stringify({\n",
    "    \"job_id\": XXX\n",
    "  });\n",
    "\n",
    "  var options = {\n",
    "     host: \"XXX-XXXXXXX-XXX.cloud.databricks.com\",\n",
    "     port: 443,\n",
    "     path: \"/api/2.0/jobs/run-now\",\n",
    "     method: \"POST\",\n",
    "     // authentication headers\n",
    "     headers: {\n",
    "      \"Authorization\": \"Bearer XXXXXXXXXXXXXXXXXXXXXXXXXXXXX\",\n",
    "      \"Content-Type\": \"application/json\",\n",
    "      \"Content-Length\": Buffer.byteLength(data)\n",
    "     }\n",
    "  };\n",
    "\n",
    "  var request = https.request(options, function(res){\n",
    "    var body = \"\";\n",
    "\n",
    "    res.on(\"data\", function(data) {\n",
    "      body += data;\n",
    "    });\n",
    "\n",
    "    res.on(\"end\", function() {\n",
    "      console.log(body);\n",
    "    });\n",
    "\n",
    "    res.on(\"error\", function(e) {\n",
    "      console.log(\"Got error: \" + e.message);\n",
    "    });\n",
    "\n",
    "  });\n",
    "\n",
    "  request.write(data);\n",
    "  request.end();\n",
    "};\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wireless-pantyhose",
   "metadata": {},
   "source": [
    "## MLflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "creative-boring",
   "metadata": {},
   "source": [
    "### Log and Register Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agricultural-brunei",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Load Libraries\n",
    "# SpaCY\n",
    "import spacy \n",
    "\n",
    "# MLflow\n",
    "import mlflow\n",
    "import mlflow.spacy\n",
    "\n",
    "# Load model\n",
    "nlp = spacy.load(\"en_textcat_prodigy_V3_base_full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appointed-scholarship",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Print metadata\n",
    "nlp.meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "living-mobile",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# MLflow Tracking\n",
    "with mlflow.start_run(run_name='SpaCy-TextCat-Prodigy-V3-Base-Full'):\n",
    "    mlflow.set_tag('model_flavor', 'spacy')\n",
    "    mlflow.spacy.log_model(spacy_model=nlp, artifact_path='model')\n",
    "    mlflow.log_metric('textcat_score', 91.774875419)\n",
    "    my_run_id = mlflow.active_run().info.run_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "serious-costume",
   "metadata": {},
   "source": [
    "### MLflow Model Serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daily-press",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Connect to Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protective-repository",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    " \n",
    "# Define function to read data\n",
    "def read_data(file):\n",
    "    read_path = '/content/drive/My Drive/Applied-NLP-in-the-Enterprise'\n",
    "    data = pd.read_csv(read_path+file)\n",
    "    return data\n",
    " \n",
    "# Read data\n",
    "data = read_data('/data/ag_dataset/prepared/train_prepared.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indonesian-level",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Convert to JSON\n",
    "data.loc[:10,\"description\"].to_json(path_or_buf= \\\n",
    "        '/content/drive/My Drive/Applied-NLP-in-the-Enterprise/data/\\\n",
    "        ag_dataset/prepared/sample.json', orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endless-breathing",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Call the Model - CURL\n",
    "MODEL_VERSION_URI = XXXXXX #the model path\n",
    "DATABRICKS_TOKEN = XXXXXX #secret access token\n",
    "JSON_PATH = XXXXXX #path to the JSON we created earlier in Colab\n",
    " \n",
    "!curl -u token:$DATABRICKS_TOKEN -H \\\n",
    " \"Content-Type: application/json; format=pandas-records\" \\\n",
    " -d@$JSON_PATH $MODEL_VERSION_URI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interpreted-person",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Define Function to Call the Model in Python\n",
    "import requests\n",
    " \n",
    "def score_model(model_uri, databricks_token, data):\n",
    "    headers = {\n",
    "        \"Authorization\": 'Bearer '+ databricks_token,\n",
    "        \"Content-Type\": \"application/json; format=pandas-records\",\n",
    "      }\n",
    "    data_json = data if isinstance(data, list) else data.to_list()\n",
    "    response = requests.request(method='POST', headers=headers,\n",
    "        url=model_uri, json=data_json)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Request failed with status {response.status_code},\n",
    "            {response.text}\")\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hidden-straight",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Score the Model\n",
    "MODEL_VERSION_URI = XXXXXX # the model path\n",
    "DATABRICKS_TOKEN = XXXXXX # secret access token\n",
    " \n",
    "score_model(MODEL_VERSION_URI, DATABRICKS_TOKEN, data.loc[:10,\"description\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "therapeutic-throat",
   "metadata": {},
   "source": [
    "## Alternatives to Databricks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hourly-device",
   "metadata": {},
   "source": [
    "### Amazon Sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "knowing-client",
   "metadata": {},
   "source": [
    "### Saturn Cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "referenced-lawyer",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4ba3388e",
   "metadata": {},
   "source": [
    "In this chpater, we explored how to productionize machine learning models. First, we established the different roles that are involved in deploying, maintaining, and monitoring machine learning models in production. Data scientists, data and machine learning engineers, and data analysts are all involved in machine learning work, and their specific needs (such as programming environment and programming language of choice) need to be considered when choosing the appropriate platform for your organization to do machine learning.\n",
    "\n",
    "The platform that we recommend is Databricks, especially since it is built on top of Spark, which is the best distributed machine learning technology available today; it is the optimal choice when doing machine learning at scale. We used Databricks to create both scheduled and event-based machine learning pipelines, and we then used these pipelines to perform batch inference with our spaCy NER model. We also used MLflow on Databricks to deploy and serve our spaCy text classification model via a REST API, and we then tested the REST API using the browser, cURL, and Python. And, in the previous chapter, we used Streamlit to build and deploy multiple web apps using our spaCy models.\n",
    "\n",
    "This concludes our section of productionizing machine learning models; at this point we've corvered web apps, APIs, and scheduled and event-based batch pipelines. What we have not covered is how to perform machine learning on streaming data, which is beyond the scope of this book."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
