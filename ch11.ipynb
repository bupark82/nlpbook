{
 "cells": [
  {
   "cell_type": "raw",
   "id": "literary-debate",
   "metadata": {},
   "source": [
    "[[ch11]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regulated-rally",
   "metadata": {},
   "source": [
    "# Productionization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "80a1c152",
   "metadata": {},
   "source": [
    "시제품 제작에서 생산으로 이동하는 과정의 어려움은 많은 회사가 실패하는 이유는 많은 회사가 시작한 기계 학습 이니셔티브에 대해 낮은 투자 수익을 얻는 주된 이유 중 하나입니다. 이전 장에서는 기계 학습을 웹 앱으로 생산하는 방법에 대해 논의했습니다. 그러나 기업이 기계 학습을 생산하고 생산 환경에서 이러한 모델의 가치를 진정으로 활용하는 주요 방법은 단순한 웹 앱을 통하는 것이 아닙니다. API와 자동화된 파이프라인을 통해 이루어지며 이 장에서 둘 다 다룰 것입니다. 또한 프로덕션에서 기계 학습 모델을 배포, 유지 관리 및 모니터링하는 것과 관련된 다양한 역할에 대해 논의하고 기업에서 데이터 과학 및 기계 학습 작업을 수행하는 현재 시장을 선도하는 플랫폼 중 하나인 Databricks를 살펴봅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developing-evolution",
   "metadata": {},
   "source": [
    "## Data Scientists, Engineers, and Analysts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ca077d0b",
   "metadata": {},
   "source": [
    "기계 학습 모델을 생산하는 방법에 대해 알아보기 전에, 전체 기계 학습 개발 및 배포 주기 동안 참여하게 될 사람 다른 개인을 검토해 보겠습니다. 이러한 개인의 역할과 프로그래밍 언어 및 프로그래밍 환경에 대한 선호도를 이해하는 것은 프로토타입 모델에서 프로덕션에 배포하는 데 걸리는 마찰을 줄이고 싶기 때문에 중요합니다. 즉, 프로덕션에서 기계 학습을 성공적으로 실행하려면 협업의 용이성을 고려해야 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "republican-young",
   "metadata": {},
   "source": [
    "### Prototyping, Deployment, and Maintenance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e619c28a",
   "metadata": {},
   "source": [
    "기계 학습 주기에는 프로토타이핑, 배포, 지속적인 모니터링 및 유지 관리의 세 가지 기술 단계가 있습니다. ptototyping 단계에서 데이터 과학자는 비즈니스 목표(일반적으로 제품 관리자가 알려주지 않음)를 고려하고 데이터를 준비하고, 기능 엔지니어링을 수행하고, 테스트할 알고리즘을 선택하고, 비용 기능을 정의하고, 여러 모델을 훈련 및 평가하고, 최상의 모델을 선택합니다. 당신이 매우 친숙한 우승자로서 모델을 수행합니다.\n",
    "\n",
    "이 프로토타이핑 단계에서 데이터 엔지니어는 여러 소스의 데이터를 하나의 중앙 집중식 위치로 통합하고 기계 학습 개발을 위해 데이터 과학자가 사용할 수 있도록 하는 데 필요한 일부 ETL(추출, 변환 및 로드) 작업을 도울 수 있습니다. 데이터 분석가는 데이터 과학자를 지원하기 위해 데이터 탐색 및 준비를 수행하고 기계 학습 모델의 결과를 평가하는 데 도움을 줄 수 있습니다. 그러나 데이터 과학자는 이 모델 개발 단계에서 주로 주요 역할을 하는 반면 엔지니어와 분석가는 지원 역할을 수행합니다.\n",
    "\n",
    "모델 배포 단계에서 데이터 및 기계 학습 엔지니어는 프로토타입 단계에서 기계 학습 모델을 개발한 데이터 과학자의 지원을 받는 주요 플레이어가 됩니다. 엔지니어는 일반적으로 데이터 과학자가 개발한 코드를 리팩터링하여 모델의 성능(즉, 대규모 데이터 세트로 확장 가능)과 견고성(즉, 오류 및 엣지 케이스를 정상적으로 처리할 수 있음)을 높입니다. 엔지니어는 또한 회사의 소프트웨어 아키텍처에 모델을 배치하여 모델이 더 큰 워크플로에서 필요한 작업을 수행하도록 해야 합니다. 이러한 데이터 및 ML 엔지니어는 모델을 프로토타입에서 프로덕션으로 가져오는 데 중요한 역할을 합니다.\n",
    "\n",
    "데이터 과학자는 페어 프로그래밍을 통해 엔지니어를 지원하고, 엔지니어가 데이터 과학자의 원래 코드에 대해 보다 성능이 뛰어나고 강력한 버전을 작성하면서 서로 협력합니다. 데이터 과학자는 모델이 어떻게 작동하는지 설명하고 엔지니어가 가질 수 있는 다른 질문에 답합니다.\n",
    "\n",
    "모델 배포 중에 데이터 분석가의 역할은 매우 제한적입니다. 그러나 일단 모델이 배포되면 데이터 분석가는 모델의 출력을 해석하고 모델의 비기술적 소비자와 인터페이스하는 주요 역할을 맡게 됩니다. 이 두 가지 모두 조직 내에서 내부적으로는 잠재적으로 클라이언트와 외부적으로 이루어집니다.\n",
    "\n",
    "데이터 및 ML 엔지니어와 분석가는 모델이 제대로 작동하지 않는 경우 첫 번째 방어선이기도 합니다. 데이터 및 ML 엔지니어는 모델을 모니터링하여 가동 시간이 거의 100%이고 대용량 데이터로 잘 확장되며 오류 대신 성공적인 응답을 생성하는지 확인합니다. 데이터 분석가는 또한 모델의 출력에 오류가 있는 경우를 식별하고 모델의 성능이 저하될 때 플래그를 지정하는 데 도움을 줍니다. 이는 모델을 개발하는 데 사용된 교육 데이터에 의해 잘 표현되지 않을 수 있는 새로운 데이터가 모델을 통해 흐르면서 시간이 지남에 따라 발생합니다.\n",
    "\n",
    "모델의 문제가 엔지니어링과 관련된 경우 엔지니어가 직접 문제를 해결합니다. 그러나 문제가 모델과 관련된 경우 엔지니어와 분석가는 데이터 과학자를 참여시켜 모델이 제대로 작동하지 않는 이유를 더 깊이 파고들 것입니다. 일반적인 해결 방법 중 하나는 모델 재교육입니다. 데이터 과학자는 모델이 프로덕션에서 추론을 수행하는 데이터를 나타내는 새 데이터에 대한 모델을 주기적으로 유지해야 합니다.\n",
    "\n",
    "데이터 과학자가 모델 재교육을 마치면 데이터 및 ML 엔지니어는 프로덕션에 배포하고 데이터 분석가는 결과를 해석하고 모델이 실제로 더 나은 성능을 발휘하는지 확인합니다. 그리고, 주기는 계속됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extraordinary-father",
   "metadata": {},
   "source": [
    "### Notebooks and Scripts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7d6d80e6",
   "metadata": {},
   "source": [
    "A common point of contention is using Jupyter Notebooks for production work. As a consequence of history, notebooks have gotten a bad rep for encouraging poor software engineering practices and nonreporoducible code. For the most part, a lot of these concerns have now been mitigated, but it's still important to think about where it might be useful to use notebooks versus scripts.\n",
    "\n",
    "During the prototyping stage, most data scientists develop models in notebook-based environment, such as Jupyter Notebook, JupyterLab, IPython, Google Colab, and even a VS Code extension. Notebook-based environments are great for prototyping and experimentation. They really do bring us close to the \"code at the speed of thought\" adage.\n",
    "\n",
    "However, your \"thoughts\" may not necessarily have unit tests or PEP compliance, and they generally don't need to work with a distributed version control system with frequnt force pushes to main...\n",
    "\n",
    "So after the data scientists are finished developing the model in these notebook-based environments, engineers typically refactor the code into scripts, writing functions and organizing them into classes to make it easier to reproduce results, experiment at sacle, and debug in production.\n",
    "\n",
    "Data scientists prefer notebooks because it allows them to experiment and iterate quickly. Engineers prefer scripts because they work with the broader set of tools rquired deployment. Once the models are in production, the data and ML engineers still need to monitor the models for performance and robustness, and the data scientists still need to maintain the quality of the model by occasionally retraining it, so the desire to go back and forth between notebooks and scripts still exists.\n",
    "\n",
    "Data analysts, who typically interface more with business functions and are less involved in developing and deploying machine learning models, will need to engage with the model's outputs and interpret the results. The needs of these data analysts will also need to be accommondated.\n",
    "\n",
    "Companies that want to do machine learning in production need to be aware of these various roles-data scientists, data and ML engineers, and data analysts-and how to establish a unified platform for work across all three functions. Note that different organzaitons may refer to these roles by different names. There is no standardized convertion, and many times a single person may wear many hats. But at the end of the day, the crucial tasks can be split into building new models, intergrating them into a production pipeline, and analyzing/interpreting results to produce valuable insights to the organization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "endangered-origin",
   "metadata": {},
   "source": [
    "## Databricks: Your Unified Data Analytics Platform"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aa7f38fe",
   "metadata": {},
   "source": [
    "Fortunately there is a platform today that accommondates these varing needs and makes it easier to push machine learning models into production: Databricks. Databricks is the industry leader for collaborative data science and machine learning work for large-scale production use cases today. It accomondates data scientists, data and ML engineers, and data analysts and supports the entire ML life cycle from model development to testing and deployment to monitoring and maintenance.\n",
    "\n",
    "Let's explore Databricks and use it to deploy one of the machine learning models we developed earlier in the book.\n",
    "\n",
    "> While there are alternatives to Databricks, such as Amazon SageMaker and Saturn Cloud (discussed later in the chapter), Databricks is by far the market leader today for several reasons:\n",
    "\n",
    "> - It has been around the longest, which means that many practitioners are already familiar with the technology.\n",
    ">- It currently has the most mature offering for organizations, including the security and compliance features most companies will need.\n",
    "> - It is bauilt for big data and continues to innovate on this front; in fact, the creators of Databricks were the original creators of the most popular big data processing framework today(Spark, which we'll cover in the next section)\n",
    "\n",
    "> We expect the landscape to become more competitive in the coming years, so spend time exploring alternatives to Databricks, too, as you advance in your career."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floppy-inflation",
   "metadata": {},
   "source": [
    "### Support for Big Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4f41080a",
   "metadata": {},
   "source": [
    "When developing machine learning models, many data scientists do not fully consider how to make them performant on very large datasets or how to handle streaming use cases (e.g., where data is flowing through the model for real-time interence). Data scientists typically work on small to medium-size datasets that are prepared for model training. But, once in production, these models may need to perform interence quickly on orders of magnitude larger datasets.\n",
    "\n",
    "To perform machine learning at scale, it is impracical to use a single machine to perform inference. Rather, many machines are required; these machines, when linked together for a task, are often referred to as a cluster of machines.\n",
    "\n",
    "This is also where big data technologies such as Hadoop and Spark come into play. Hadoop is the original big data technology, starting out as a Yahoo project in 2006. Hadoop allows users to perform data operations in parallel across many machines in a cluster and string them back together as an output; this mechanism to farm out data operations and generate results is known as MapReduce. For example, if a user wanted to add a scalar to every element in a 10-billion row dataset, the user could use a 10-machine cluster to perform the task nearly 10 times faster than if the task had been performed on a single machine.\n",
    "\n",
    "Spark, the newer big data technology, was developed in 2012 at the AMPLab at UC Berkeley. It is simliar to Hadoop in many ways; Spark processes data in parallel across a cluster, just like Hadoop. However, Spark performs these data operations in memory(also known as RAM), where Hadoop reads and writes files to its filesystem format(Hadoop Distributed File System, or HDFS), which is on disk.\n",
    "\n",
    "While Hadoop is still popular, Spark is the clear winner and rising star in big data. Spark can run 100 times faster in-memory and 10 times faster on disk compared to Hadoop. Spark is also much faster on machine learning applications and supports abractions that are preferred by data scientists(Spark DataFrames, which are similar to Python's pandas and R packages) and by data analysts (Spark SQL, which is similar to SQL tables in relation data stores).\n",
    "\n",
    "Databricks is built on Spark, and one of Databricks' cofounders, Matei Zaharia, is also the founder of Spark. Although Spark is an open source framesork and supported by the Apache Software Foundation, the Databricks version of Spark is a commercially focursed and futher optimized instance of Spark and yet anohter reason Databricks is the preferred platform for data and machine learning work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extensive-mineral",
   "metadata": {},
   "source": [
    "### Support For Multiple Programming Languages"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0bfba2e6",
   "metadata": {},
   "source": [
    "One of the major challenges for data scientists, engineers, and analysts is the disconnet in programming languages. Data scientists typically code in Python (and, R, although decreasingly so), data and machine learning engineers prefer Scala or PySpark (both of which leverage Spark), and data analysts prefer SQL.\n",
    "\n",
    "Databricks makes it easy to transition from one programming language to another(from Python to Scala to SQL) and supports a notebook-based environment (which data scientists typically prefer) as well as scripts, programmatic access, and APIs (which engineers will need to deploy, monitor, and maintain systems in production efficiently).\n",
    "\n",
    "The beauty of this multilanguage approach is that data scientists, engineers, and analysts can all work on a single platform. Once the models are developed, Databricks makes it easy to deploy these models fast; no need to switch from one platform to another to take a model from prototype to production. Databricks dissolves the organizational and technological silos taht data scientists, egineers, and analystst traditionally encounter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enhanced-association",
   "metadata": {},
   "source": [
    "### Support For ML Frameworks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "72541403",
   "metadata": {},
   "source": [
    "Figure 11-1 shows how Databricks describes the challenges and their solutions. The challenges include the need to support multiple programming languages, multiple machine learning frameworks, and multiple DevOps tools as well as the need to reduce friction among teams that are preparing data, building models, and moving the models into production. And, security and compliance are crucial considerations for any mature organization.\n",
    "\n",
    "Figure 11-1. Databricks platform (courtesy of Databricks(https://oreil.ly/iX7SW))\n",
    "\n",
    "Databricks's solution to these challenges in a ready-to-use, optimized, and scalable machine learning environment with built-in support for the most common data science programming languages and many of the most popular machine learning frameworks, such as PyTorch, TensorFlow, and Scikit-learn. This environment is available for use with just a few clicks (after some initial setup), allowing data scientists to quickly spin up clusters of machines to perform data science and develop machine learning models and then spin the clusters down once the work is done. These environments are also customizable, allowing data science and ML teams to import libraries and run init scripts, as necessary.\n",
    "\n",
    "Spark also comes with its own Spark-optimzied machine learning library, MLlib, which is available for use on Databricks. MLlib supports the more common machine learning algorithms such as calssification, rgression, clustering, and collaborative filtering, and allows data scientists to develop more performant machine learning pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stable-paint",
   "metadata": {},
   "source": [
    "### Support for Model Repository, Access Control, Data Lineage, and Versioning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bf045aed",
   "metadata": {},
   "source": [
    "Finally, Databricks provides the ability to track experiments, register and version machine learning models, grant access and permissions based on roles, and track how data flows through the data pipeline. Databricks comes with many of the compliance features larger enterprises will need as they scale their machine learning operations.\n",
    "\n",
    "To sum up, see Figure 11-2, which shows the ML life cycle on Databricks. This figure shows how data from multiple data sources (e.g, flat files, big data, and streaming data) flows into Databricks, which has multiple layers to support the work required (everything from collaborative workspaces, AutoML features, feature engineering stores, experiment tracking, model registry, model deployment options, and security and compliance support). The work done on Databricks powers both analytical use cases such as reports and dashboards as well as operational AI via APIs, batch scoring, and edge device deployment.\n",
    "\n",
    "Figure 11-2. Machine learning life cycle (courtesy of Databricks (https://oreil.ly/iMuq7).)\n",
    "\n",
    "Databricks provides a unitied platform for data scientists, engineers, and analysts to develop and deploy machine learning models at scale, work in multiple programming languages, acess a variety of ML framworks an libraries with a few clicks, and collbarorate on every stage from model prototype to production."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "earned-premium",
   "metadata": {},
   "source": [
    "## Databricks Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spectacular-democrat",
   "metadata": {},
   "source": [
    "### Set Up Access to S3 Bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hollow-howard",
   "metadata": {},
   "source": [
    "### Set Up Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "separated-oasis",
   "metadata": {},
   "source": [
    "### Create Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sorted-ordering",
   "metadata": {},
   "source": [
    "### Create Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "psychological-racing",
   "metadata": {},
   "source": [
    "```python\n",
    "# Make directory in DBFS\n",
    "dbutils.fs.mkdirs(\"dbfs:/databricks/models/spacy\")\n",
    "\n",
    "# Copy files from S3 to DBFS\n",
    "dbutils.fs.cp(\"s3a://nlp-demo/models/spacy/\",\n",
    "              \"dbfs:/databricks/models/spacy/\", True)\n",
    "\n",
    "# Confirm files in DBFS\n",
    "display(dbutils.fs.ls(\"dbfs:/databricks/models/spacy/\"))\n",
    "\n",
    "# Make directory in DBFS\n",
    "dbutils.fs.mkdirs(\"dbfs:/databricks/scripts/\")\n",
    "\n",
    "# Put script in DBFS\n",
    "dbutils.fs.put(\"dbfs:/databricks/scripts/spacy_with_models.sh\", \\\n",
    "\"\"\"pip install /dbfs/databricks/models/spacy/en_core_web_lg-2.3.1.tar.gz \\\n",
    "pip install /dbfs/databricks/models/spacy/en_ner_base_V3-0.0.0.tar.gz \\\n",
    "pip install /dbfs/databricks/models/spacy/\\\n",
    "en_textcat_prodigy_V3_base_full-0.0.0.tar.gz\"\"\", True)\n",
    "\n",
    "# Confirm file in DBFS\n",
    "display(dbutils.fs.ls(\"dbfs:/databricks/scripts/spacy_with_models.sh\"))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unnecessary-encoding",
   "metadata": {},
   "source": [
    "### Enable Init Script and Restart Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floating-christian",
   "metadata": {},
   "source": [
    "### Run Speed Test - Inference on NER using SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "objective-doctor",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "# Python\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# PySpark\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "associate-antarctica",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "inputPath = \"s3a://nlp-demo/ag_dataset/prepared/train_prepared.csv\" \\\n",
    " # path to your S3 bucket\n",
    "df = spark.read.format('csv').options(header='true', inferSchema='true', \\\n",
    " quote=\"\\\"\", escape= \"\\\"\").load(inputPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finnish-emission",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Cache\n",
    "df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "standard-knock",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# View shape of data\n",
    "print((df.count(), len(df.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "formed-stanley",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Define schema\n",
    "schema = ArrayType(StructType([\n",
    "    StructField(\"text\", StringType(), False),\n",
    "    StructField(\"start_char\", IntegerType(), False),\n",
    "    StructField(\"end_char\", IntegerType(), False),\n",
    "    StructField(\"label\", StringType(), False)\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorporate-independence",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Define Function to Get Entities\n",
    "def get_entities(text):\n",
    "    global nlp\n",
    "    try:\n",
    "        doc = nlp(str(text))\n",
    "    except:\n",
    "        nlp = spacy.load('en_core_web_lg')\n",
    "        doc = nlp(str(text))\n",
    "    return [[e.text, e.start_char, e.end_char, e.label_] for e in doc.ents]\n",
    "\n",
    "get_entities_udf = udf(get_entities, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "referenced-afternoon",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Get Entities\n",
    "documents_df = df.withColumn('entities', get_entities_udf('description'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improved-pathology",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Write parquet\n",
    "documents_df.write.parquet(\\\n",
    " \"s3a://nlp-demo/ag_dataset/prepared/write_test.parquet\", \\\n",
    " mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dried-balance",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "'''Main Libraries'''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Connect to Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "write_path = '/content/drive/My Drive/Applied-NLP-in-the-Enterprise'\n",
    "\n",
    "# Install SpaCy\n",
    "!pip install -U spacy\n",
    "!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "super-evening",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    " \n",
    "# Start timer\n",
    "start_time = time.time()\n",
    " \n",
    "# Define function to read data\n",
    "def read_data(file):\n",
    "    read_path = '/content/drive/My Drive/Applied-NLP-in-the-Enterprise'\n",
    "    data = pd.read_csv(read_path+file)\n",
    "    return data\n",
    " \n",
    "# Read data\n",
    "data = read_data('/data/ag_dataset/prepared/train_prepared.csv')\n",
    " \n",
    "# Load model\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    " \n",
    "# Load time\n",
    "load_time = time.time()\n",
    "print(\"Time to load data and model: \", np.round(load_time-start_time,2))\n",
    " \n",
    "# Apply NLP model\n",
    "data[\"entities\"] = data[\"description\"].apply(lambda x: \\\n",
    " [(e.text, e.start_char, e.end_char, e.label_) for e in nlp(x).ents])\n",
    " \n",
    "# End timer\n",
    "end_time = time.time()\n",
    "print(\"Time to perform NER: \", np.round(end_time-load_time,2))\n",
    "print(\"Total time: \", np.round(time.time()-start_time,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "associate-blogger",
   "metadata": {},
   "source": [
    "## Machine Learning Jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polish-object",
   "metadata": {},
   "source": [
    "### Production Pipeline Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extensive-patch",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Load Libraries\n",
    "# Python\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# PySpark\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType, ArrayType\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "honest-surface",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Load Data\n",
    "inputPath = getArgument(\"inputPath\", \"default\")\n",
    "df = spark.read.format('csv').options(header='true', inferSchema='true', \\\n",
    " quote=\"\\\"\", escape= \"\\\"\").load(inputPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specific-rebecca",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Define Schema\n",
    "schema = ArrayType(StructType([\n",
    "    StructField(\"text\", StringType(), False),\n",
    "    StructField(\"start_char\", IntegerType(), False),\n",
    "    StructField(\"end_char\", IntegerType(), False),\n",
    "    StructField(\"label\", StringType(), False)\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radical-advocate",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Define Function to Get Entities\n",
    "def get_entities(text):\n",
    "    global nlp\n",
    "    try:\n",
    "        doc = nlp(text)\n",
    "    except:\n",
    "        nlp = spacy.load('en_ner_base_V3')\n",
    "        doc = nlp(text)\n",
    "    return [[e.text, e.start_char, e.end_char, e.label_] for e in doc.ents]\n",
    "\n",
    "get_entities_udf = udf(lambda x: get_entities(x), schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "failing-cable",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Get Entities\n",
    "documents_df = df.withColumn('entities', get_entities_udf('description'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wired-macedonia",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Write Parquet\n",
    "outPath = getArgument(\"outputPath\", \"default\")\n",
    "documents_df.write.format(\"parquet\").mode(\"overwrite\").save(outPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collect-linux",
   "metadata": {},
   "source": [
    "### Scheduled Machine Learning Jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sunset-railway",
   "metadata": {},
   "source": [
    "### Event-Driven Machine Learning Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "postal-newcastle",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "const https = require(\"https\");\n",
    "\n",
    "exports.handler = (event, context, callback) => {\n",
    "  var data = JSON.stringify({\n",
    "    \"job_id\": XXX\n",
    "  });\n",
    "\n",
    "  var options = {\n",
    "     host: \"XXX-XXXXXXX-XXX.cloud.databricks.com\",\n",
    "     port: 443,\n",
    "     path: \"/api/2.0/jobs/run-now\",\n",
    "     method: \"POST\",\n",
    "     // authentication headers\n",
    "     headers: {\n",
    "      \"Authorization\": \"Bearer XXXXXXXXXXXXXXXXXXXXXXXXXXXXX\",\n",
    "      \"Content-Type\": \"application/json\",\n",
    "      \"Content-Length\": Buffer.byteLength(data)\n",
    "     }\n",
    "  };\n",
    "\n",
    "  var request = https.request(options, function(res){\n",
    "    var body = \"\";\n",
    "\n",
    "    res.on(\"data\", function(data) {\n",
    "      body += data;\n",
    "    });\n",
    "\n",
    "    res.on(\"end\", function() {\n",
    "      console.log(body);\n",
    "    });\n",
    "\n",
    "    res.on(\"error\", function(e) {\n",
    "      console.log(\"Got error: \" + e.message);\n",
    "    });\n",
    "\n",
    "  });\n",
    "\n",
    "  request.write(data);\n",
    "  request.end();\n",
    "};\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wireless-pantyhose",
   "metadata": {},
   "source": [
    "## MLflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "creative-boring",
   "metadata": {},
   "source": [
    "### Log and Register Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agricultural-brunei",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Load Libraries\n",
    "# SpaCY\n",
    "import spacy \n",
    "\n",
    "# MLflow\n",
    "import mlflow\n",
    "import mlflow.spacy\n",
    "\n",
    "# Load model\n",
    "nlp = spacy.load(\"en_textcat_prodigy_V3_base_full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appointed-scholarship",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Print metadata\n",
    "nlp.meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "living-mobile",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# MLflow Tracking\n",
    "with mlflow.start_run(run_name='SpaCy-TextCat-Prodigy-V3-Base-Full'):\n",
    "    mlflow.set_tag('model_flavor', 'spacy')\n",
    "    mlflow.spacy.log_model(spacy_model=nlp, artifact_path='model')\n",
    "    mlflow.log_metric('textcat_score', 91.774875419)\n",
    "    my_run_id = mlflow.active_run().info.run_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "serious-costume",
   "metadata": {},
   "source": [
    "### MLflow Model Serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daily-press",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Connect to Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protective-repository",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    " \n",
    "# Define function to read data\n",
    "def read_data(file):\n",
    "    read_path = '/content/drive/My Drive/Applied-NLP-in-the-Enterprise'\n",
    "    data = pd.read_csv(read_path+file)\n",
    "    return data\n",
    " \n",
    "# Read data\n",
    "data = read_data('/data/ag_dataset/prepared/train_prepared.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indonesian-level",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Convert to JSON\n",
    "data.loc[:10,\"description\"].to_json(path_or_buf= \\\n",
    "        '/content/drive/My Drive/Applied-NLP-in-the-Enterprise/data/\\\n",
    "        ag_dataset/prepared/sample.json', orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endless-breathing",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Call the Model - CURL\n",
    "MODEL_VERSION_URI = XXXXXX #the model path\n",
    "DATABRICKS_TOKEN = XXXXXX #secret access token\n",
    "JSON_PATH = XXXXXX #path to the JSON we created earlier in Colab\n",
    " \n",
    "!curl -u token:$DATABRICKS_TOKEN -H \\\n",
    " \"Content-Type: application/json; format=pandas-records\" \\\n",
    " -d@$JSON_PATH $MODEL_VERSION_URI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interpreted-person",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Define Function to Call the Model in Python\n",
    "import requests\n",
    " \n",
    "def score_model(model_uri, databricks_token, data):\n",
    "    headers = {\n",
    "        \"Authorization\": 'Bearer '+ databricks_token,\n",
    "        \"Content-Type\": \"application/json; format=pandas-records\",\n",
    "      }\n",
    "    data_json = data if isinstance(data, list) else data.to_list()\n",
    "    response = requests.request(method='POST', headers=headers,\n",
    "        url=model_uri, json=data_json)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Request failed with status {response.status_code},\n",
    "            {response.text}\")\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hidden-straight",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Score the Model\n",
    "MODEL_VERSION_URI = XXXXXX # the model path\n",
    "DATABRICKS_TOKEN = XXXXXX # secret access token\n",
    " \n",
    "score_model(MODEL_VERSION_URI, DATABRICKS_TOKEN, data.loc[:10,\"description\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "therapeutic-throat",
   "metadata": {},
   "source": [
    "## Alternatives to Databricks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hourly-device",
   "metadata": {},
   "source": [
    "### Amazon Sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "knowing-client",
   "metadata": {},
   "source": [
    "### Saturn Cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "referenced-lawyer",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4ba3388e",
   "metadata": {},
   "source": [
    "In this chpater, we explored how to productionize machine learning models. First, we established the different roles that are involved in deploying, maintaining, and monitoring machine learning models in production. Data scientists, data and machine learning engineers, and data analysts are all involved in machine learning work, and their specific needs (such as programming environment and programming language of choice) need to be considered when choosing the appropriate platform for your organization to do machine learning.\n",
    "\n",
    "The platform that we recommend is Databricks, especially since it is built on top of Spark, which is the best distributed machine learning technology available today; it is the optimal choice when doing machine learning at scale. We used Databricks to create both scheduled and event-based machine learning pipelines, and we then used these pipelines to perform batch inference with our spaCy NER model. We also used MLflow on Databricks to deploy and serve our spaCy text classification model via a REST API, and we then tested the REST API using the browser, cURL, and Python. And, in the previous chapter, we used Streamlit to build and deploy multiple web apps using our spaCy models.\n",
    "\n",
    "This concludes our section of productionizing machine learning models; at this point we've corvered web apps, APIs, and scheduled and event-based batch pipelines. What we have not covered is how to perform machine learning on streaming data, which is beyond the scope of this book."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
