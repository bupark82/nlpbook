{
 "cells": [
  {
   "cell_type": "raw",
   "id": "literary-debate",
   "metadata": {},
   "source": [
    "[[ch11]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regulated-rally",
   "metadata": {},
   "source": [
    "# Productionization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "80a1c152",
   "metadata": {},
   "source": [
    "시제품 제작에서 생산으로 이동하는 과정의 어려움은 많은 회사가 실패하는 이유는 많은 회사가 시작한 기계 학습 이니셔티브에 대해 낮은 투자 수익을 얻는 주된 이유 중 하나입니다. 이전 장에서는 기계 학습을 웹 앱으로 생산하는 방법에 대해 논의했습니다. 그러나 기업이 기계 학습을 생산하고 생산 환경에서 이러한 모델의 가치를 진정으로 활용하는 주요 방법은 단순한 웹 앱을 통하는 것이 아닙니다. API와 자동화된 파이프라인을 통해 이루어지며 이 장에서 둘 다 다룰 것입니다. 또한 프로덕션에서 기계 학습 모델을 배포, 유지 관리 및 모니터링하는 것과 관련된 다양한 역할에 대해 논의하고 기업에서 데이터 과학 및 기계 학습 작업을 수행하는 현재 시장을 선도하는 플랫폼 중 하나인 Databricks를 살펴봅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developing-evolution",
   "metadata": {},
   "source": [
    "## Data Scientists, Engineers, and Analysts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ca077d0b",
   "metadata": {},
   "source": [
    "기계 학습 모델을 생산하는 방법에 대해 알아보기 전에, 전체 기계 학습 개발 및 배포 주기 동안 참여하게 될 사람 다른 개인을 검토해 보겠습니다. 이러한 개인의 역할과 프로그래밍 언어 및 프로그래밍 환경에 대한 선호도를 이해하는 것은 프로토타입 모델에서 프로덕션에 배포하는 데 걸리는 마찰을 줄이고 싶기 때문에 중요합니다. 즉, 프로덕션에서 기계 학습을 성공적으로 실행하려면 협업의 용이성을 고려해야 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "republican-young",
   "metadata": {},
   "source": [
    "### Prototyping, Deployment, and Maintenance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e619c28a",
   "metadata": {},
   "source": [
    "기계 학습 주기에는 프로토타이핑, 배포, 지속적인 모니터링 및 유지 관리의 세 가지 기술 단계가 있습니다. ptototyping 단계에서 데이터 과학자는 비즈니스 목표(일반적으로 제품 관리자가 알려주지 않음)를 고려하고 데이터를 준비하고, 기능 엔지니어링을 수행하고, 테스트할 알고리즘을 선택하고, 비용 기능을 정의하고, 여러 모델을 훈련 및 평가하고, 최상의 모델을 선택합니다. 당신이 매우 친숙한 우승자로서 모델을 수행합니다.\n",
    "\n",
    "이 프로토타이핑 단계에서 데이터 엔지니어는 여러 소스의 데이터를 하나의 중앙 집중식 위치로 통합하고 기계 학습 개발을 위해 데이터 과학자가 사용할 수 있도록 하는 데 필요한 일부 ETL(추출, 변환 및 로드) 작업을 도울 수 있습니다. 데이터 분석가는 데이터 과학자를 지원하기 위해 데이터 탐색 및 준비를 수행하고 기계 학습 모델의 결과를 평가하는 데 도움을 줄 수 있습니다. 그러나 데이터 과학자는 이 모델 개발 단계에서 주로 주요 역할을 하는 반면 엔지니어와 분석가는 지원 역할을 수행합니다.\n",
    "\n",
    "모델 배포 단계에서 데이터 및 기계 학습 엔지니어는 프로토타입 단계에서 기계 학습 모델을 개발한 데이터 과학자의 지원을 받는 주요 플레이어가 됩니다. 엔지니어는 일반적으로 데이터 과학자가 개발한 코드를 리팩터링하여 모델의 성능(즉, 대규모 데이터 세트로 확장 가능)과 견고성(즉, 오류 및 엣지 케이스를 정상적으로 처리할 수 있음)을 높입니다. 엔지니어는 또한 회사의 소프트웨어 아키텍처에 모델을 배치하여 모델이 더 큰 워크플로에서 필요한 작업을 수행하도록 해야 합니다. 이러한 데이터 및 ML 엔지니어는 모델을 프로토타입에서 프로덕션으로 가져오는 데 중요한 역할을 합니다.\n",
    "\n",
    "데이터 과학자는 페어 프로그래밍을 통해 엔지니어를 지원하고, 엔지니어가 데이터 과학자의 원래 코드에 대해 보다 성능이 뛰어나고 강력한 버전을 작성하면서 서로 협력합니다. 데이터 과학자는 모델이 어떻게 작동하는지 설명하고 엔지니어가 가질 수 있는 다른 질문에 답합니다.\n",
    "\n",
    "모델 배포 중에 데이터 분석가의 역할은 매우 제한적입니다. 그러나 일단 모델이 배포되면 데이터 분석가는 모델의 출력을 해석하고 모델의 비기술적 소비자와 인터페이스하는 주요 역할을 맡게 됩니다. 이 두 가지 모두 조직 내에서 내부적으로는 잠재적으로 클라이언트와 외부적으로 이루어집니다.\n",
    "\n",
    "데이터 및 ML 엔지니어와 분석가는 모델이 제대로 작동하지 않는 경우 첫 번째 방어선이기도 합니다. 데이터 및 ML 엔지니어는 모델을 모니터링하여 가동 시간이 거의 100%이고 대용량 데이터로 잘 확장되며 오류 대신 성공적인 응답을 생성하는지 확인합니다. 데이터 분석가는 또한 모델의 출력에 오류가 있는 경우를 식별하고 모델의 성능이 저하될 때 플래그를 지정하는 데 도움을 줍니다. 이는 모델을 개발하는 데 사용된 교육 데이터에 의해 잘 표현되지 않을 수 있는 새로운 데이터가 모델을 통해 흐르면서 시간이 지남에 따라 발생합니다.\n",
    "\n",
    "모델의 문제가 엔지니어링과 관련된 경우 엔지니어가 직접 문제를 해결합니다. 그러나 문제가 모델과 관련된 경우 엔지니어와 분석가는 데이터 과학자를 참여시켜 모델이 제대로 작동하지 않는 이유를 더 깊이 파고들 것입니다. 일반적인 해결 방법 중 하나는 모델 재교육입니다. 데이터 과학자는 모델이 프로덕션에서 추론을 수행하는 데이터를 나타내는 새 데이터에 대한 모델을 주기적으로 유지해야 합니다.\n",
    "\n",
    "데이터 과학자가 모델 재교육을 마치면 데이터 및 ML 엔지니어는 프로덕션에 배포하고 데이터 분석가는 결과를 해석하고 모델이 실제로 더 나은 성능을 발휘하는지 확인합니다. 그리고, 주기는 계속됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extraordinary-father",
   "metadata": {},
   "source": [
    "### Notebooks and Scripts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7d6d80e6",
   "metadata": {},
   "source": [
    "일반적인 논쟁점은 프로덕션 작업에 Jupyter 노트북을 사용하는 것입니다. 역사의 결과로 노트북은 열악한 소프트웨어 엔지니어링 관행과 재현할 수 없는 코드를 조장하는 나쁜 평판을 얻었습니다. 대부분의 경우 이러한 우려가 많이 완화되었습니다. 그러나 노트북과 스크립트를 사용하는 것이 어디에 유용할지 생각하는 것은 여전히 ​​중요합니다.\n",
    "\n",
    "프로토타이핑 단계에서 대부분의 데이터 과학자는 Jupyter Notebook, JupyterLab, IPython, Google Colab 및 심지어 VS Code 확장과 같은 노트북 기반 환경에서 모델을 개발합니다. 노트북 기반 환경은 프로토타이핑 및 실험에 적합합니다. 그들은 정말로 우리를 \"생각의 속도에 맞춘 코드\" 격언에 가깝게 만듭니다.\n",
    "\n",
    "그러나 \"생각\"에 단위 테스트 또는 PEP 준수가 반드시 필요한 것은 아니며 일반적으로 메인에 강제로 자주 푸시되는 분산 버전 제어 시스템과 함께 작동할 필요가 없습니다.\n",
    "\n",
    "따라서 데이터 과학자가 이러한 노트북 기반 환경에서 모델 개발을 완료한 후 엔지니어는 일반적으로 코드를 스크립트로 리팩터링하고 기능을 작성하고 클래스로 구성하여 결과를 더 쉽게 재현하고, 즉시 실험하고, 프로덕션에서 디버그할 수 있도록 합니다.\n",
    "\n",
    "데이터 과학자는 빠르게 실험하고 반복할 수 있기 때문에 노트북을 선호합니다. 엔지니어는 배포에 필요한 더 광범위한 도구 세트로 작업하기 때문에 스크립트를 선호합니다. 모델이 생산되면 데이터 및 ML 엔지니어는 여전히 모델의 성능과 견고성을 모니터링해야 하며 데이터 과학자는 때때로 모델을 재훈련하여 모델의 품질을 유지해야 합니다. 따라서 노트북과 스크립트 사이를 오가려는 욕구는 여전히 존재합니다.\n",
    "\n",
    "일반적으로 비즈니스 기능과 더 많이 인터페이스하고 기계 학습 모델 개발 및 배포에는 덜 관여하는 데이터 분석가는 모델의 출력에 참여하고 결과를 해석해야 합니다. 이러한 데이터 분석가의 요구 사항도 수용해야 합니다.\n",
    "\n",
    "프로덕션 환경에서 기계 학습을 수행하려는 회사는 데이터 과학자, 데이터 및 ML 엔지니어, 데이터 분석가 등 다양한 역할과 세 기능 모두에서 작업을 위한 통합 플랫폼을 구축하는 방법을 알고 있어야 합니다. 다른 기관은 이러한 역할을 다른 이름으로 참조할 수 있습니다. 표준화된 개종은 없으며 여러 번 한 사람이 여러 모자를 쓸 수 있습니다. 그러나 하루가 끝나면 중요한 작업은 새로운 모델 구축, 프로덕션 파이프라인에 통합, 결과 분석/해석으로 분할되어 조직에 귀중한 통찰력을 생성할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "endangered-origin",
   "metadata": {},
   "source": [
    "## Databricks: Your Unified Data Analytics Platform"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aa7f38fe",
   "metadata": {},
   "source": [
    "다행스럽게도 오늘날 이러한 다양한 요구 사항을 수용하고 머신 러닝 모델을 프로덕션에 쉽게 적용할 수 있는 플랫폼인 Databricks가 있습니다. Databricks는 오늘날 대규모 프로덕션 사용 사례를 위한 협업 데이터 과학 및 기계 학습 작업의 업계 리더입니다. 데이터 과학자, 데이터 및 ML 엔지니어, 데이터 분석가를 수용하고 모델 개발에서 테스트 및 배포, 모니터링 및 유지 관리에 이르는 전체 ML 수명 주기를 지원합니다.\n",
    "\n",
    "Databricks를 살펴보고 이를 사용하여 이 책의 앞부분에서 개발한 기계 학습 모델 중 하나를 배포해 보겠습니다.\n",
    "\n",
    "> Amazon SageMaker 및 Saturn Cloud(이 장의 뒷부분에서 설명)와 같은 Databricks에 대한 대안이 있지만 Databricks는 여러 가지 이유로 오늘날 시장의 선두 주자입니다.\n",
    "\n",
    "> - 가장 오랜 역사를 가지고 있어 많은 실무자들이 이미 이 기술에 익숙해져 있음을 의미합니다.\n",
    "> - 현재 대부분의 회사에 필요한 보안 및 규정 준수 기능을 포함하여 조직을 위한 가장 성숙한 제품을 제공합니다.\n",
    "> - 빅 데이터를 위한 기본이며 이 분야에서 계속해서 혁신하고 있습니다. 사실, Databricks의 제작자는 오늘날 가장 인기 있는 빅 데이터 처리 프레임워크(Spark, 다음 섹션에서 다룰 예정)의 원래 제작자였습니다.\n",
    "\n",
    "> 우리는 앞으로 몇 년 동안 업계의 경쟁이 더욱 치열해질 것으로 예상하므로 경력을 쌓으면서 Databricks의 대안을 탐색하는 데 시간을 투자하십시오."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floppy-inflation",
   "metadata": {},
   "source": [
    "### Support for Big Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4f41080a",
   "metadata": {},
   "source": [
    "기계 학습 모델을 개발할 때 많은 데이터 과학자는 매우 큰 데이터 세트에서 성능을 발휘하는 방법이나 스트리밍 사용 사례(예: 실시간 추론을 위해 데이터가 모델을 통해 흐르는 경우)를 처리하는 방법을 충분히 고려하지 않습니다. 데이터 과학자는 일반적으로 모델 훈련을 위해 준비된 중소 규모의 데이터 세트에서 작업합니다. 그러나 일단 생산에 들어가면 이러한 모델은 수십 배 더 큰 데이터 세트에서 신속하게 추론을 수행해야 할 수 있습니다.\n",
    "\n",
    "기계 학습을 대규모로 수행하기 위해 단일 기계를 사용하여 추론을 수행하는 것은 비실용적입니다. 오히려 많은 기계가 필요합니다. 이러한 기계는 작업을 위해 함께 연결될 때 종종 머신 클러스터라고 합니다.\n",
    "\n",
    "Hadoop 및 Spark와 같은 빅 데이터 기술이 작동하는 곳이기도 합니다. Hadoop은 2006년 Yahoo 프로젝트로 시작된 최초의 빅 데이터 기술입니다. Hadoop을 사용하면 사용자가 클러스터의 여러 시스템에서 병렬로 데이터 작업을 수행하고 이를 출력으로 다시 연결할 수 있습니다. 데이터 작업을 팜 아웃하고 결과를 생성하는 이 메커니즘을 MapReduce라고 합니다. 예를 들어 사용자가 100억 행 데이터 세트의 모든 요소에 스칼라를 추가하려는 경우 사용자는 10대의 시스템 클러스터를 사용하여 단일 시스템에서 작업을 수행하는 것보다 거의 10배 빠르게 작업을 수행할 수 있습니다. \n",
    "\n",
    "새로운 빅 데이터 기술인 Spark는 2012년 UC Berkeley의 AMPLab에서 개발되었습니다. 여러 면에서 Hadoop과 유사합니다. Spark는 Hadoop과 마찬가지로 클러스터 전체에서 병렬로 데이터를 처리합니다. 그러나 Spark는 이러한 데이터 작업을 메모리(RAM이라고도 함)에서 수행하며 여기서 Hadoop은 디스크에 있는 파일 시스템 형식(Hadoop 분산 파일 시스템 또는 HDFS)으로 파일을 읽고 씁니다.\n",
    "\n",
    "Hadoop은 여전히 ​​인기가 있지만 Spark는 빅 데이터 분야에서 확실한 승자이자 떠오르는 별입니다. Spark는 Hadoop에 비해 인메모리에서 100배, 디스크에서 10배 더 빠르게 실행할 수 있습니다. Spark는 또한 기계 학습 애플리케이션에서 훨씬 빠르며 데이터 과학자(Python의 pandas 및 R 패키지와 유사한 Spark DataFrames)와 데이터 분석가(Spark SQL, 관계 데이터 저장소의 SQL 테이블과 유사)가 선호하는 추상화를 지원합니다.\n",
    "\n",
    "Databricks는 Spark를 기반으로 하며 Databricks의 공동 창립자 중 한 명인 Matei Zaharia는 Spark의 창립자이기도 합니다. Spark는 오픈 소스 프레임워크이고 Apache Software Foundation에서 지원하지만 Databricks 버전의 Spark는 상업적으로 초점을 맞추고 향후 최적화된 Spark 인스턴스이지만 Databricks가 데이터 및 기계 학습 작업에 선호되는 플랫폼인 또 다른 이유입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extensive-mineral",
   "metadata": {},
   "source": [
    "### Support For Multiple Programming Languages"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0bfba2e6",
   "metadata": {},
   "source": [
    "데이터 과학자, 엔지니어 및 분석가의 주요 과제 중 하나는 프로그래밍 언어의 불일치입니다. 데이터 과학자는 일반적으로 Python(및 R, 점점 줄어들고 있음)으로 코딩하고 데이터 및 기계 학습 엔지니어는 Scala 또는 PySpark(둘 다 Spark를 활용함)를 선호하고 데이터 분석가는 SQL을 선호합니다.\n",
    "\n",
    "Databricks를 사용하면 한 프로그래밍 언어에서 다른 프로그래밍 언어(Python에서 Scala, SQL로)로 쉽게 전환할 수 있으며 노트북 기반 환경(일반적으로 데이터 과학자가 선호함)과 스크립트, 프로그래밍 방식 액세스 및 API(엔지니어가 프로덕션 환경에서 시스템을 효율적으로 배포, 모니터링 및 유지 관리)를 지원합니다.\n",
    "\n",
    "이 다국어 접근 방식의 장점은 데이터 과학자, 엔지니어 및 분석가가 모두 단일 플랫폼에서 작업할 수 있다는 것입니다. 모델이 개발되면 Databricks를 통해 이러한 모델을 빠르게 배포할 수 있습니다. 모델을 프로토타입에서 프로덕션으로 가져오기 위해 한 플랫폼에서 다른 플랫폼으로 전환할 필요가 없습니다. Databricks는 데이터 과학자, 엔지니어 및 분석가가 전통적으로 직면하는 조직 및 기술 사일로를 해체합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enhanced-association",
   "metadata": {},
   "source": [
    "### Support For ML Frameworks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "72541403",
   "metadata": {},
   "source": [
    "그림 11-1은 Databricks가 과제와 해결 방법을 설명하는 방법을 보여줍니다. 과제에는 여러 프로그래밍 언어, 여러 기계 학습 프레임워크 및 여러 DevOps 도구를 지원해야 할 필요성과 데이터를 준비하고 모델을 구축하고 모델을 프로덕션으로 옮기는 팀 간의 마찰을 줄여야 할 필요성이 포함됩니다. 그리고 보안 및 규정 준수는 모든 성숙한 조직에서 중요한 고려 사항입니다.\n",
    "\n",
    "Figure 11-1. Databricks platform (courtesy of Databricks(https://oreil.ly/iX7SW))\n",
    "\n",
    "가장 일반적인 데이터 과학 프로그래밍 언어와 PyTorch, TensorFlow, Scikit 학습. 이 환경은 몇 번의 클릭만으로(일부 초기 설정 후) 사용할 수 있으므로 데이터 과학자는 데이터 과학을 수행하고 기계 학습 모델을 개발하기 위해 컴퓨터 클러스터를 신속하게 가동한 다음 작업이 완료되면 클러스터를 가동 중지할 수 있습니다. 이러한 환경은 사용자 정의가 가능하여 데이터 과학 및 ML 팀이 필요에 따라 라이브러리를 가져오고 초기화 스크립트를 실행할 수 있습니다.\n",
    "\n",
    "Spark는 또한 Databricks에서 사용할 수 있는 자체 Spark 최적화 기계 학습 라이브러리인 MLlib와 함께 제공됩니다. MLlib는 계산, 회귀, 클러스터링 및 협업 필터링과 같은 보다 일반적인 기계 학습 알고리즘을 지원하고 데이터 과학자가 보다 성능이 뛰어난 기계 학습 파이프라인을 개발할 수 있도록 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stable-paint",
   "metadata": {},
   "source": [
    "### Support for Model Repository, Access Control, Data Lineage, and Versioning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bf045aed",
   "metadata": {},
   "source": [
    "마지막으로 Databricks는 실험을 추적하고, 기계 학습 모델을 등록 및 버전화하고, 역할에 따라 액세스 및 권한을 부여하고, 데이터 파이프라인을 통해 데이터가 흐르는 방식을 추적하는 기능을 제공합니다. Databricks는 대기업이 기계 학습 작업을 확장할 때 필요한 많은 규정 준수 기능과 함께 제공됩니다.\n",
    "\n",
    "요약하자면 Databricks의 ML 수명 주기를 보여주는 그림 11-2를 참조하십시오. 이 그림은 여러 데이터 소스의 데이터(예: 플랫 파일, 빅 데이터 및 스트리밍 데이터)가 필요한 작업(협업 작업 공간, AutoML 기능, 기능 엔지니어링 저장소, 실험 추적, 모델 레지스트리, 모델 배포 옵션, 보안 및 규정 준수 지원의 모든 것))을 지원하기 위해 여러 계층이 있는 Databricks로 흐르는 방식을 보여줍니다. Databricks에서 수행된 작업은 보고서 및 대시보드와 같은 분석 사용 사례와 API를 통한 운영 AI, 일괄 점수 매기기 및 에지 장치 배포를 모두 지원합니다.\n",
    "\n",
    "Figure 11-2. Machine learning life cycle (courtesy of Databricks (https://oreil.ly/iMuq7).)\n",
    "\n",
    "Databricks는 데이터 과학자, 엔지니어 및 분석가가 기계 학습 모델을 대규모로 개발 및 배포하고, 여러 프로그래밍 언어로 작업하고, 몇 번의 클릭만으로 다양한 ML 프레임워크 라이브러리에 액세스할 수 있는 통합 플랫폼을 제공합니다. 모델 프로토타입에서 생산까지 모든 단계에서 협업합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "earned-premium",
   "metadata": {},
   "source": [
    "## Databricks Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spectacular-democrat",
   "metadata": {},
   "source": [
    "### Set Up Access to S3 Bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hollow-howard",
   "metadata": {},
   "source": [
    "### Set Up Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "separated-oasis",
   "metadata": {},
   "source": [
    "### Create Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sorted-ordering",
   "metadata": {},
   "source": [
    "### Create Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "psychological-racing",
   "metadata": {},
   "source": [
    "```python\n",
    "# Make directory in DBFS\n",
    "dbutils.fs.mkdirs(\"dbfs:/databricks/models/spacy\")\n",
    "\n",
    "# Copy files from S3 to DBFS\n",
    "dbutils.fs.cp(\"s3a://nlp-demo/models/spacy/\",\n",
    "              \"dbfs:/databricks/models/spacy/\", True)\n",
    "\n",
    "# Confirm files in DBFS\n",
    "display(dbutils.fs.ls(\"dbfs:/databricks/models/spacy/\"))\n",
    "\n",
    "# Make directory in DBFS\n",
    "dbutils.fs.mkdirs(\"dbfs:/databricks/scripts/\")\n",
    "\n",
    "# Put script in DBFS\n",
    "dbutils.fs.put(\"dbfs:/databricks/scripts/spacy_with_models.sh\", \\\n",
    "\"\"\"pip install /dbfs/databricks/models/spacy/en_core_web_lg-2.3.1.tar.gz \\\n",
    "pip install /dbfs/databricks/models/spacy/en_ner_base_V3-0.0.0.tar.gz \\\n",
    "pip install /dbfs/databricks/models/spacy/\\\n",
    "en_textcat_prodigy_V3_base_full-0.0.0.tar.gz\"\"\", True)\n",
    "\n",
    "# Confirm file in DBFS\n",
    "display(dbutils.fs.ls(\"dbfs:/databricks/scripts/spacy_with_models.sh\"))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unnecessary-encoding",
   "metadata": {},
   "source": [
    "### Enable Init Script and Restart Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floating-christian",
   "metadata": {},
   "source": [
    "### Run Speed Test - Inference on NER using SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "objective-doctor",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "# Python\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# PySpark\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "associate-antarctica",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "inputPath = \"s3a://nlp-demo/ag_dataset/prepared/train_prepared.csv\" \\\n",
    " # path to your S3 bucket\n",
    "df = spark.read.format('csv').options(header='true', inferSchema='true', \\\n",
    " quote=\"\\\"\", escape= \"\\\"\").load(inputPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finnish-emission",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Cache\n",
    "df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "standard-knock",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# View shape of data\n",
    "print((df.count(), len(df.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "formed-stanley",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Define schema\n",
    "schema = ArrayType(StructType([\n",
    "    StructField(\"text\", StringType(), False),\n",
    "    StructField(\"start_char\", IntegerType(), False),\n",
    "    StructField(\"end_char\", IntegerType(), False),\n",
    "    StructField(\"label\", StringType(), False)\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorporate-independence",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Define Function to Get Entities\n",
    "def get_entities(text):\n",
    "    global nlp\n",
    "    try:\n",
    "        doc = nlp(str(text))\n",
    "    except:\n",
    "        nlp = spacy.load('en_core_web_lg')\n",
    "        doc = nlp(str(text))\n",
    "    return [[e.text, e.start_char, e.end_char, e.label_] for e in doc.ents]\n",
    "\n",
    "get_entities_udf = udf(get_entities, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "referenced-afternoon",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Get Entities\n",
    "documents_df = df.withColumn('entities', get_entities_udf('description'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improved-pathology",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Write parquet\n",
    "documents_df.write.parquet(\\\n",
    " \"s3a://nlp-demo/ag_dataset/prepared/write_test.parquet\", \\\n",
    " mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dried-balance",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "'''Main Libraries'''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Connect to Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "write_path = '/content/drive/My Drive/Applied-NLP-in-the-Enterprise'\n",
    "\n",
    "# Install SpaCy\n",
    "!pip install -U spacy\n",
    "!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "super-evening",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    " \n",
    "# Start timer\n",
    "start_time = time.time()\n",
    " \n",
    "# Define function to read data\n",
    "def read_data(file):\n",
    "    read_path = '/content/drive/My Drive/Applied-NLP-in-the-Enterprise'\n",
    "    data = pd.read_csv(read_path+file)\n",
    "    return data\n",
    " \n",
    "# Read data\n",
    "data = read_data('/data/ag_dataset/prepared/train_prepared.csv')\n",
    " \n",
    "# Load model\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    " \n",
    "# Load time\n",
    "load_time = time.time()\n",
    "print(\"Time to load data and model: \", np.round(load_time-start_time,2))\n",
    " \n",
    "# Apply NLP model\n",
    "data[\"entities\"] = data[\"description\"].apply(lambda x: \\\n",
    " [(e.text, e.start_char, e.end_char, e.label_) for e in nlp(x).ents])\n",
    " \n",
    "# End timer\n",
    "end_time = time.time()\n",
    "print(\"Time to perform NER: \", np.round(end_time-load_time,2))\n",
    "print(\"Total time: \", np.round(time.time()-start_time,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "associate-blogger",
   "metadata": {},
   "source": [
    "## Machine Learning Jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polish-object",
   "metadata": {},
   "source": [
    "### Production Pipeline Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extensive-patch",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Load Libraries\n",
    "# Python\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# PySpark\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType, ArrayType\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "honest-surface",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Load Data\n",
    "inputPath = getArgument(\"inputPath\", \"default\")\n",
    "df = spark.read.format('csv').options(header='true', inferSchema='true', \\\n",
    " quote=\"\\\"\", escape= \"\\\"\").load(inputPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specific-rebecca",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Define Schema\n",
    "schema = ArrayType(StructType([\n",
    "    StructField(\"text\", StringType(), False),\n",
    "    StructField(\"start_char\", IntegerType(), False),\n",
    "    StructField(\"end_char\", IntegerType(), False),\n",
    "    StructField(\"label\", StringType(), False)\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radical-advocate",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Define Function to Get Entities\n",
    "def get_entities(text):\n",
    "    global nlp\n",
    "    try:\n",
    "        doc = nlp(text)\n",
    "    except:\n",
    "        nlp = spacy.load('en_ner_base_V3')\n",
    "        doc = nlp(text)\n",
    "    return [[e.text, e.start_char, e.end_char, e.label_] for e in doc.ents]\n",
    "\n",
    "get_entities_udf = udf(lambda x: get_entities(x), schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "failing-cable",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Get Entities\n",
    "documents_df = df.withColumn('entities', get_entities_udf('description'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wired-macedonia",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Write Parquet\n",
    "outPath = getArgument(\"outputPath\", \"default\")\n",
    "documents_df.write.format(\"parquet\").mode(\"overwrite\").save(outPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collect-linux",
   "metadata": {},
   "source": [
    "### Scheduled Machine Learning Jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sunset-railway",
   "metadata": {},
   "source": [
    "### Event-Driven Machine Learning Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "postal-newcastle",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "const https = require(\"https\");\n",
    "\n",
    "exports.handler = (event, context, callback) => {\n",
    "  var data = JSON.stringify({\n",
    "    \"job_id\": XXX\n",
    "  });\n",
    "\n",
    "  var options = {\n",
    "     host: \"XXX-XXXXXXX-XXX.cloud.databricks.com\",\n",
    "     port: 443,\n",
    "     path: \"/api/2.0/jobs/run-now\",\n",
    "     method: \"POST\",\n",
    "     // authentication headers\n",
    "     headers: {\n",
    "      \"Authorization\": \"Bearer XXXXXXXXXXXXXXXXXXXXXXXXXXXXX\",\n",
    "      \"Content-Type\": \"application/json\",\n",
    "      \"Content-Length\": Buffer.byteLength(data)\n",
    "     }\n",
    "  };\n",
    "\n",
    "  var request = https.request(options, function(res){\n",
    "    var body = \"\";\n",
    "\n",
    "    res.on(\"data\", function(data) {\n",
    "      body += data;\n",
    "    });\n",
    "\n",
    "    res.on(\"end\", function() {\n",
    "      console.log(body);\n",
    "    });\n",
    "\n",
    "    res.on(\"error\", function(e) {\n",
    "      console.log(\"Got error: \" + e.message);\n",
    "    });\n",
    "\n",
    "  });\n",
    "\n",
    "  request.write(data);\n",
    "  request.end();\n",
    "};\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wireless-pantyhose",
   "metadata": {},
   "source": [
    "## MLflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "creative-boring",
   "metadata": {},
   "source": [
    "### Log and Register Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agricultural-brunei",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Load Libraries\n",
    "# SpaCY\n",
    "import spacy \n",
    "\n",
    "# MLflow\n",
    "import mlflow\n",
    "import mlflow.spacy\n",
    "\n",
    "# Load model\n",
    "nlp = spacy.load(\"en_textcat_prodigy_V3_base_full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appointed-scholarship",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Print metadata\n",
    "nlp.meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "living-mobile",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# MLflow Tracking\n",
    "with mlflow.start_run(run_name='SpaCy-TextCat-Prodigy-V3-Base-Full'):\n",
    "    mlflow.set_tag('model_flavor', 'spacy')\n",
    "    mlflow.spacy.log_model(spacy_model=nlp, artifact_path='model')\n",
    "    mlflow.log_metric('textcat_score', 91.774875419)\n",
    "    my_run_id = mlflow.active_run().info.run_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "serious-costume",
   "metadata": {},
   "source": [
    "### MLflow Model Serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daily-press",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Connect to Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protective-repository",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    " \n",
    "# Define function to read data\n",
    "def read_data(file):\n",
    "    read_path = '/content/drive/My Drive/Applied-NLP-in-the-Enterprise'\n",
    "    data = pd.read_csv(read_path+file)\n",
    "    return data\n",
    " \n",
    "# Read data\n",
    "data = read_data('/data/ag_dataset/prepared/train_prepared.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indonesian-level",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Convert to JSON\n",
    "data.loc[:10,\"description\"].to_json(path_or_buf= \\\n",
    "        '/content/drive/My Drive/Applied-NLP-in-the-Enterprise/data/\\\n",
    "        ag_dataset/prepared/sample.json', orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endless-breathing",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Call the Model - CURL\n",
    "MODEL_VERSION_URI = XXXXXX #the model path\n",
    "DATABRICKS_TOKEN = XXXXXX #secret access token\n",
    "JSON_PATH = XXXXXX #path to the JSON we created earlier in Colab\n",
    " \n",
    "!curl -u token:$DATABRICKS_TOKEN -H \\\n",
    " \"Content-Type: application/json; format=pandas-records\" \\\n",
    " -d@$JSON_PATH $MODEL_VERSION_URI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interpreted-person",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Define Function to Call the Model in Python\n",
    "import requests\n",
    " \n",
    "def score_model(model_uri, databricks_token, data):\n",
    "    headers = {\n",
    "        \"Authorization\": 'Bearer '+ databricks_token,\n",
    "        \"Content-Type\": \"application/json; format=pandas-records\",\n",
    "      }\n",
    "    data_json = data if isinstance(data, list) else data.to_list()\n",
    "    response = requests.request(method='POST', headers=headers,\n",
    "        url=model_uri, json=data_json)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Request failed with status {response.status_code},\n",
    "            {response.text}\")\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hidden-straight",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Score the Model\n",
    "MODEL_VERSION_URI = XXXXXX # the model path\n",
    "DATABRICKS_TOKEN = XXXXXX # secret access token\n",
    " \n",
    "score_model(MODEL_VERSION_URI, DATABRICKS_TOKEN, data.loc[:10,\"description\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "therapeutic-throat",
   "metadata": {},
   "source": [
    "## Alternatives to Databricks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hourly-device",
   "metadata": {},
   "source": [
    "### Amazon Sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "knowing-client",
   "metadata": {},
   "source": [
    "### Saturn Cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "referenced-lawyer",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4ba3388e",
   "metadata": {},
   "source": [
    "이번 장에서는 기계 학습 모델을 생산화하는 방법을 살펴보았습니다. 먼저 프로덕션에서 기계 학습 모델을 배포, 유지 관리 및 모니터링하는 것과 관련된 다양한 역할을 설정했습니다. 데이터 과학자, 데이터 및 머신 러닝 엔지니어, 데이터 분석가는 모두 머신 러닝 작업에 참여하며 조직에서 머신 러닝을 수행하는 데 적합한 플랫폼을 선택할 때 이들의 특정 요구 사항(예: 선택한 프로그래밍 환경 및 프로그래밍 언어)을 고려해야 합니다.\n",
    "\n",
    "우리가 권장하는 플랫폼은 Databricks입니다. 특히 오늘날 사용할 수 있는 최고의 분산 기계 학습 기술인 Spark를 기반으로 구축되었기 때문입니다. 대규모 기계 학습을 수행할 때 최적의 선택입니다. 우리는 Databricks를 사용하여 예약 및 이벤트 기반 기계 학습 파이프라인을 모두 생성한 다음 이러한 파이프라인을 사용하여 spaCy NER 모델로 일괄 추론을 수행했습니다. 또한 Databricks에서 MLflow를 사용하여 REST API를 통해 spaCy 텍스트 분류 모델을 배포하고 제공한 다음 브라우저, cURL 및 Python을 사용하여 REST API를 테스트했습니다. 그리고 이전 장에서는 Streamlit을 사용하여 spaCy 모델을 사용하여 여러 웹 앱을 구축하고 배포했습니다.\n",
    "\n",
    "이것으로 기계 학습 모델 생산 섹션을 마칩니다. 이 시점에서 우리는 웹 앱, API, 예약 및 이벤트 기반 배치 파이프라인을 조사했습니다. 우리가 다루지 않은 것은 이 책의 범위를 벗어나는 스트리밍 데이터에 대해 기계 학습을 수행하는 방법입니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
