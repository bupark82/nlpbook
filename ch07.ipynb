{
 "cells": [
  {
   "cell_type": "raw",
   "id": "professional-partnership",
   "metadata": {},
   "source": [
    "[[ch07]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reflected-blackjack",
   "metadata": {},
   "source": [
    "# Tranformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0917d8-c793-4166-9af7-24d76991d8dd",
   "metadata": {},
   "source": [
    "In the previous chapter, we covered RNNs, the modeling architecture in vogue in NLP until the Transformer architecture gained prominence.\n",
    "\n",
    "Transformers are the workhorse of modern NLP. The original architecture, first propsed in 2017, has taken the (deep learning) world by storm. Since then, NLP literature has been inundated with all sorts of new architectures that are broadly classified into either Sesame street characters or words that end with \"-former\"\n",
    "\n",
    "In this chapter, we'll look at that very architecture-the transformer-in detail. We'll analyze the core innovations and explore a hot new category of neural network layers: the attention mechanism."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floating-spread",
   "metadata": {},
   "source": [
    "## Building a Transformer from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fb3884-e59d-441a-acd9-60f0681fb626",
   "metadata": {},
   "source": [
    "In Chapter 2 and 3, we explored how to use transformers in practice and how to leverage pretrained transformers to solve complex NLP problems. Now we're going to take a deep dive into the architecture itself and learn how transformers work from first principles.\n",
    "\n",
    "What does \"first principles\" mean? Well, for starters, it means we're not allowed to use the Hugging Face Transformers library. We've raved about it plenty in this book already, so it's about time we take a break from that and see how things actually work under the hood. For this chapter, we're going to be using raw PyTorch instead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b731210-e8e9-483e-9534-413372746037",
   "metadata": {},
   "source": [
    "PyTorch, being a fully fledged deep learning library that most researchers use, naturally has an implementation of the extremly popular transformer architecture, just like a Hugging Face library does. This version, though, exposed as an nn.Module, is much more DIY and is meant to be used with the other familiar PyTorch tools like dataloaders, optimizers, etc.\n",
    "\n",
    "As we've mentioned before, one of the best ways to see what any deep learning related class/function does is by looking at the type signature and he dimensionality of the inputs and outputs. So let's do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "everyday-baking",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detailed-wilson",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Transformer()\n",
    "model.encoder.layers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a901e9-3427-4f0e-a7c4-4f401c61ceda",
   "metadata": {},
   "outputs": [],
   "source": [
    "TransformerEncoderLayer(\n",
    "    (self_attn): MultiheadAttention(\n",
    "        (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
    "    )\n",
    "    (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
    "    (dropout): Dropout(p=0.1, inplace=False)\n",
    "    (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
    "    (norm1): LayerNorm(512,), eps=1e-0.5, elementwise_affine=True)\n",
    "    (norm2): LayerNorm(512,), eps=1e-0.5, elementwise_affine=True)\n",
    "    (dropout1): Dropout(p=0.1, inplace=False)\n",
    "    (dropout2): Dropout(p=0.1, inplace=False)\n",
    ")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d6f291-30f6-48a3-a319-297d0578dbf8",
   "metadata": {},
   "source": [
    "At the outset, there doesn't seem to be too much to take away from this. It's a fairly standard PyTorch nn.Module with the standard forward() function defined for us. In principle, we could just plug it into our training pipeline and carry on. in fact, this is essentially what we did in Chapter 2. But let's try to understand the exact components of this module.\n",
    "\n",
    "Of particular interest is the MultiheadAttention layer. Most of the other layers, like Dropout, Linear, and LayerNorm, are things you'd expect to see in nontransformer models as well.This particular implementation of he transformer by PyTorch (with no additional configuration parameters), exactly matches the specification of the architecture in the original paper (shown in Figure 7-1) which, coincidentally, is titled \"Attention Is All you Need.\"\n",
    "\n",
    "In short, it's safe to say that the most important component of this Transformer class is the MultiheadAttention layer. So it makes sense to take some time to understand what that is and how it works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "formed-hygiene",
   "metadata": {},
   "source": [
    "## Attention Mechanisms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8824eb-e36f-4aaf-b75c-689242df81ba",
   "metadata": {},
   "source": [
    "An attention mechanism is a layer in a deep neural network. Its job, while still open to interpretation, is to learn long-range, \"global\" features. An attention mechanism acts as what we like to call an \"information router\" that decides what components of the input sequence of embedding vectors contribute to a single output vector. This idea will become more clear as we actually work through the details.\n",
    "\n",
    "We're just as excited to talk about attention as the other couple thousand people that attended NeurIPS within that last year, but before we de, we should mention that an important theme to pay attention to is the computaional complexity of the operations involved.Think about how many dot products/matrix multiplications you see and the size of the tensors involved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "descending-chassis",
   "metadata": {},
   "source": [
    "### Dot Product Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a1493f-17c5-450c-b817-6aeb2176506e",
   "metadata": {},
   "source": [
    "OK, strictly speaking, we don't think we've seen this type of attention actually being applied in real networks. Scaled dot product attention is usually just talked about as a component of the next thing we'll discuss: Multi-Head Self-Attention.\n",
    "\n",
    "The most important question you need to ask in the world of exotic attention mechanisms is this: how, exactly, do you measure the similarity between things? This core idea, shrouded in a veil of linear algebra and bucket-loads of GPUs, is what drives the fundamental behavior of neural nets in NLP today.\n",
    "\n",
    "And the scaled dot product uses probably one of the simplest and most intuitive methods of measureing similarity-the dot product.\n",
    "\n",
    "You should be familiar with this, but let's do a quick recap. The dot product is an operation that takes two vectors, multiples them element-wise, and then adds up the results. This measures similarity becasue if the two vectors that we're \"dot-producting\" have similar components, the product of their elements will be large, and vice versa (in the sense that vectors with dissimilar components will have a small dot product).\n",
    "\n",
    "But the real question is, what exactly are we taking the dot product of? To answer this question, let's focus on how these attention mechanisms are implemented in transformers(see Figure 7-1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf9af7f-1ec0-40a4-bfb9-687a39dd6b83",
   "metadata": {},
   "source": [
    "![image](images/transformer_layer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba00a38e-7bb4-4c32-8c16-767e4c4228c7",
   "metadata": {},
   "source": [
    "A typical transformer takes in sequences of word vectors as input, and at each layer, transforms (and no, we don't think that's how they got their name) them into another sequence of vectors, which we call the hidden representation/state.\n",
    "\n",
    "So at each hidden layer in the network, we have sequences of vectors that we want to \"attend\" over. See Figure 7-2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec6f4c9-77f6-40c9-b726-c3259af20c7c",
   "metadata": {},
   "source": [
    "![image](images/dotproduct_1-768x412.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c95b01c-9294-477d-965b-f383bc5bc6fa",
   "metadata": {},
   "source": [
    "Now, here's the important bit, so pay attention (pun very much intented).\n",
    "\n",
    "What we're going to do is transform each one of these hidden state vectors into three seperate, completely independent vectors-the query, the key, and the value.\n",
    "\n",
    "We do this transformation via a simple matrix multiply, and the dimensions of these vectors are up to us. The only restriction is that the query and key vectors need to have the same dimensions (since we're going to take the dot product between them):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23b32c2-0f6e-47c7-aab8-44f0bb4c7c3f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e8a9723a-8f7e-44e3-a516-3ba766ea0ac2",
   "metadata": {},
   "source": [
    "We then compute the attention weights by taking the dot product of the query vector at each time step as with all of the key vectors, and softmax the result. To do this over all time steps simultaneously, it's more efficient to pack these vectors into a matrix taht performs the multiplicaitons in parallel. The final calculation would look something like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a42910a-c5ee-4fa7-ad84-381e6da08041",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ad68a513-8687-4f65-b615-054351c03a2b",
   "metadata": {},
   "source": [
    "That's not all we can do, though. Since each of the query vectors are independent, we can parallelize across time:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4d8158-b86f-434c-9850-2a5a00490d14",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "85e09b69-ff24-4182-bd90-a1c5d9d8dc89",
   "metadata": {},
   "source": [
    "But why? That's the question we were asking ourselves when reading the transformer paper. Splitting into three vectors seems a little arbirary and complicated. Like, why not two or four?\n",
    "\n",
    "Base on just the naming, it seems like the intuition here is rooted in databases.Think about how this would work in regular old Python dictionaries, no neural nets involved.\n",
    "\n",
    "You have a large sequence of key-value pairs. That's the dictionary structure. I could look something like this:\n",
    "\n",
    "```\n",
    "sentence = {\n",
    "    \"word_1\": \"Squirtle\"\n",
    "    \"word_2\": \"is\"\n",
    "    \"word_3\": \"the\"\n",
    "    \"word_4\": \"greatest\"\n",
    "    \"word_5\": \"Pokemon\"\n",
    "    \"word_6\": \"ever\"\n",
    "}\n",
    "```\n",
    "\n",
    "Now I know what you're thinking-\"Puh-lease! We all know Squirtle doesn't stand a sliver of a chance against Charizard.\"\n",
    "\n",
    "Well, We beg to differ. Deal with it.\n",
    "\n",
    "When you want to get a value from the dictionary, you'd use a query that looks something like this:\n",
    "\n",
    "```\n",
    "a = sentence['word_3']\n",
    "```\n",
    "\n",
    "And what Python would do, behind the scenes, is compare your query word_3 against all the possible keys in the sentence dictionary. It would then return the value and store it in a vairable.\n",
    "\n",
    "What we're doing with dot product attention is similar. The query vector represents, in some abstract sense, what the current word is looking for. The key associated with each word kind of represents what each word has to offer. The value vector contains the information that the query vector was looking for. But we know that sounds super abastract, so let us show you an example.\n",
    "\n",
    "Consider the following sentence:\n",
    "\n",
    ">Mario is short, but the he can jump super high.\n",
    "\n",
    "Now say our transformer is currently working on the word \"he,\" and it's trying to propagatge it to the next layer in the network. The query vector here might be something that's looking for a name or a person to clarify what exactly the pronoun \"he\" is referring to. So, the transformer takes the query vector for \"he\" and computes dot products of this query vector with the key vectors of every other word in the sentence. Each of these dot products generates a sort of alignment socre that measures how much the query and key match.\n",
    "\n",
    "As it does it, the key vector corresponding to \"Mario\" is likely to light up, and will generate the largest alignment score. This indicates to the network that there's something interesting going on there, and the network should pay attention (see what we did there?).\n",
    "\n",
    "But the job is'net done yet. Once the transformer calculates all the alignment socres between \"he\" and the other words in the sentence, it passes these scores through a softmax, to generate a nice distribution. You can interpret the score more naturally: 0 would tell you that there's little connection between the words, and a 1 would tell you that there's a near-perfect alignment.\n",
    "\n",
    "Remember that each word also has an associated value vector, which, in our picture, is supposed to represent tha actually meaningful connect of the world, just like in the case of values in Python dictionaries. Unlike Python dictionaries, however, each query doesn't return a single result. Instead, the transformer takes the normalized alignment scores that we calcuated for each word and uses them to perform a wighted sum over all athe value vectors. The reason for this is somewhat simple-say the jump very high.\" Here, the query for \"they\" is not just looking for a single word, but every possible person that fits into this group.\n",
    "\n",
    "Creating a distribution of alignment scores now lets us pick up different parts of the sentence in different amounts! Oversimplifying a bit, you can imagine that the normalzied alignement scores for the words \"Mario\" and \"Luigi\" are 0.5 and 0 for all other words.\n",
    "\n",
    "The transformer has now attented (seriously, is that even a word?) over the sentence, and has created a vector for a particular word (\"he\" and \"they\", in our example) that encapsulates how the relevant parts of the sentence related to this word in the grand shceme of things.\n",
    "\n",
    "You'd now repeat this process for every word in the sentence, thereby getting another sequence of vectors to pass on to deeper layers in the network.\n",
    "\n",
    "When we're computing the so-called \"self-attenction\" in the encoder parts of the transformer, the following hidden states are used to calculate:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddff24d6-a83b-4b97-83c9-dcb0f99b396b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ecb3e81d-1284-40aa-816b-8e0caf8d8e39",
   "metadata": {},
   "source": [
    "These all come from the sequence in that layer of the encoder. The same is true for self-attention in the decoder.\n",
    "\n",
    "There's another one, though-the attention layer used in the decoder that uses the decoder hidden representation for the queries, and the encoder hidden representation for the keys and values. This allows the decoder to attend over all previous encoder hidden representations, which is useful in tasks like machine translation. You wouldn't want you French translator to starts spewing out gibberish without actually reading the whole sentence in English first.\n",
    "\n",
    "You can visualize the self-attention layer in Figure 7-2 and the entire layer put together in Figure 7-1. Stack a few of these layers on top of one another, and boom! You've (almost) got yourself a transformer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blocked-acoustic",
   "metadata": {},
   "source": [
    "### Scaled Dot Product Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78dd6d02-4ccc-410d-a095-f3b5d534ca61",
   "metadata": {},
   "source": [
    "There's a minor problem with this, though. Although dot products are really fast, cool, and all that, when the size of the vectors are large, the dot product can get pretty big.\n",
    "\n",
    "To see what we mean, consider two random vectors. Instead of just talking about it, though, let us show you some actual computations from NumPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dimensional-seventh",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "breeding-middle",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_dots = [\n",
    "    np.dot(np.random.randn(10),\n",
    "           np.random.randn(10))\n",
    "    for i in range(100)]\n",
    "np.mean(np.absolute(small_dots))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bbf7de-b633-47da-b539-fd23a5a183db",
   "metadata": {},
   "source": [
    "What we just did there was generate two random arrays of size 10 and take the dot product of them. Just to be sure, we repeated this 100 times and calculated the average magnitude of the dot product to make sure that we're not getting a random outlier.\n",
    "\n",
    "And so what the value was around 2.74. How's that usefull? Well, let's try the same thing with arrays of size 10,000:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loved-edmonton",
   "metadata": {},
   "outputs": [],
   "source": [
    "large_dots = [np.dot(np.random.randn(10000),\n",
    "                     np.random.randn(10000))\n",
    "              for i in range(100)]\n",
    "np.mean(np.absolute(large_dots))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb012f6d-1319-4bc1-a63d-84379c37914c",
   "metadata": {},
   "source": [
    "Ok. That's a lot bigger. But think about it-since we're using dot products to measure alignment, something is clearly wrong here. In both cases, we generated purely random vectors, so ideally, their alignment scores should be similar.\n",
    "\n",
    "But since the components are chosen from a standard normal distribution with mean 0 and variance 1, an n-dimensional vector will have a variance of n (you get his by adding up the variances of the components, and if you're going to be pedantic, it's the trace of the covariance matrix of the vector, but that's way too long a name).\n",
    "\n",
    "To correct for this and ensure that vectors of any dimensionality have roughly the same alignment scores, we'll scale our previous attention mechanism similar to how you'd normalize to unit variance in statistics.\n",
    "\n",
    "The new, corrected attention mechanism would be:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extensive-plastic",
   "metadata": {},
   "source": [
    "### Multi Head Self Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab6ed50-d35e-46e5-8a01-222d701b106b",
   "metadata": {},
   "source": [
    "Here's something you might find interesting: the two attention mechanisms that we just discussed, and the one we're about to show you now, all came from the same paper-\"Attention Is All You Need\" (aka the transformer paper). Pretty cool, huh?\n",
    "\n",
    "Anyway, the next thing we can do is try to split up our attention mechanism into many smaller attention mechanisms (with an \"s\"). Why would wnat to do this? A good way to illustrate the rationale is through a popular attention test video (https://youtu.be/vJG698U2Mvo)\n",
    "\n",
    "Now you've probably seen that video before (if you haven't, surprise!), and you know why it's so hard to spot the gorilla on your first pass-it's easier and more national to pay attention to one thing at a time. In this case, that's baskeball passes, since that's what the video asks you to look for. If instead, you were asked to look for a gorilla, it probably would have been easier to find the gorilla.\n",
    "\n",
    "Attention mechnisms kind of work in the same way. There's a lot of stuff to pay attention to and keep track of in language, like pronouns, as we discussed earlier (\"Mario is short, but he can jump super high\"), but also other things, like where the main characters are going in physical space (\"Mario went to the flower store and then to the gym, where he did 50 squats\")\n",
    "\n",
    "Having one set of queries, keys, and values do all that work might be a bit too much, and they might miss out on the occational gorilla, just like you probably did.\n",
    "\n",
    "Multi-head attention mechanisms try to fix this issue by independently applying the attention mechanism multiple times on the same sequence in a single pass. In terms of the gorilla video, this would be like having your buddy watch the video with you. One of you could pay attention to the passes, while the other could look for gorillas, thereby increasing the overall attention capablilities.\n",
    "\n",
    "Crucially, the query key and value matrices need to be different, otherwise redoing the whole attention thing multiple times would just be a waste of computation (asking your friend to look for passes while you also look for passes).\n",
    "\n",
    "To create variety in the queries, keys, and values, the transformer network simply uses multiple sperate weight matrices to transform the input into multiple queiries, keys, and values:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b3b4d4-9c01-4e47-ac1c-5430d7e9ccab",
   "metadata": {},
   "source": [
    "Here, n is the parameter you set, and it's called the number of heads. It represents how many different attention computations are being done on the same sequence. You can think of it as the number of people you invite over to watch that gorilla video with you.\n",
    "\n",
    "Are you tired of that analogy yet? Don't worry, we're almost done with it.\n",
    "\n",
    "Each one of these \"heads\" performs the scaled dot product attention calculation independently (and, crucially, in parallel):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a189c42-62f4-4be5-87da-2ca4a0101ba6",
   "metadata": {},
   "source": [
    "At the end of all this number-crunching, we're going to be left n different output vectors per spot in the sequence, corresponding to the outputs from each of the attention heads. But since the next layer needs a sequence of vectors (and a sequence of n vectors), the transformer concatenates the output from the multiple attention heads and passes it through another learned linear transform to make the dimenstions work right:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718fdd2f-8ee3-479b-8f80-9178b8aee807",
   "metadata": {},
   "source": [
    "A sequence of these new concatenated and transformed z vectors is what gets passed on to the next layer of the transformer.\n",
    "\n",
    "That's difinitely a lot of linear algebra to take in at once, so go through it slowly again to make sure you actually get it. In particular, visualizing Multi-Head Self-Attention is probably the best way to understand how it works. Jay Alammar has an exellent set of article on this (https://jalammar.github.io/illustrated-gpt2/), and we hightly encourage you to take a look at the viusalizations presented there."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vanilla-fruit",
   "metadata": {},
   "source": [
    "### Adaptive Attention Span"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7486b1-9607-43f0-a92b-137618909f04",
   "metadata": {},
   "source": [
    "OK, we're finally moving on to some (relatively) newer and cooler stuff. In 2019, some cool people at Facebook AI Research asked a really cool question-what if we could get transformer to learn what to pay attention to?\n",
    "\n",
    "But isn't that what transformers aleardy do? Isn't this the entire point of the attention mechanism?\n",
    "\n",
    "Well, yes. But there's also another very important thing we haven't talked about-computaional cost. You see, adding an attention mechanism isn't cheap. If you have n words in a batch/sentence, it would taken $n^2$ dot products (per layer) to compute each of the attention weights across all the tokens in the sequence. This is because you have to take the dot product of each of the $n$ query vectors with each of the n keys.\n",
    "\n",
    "As you can tell, this can blow up pretty fast. If you had, say, 50 takens/words in your batch/sentence, then there are at least $50^2 = 2,500$ dot products to compute. But simply by increasing the number of tokens by two, to 52, you'd now have more than $52^2 = 2704$ dot products to compute. That's about 200 more dot products justfor adding two extra tokens per batch, and that's not even factoring in multi-headed attention!\n",
    "\n",
    "Of course, one could question if we really need to compute attention over every single token every single time. It sees a little excessive. Especially in character-level or subword-level models, where some of the attention heads might simply be looking at the last few tokens to try and fit characters or subwords together into words. But then make every head look at only the last few tokens.\n",
    "\n",
    "The way we (or in this case, the Facebook team; we are just reaping the benefits of their work) strike a balance is by having some heads attend over a larger set of tokens, and have some heads attend over only the last few tokens.\n",
    "\n",
    "There's one term we'll introduce here: attend span. This simply refers to how many previous tokens the model is attending to. So if a head has an attention span of 5, this means that head runs an attention mechaniszm over the last 5 tokens from the current position in the sequence.\n",
    "\n",
    "So how do we decide the attention span for each of the heads? Typically, this would involve experiments, plots, some hand-waving, and a fair bit of guesswork. But what makes adaptive attention span so cool is that each head can learn its own attention span though the training process!\n",
    "\n",
    "This idea is really cool because it takes something that would have been a hyperparameter, the number of tokens to attend over, and makes it a simple parameter that can be automatically tuned through backprop.\n",
    "\n",
    "Here's the main issue at hand: the number of tokens that each head looks at, also called the attention space, is an interger, and therefore can't be differentiated. Being nodifferentiable means that you can't really learn that parameter though training. So instead, the research team had to come up with a clever way to get a differentiable version of the attention span.\n",
    "\n",
    "They did this by creating something called a masking function, which takes in th distance between tokens and outputs a value between 0 and 1. In the paper, they define the masking function like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a642c0f7-9daa-40d3-ac0f-b784dbf6e806",
   "metadata": {},
   "source": [
    "Which we guess looks a little weird. But the plot is actually pretty clear and simple, as shown in Figure 7-3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029dcd93-9c7b-4486-b1ef-dbff6f69141f",
   "metadata": {},
   "source": [
    "So the intuition here is that if the distance x between two tokens is large enough, the value of $m_z(x)$ will be zero, which means we don't do the attention computation between those two tokens.\n",
    "\n",
    "Since this $m_z(x)$ fuction is smooth, we can get its gradient and tune the value of $z$ for each attention head. With a larger $z$, the attention head would look across more tokens, and vice versa. $R$ is a hyperparamter that controls the smoothness of that ramp section you see on the plot.\n",
    "\n",
    "But most importantly, the adaptive attention span transformer has some pretty cool results. It achives state-of-the-art performance on the enwik8 dataset using considerably less memory and FLOPs than other transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tired-helicopter",
   "metadata": {},
   "source": [
    "### Persistent Memory/All-Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stuffed-trigger",
   "metadata": {},
   "source": [
    "### Product-Key Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "binding-vacuum",
   "metadata": {},
   "source": [
    "## Transformers for Computer Vision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77022a5d-a34f-4c1f-87a1-4e537eb4d3ce",
   "metadata": {},
   "source": [
    "# Fine-tuning BERT for sentiment analysis\n",
    "\n",
    "## Contents\n",
    "1. 학습데이터 확인\n",
    "2. 보조함수 확인\n",
    "3. SenitmentClassifier 확인\n",
    "4. Analyser 확인\n",
    "5. 학습 과정 확인\n",
    "6. tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a19e501-b550-427c-b4dd-e328536b7133",
   "metadata": {},
   "source": [
    "## 학습 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e7b518-8c06-4a6a-a661-1efc6b058d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install torch\n",
    "!pip3 install transformers\n",
    "from typing import List, Tuple\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "DATA: List[Tuple[str, int]] = [\n",
    "    # 긍정적인 문장 - 1\n",
    "    (\"난 너를 좋아해\", 1),\n",
    "    # --- 부정적인 문장 - 레이블 = 0\n",
    "    (\"난 너를 싫어해\", 0)\n",
    "]\n",
    "\n",
    "TESTS = [\n",
    "    \"나는 자연어처리가 좋아\",\n",
    "    \"나는 자연어처리가 싫어\",\n",
    "    \"나는 너가 좋다\",\n",
    "    \"너는 참 좋다\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1c9831-f8a8-477e-b7b5-9277b590a3a5",
   "metadata": {},
   "source": [
    "## 보조함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9ae08b-1d45-4d5c-a6a2-00d404deb786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cuda를 사용할 수 있는지를 체크, 사용가능하다면 cuda로 설정된 device를 출력.\n",
    "def load_device() -> torch.device:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    return device\n",
    "\n",
    "\n",
    "# 텐서를 구축하는 부분 - X\n",
    "def build_X(sents: List[str], tokenizer: BertTokenizer, device: torch.device) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    return X (N, 3, L).  N, 0, L -> input_ids / N, 1, L -> token_type_ids / N, 2, L -> attention_mask\n",
    "    \"\"\"\n",
    "    encodings = tokenizer(text=sents,\n",
    "                          add_special_tokens=True,\n",
    "                          return_tensors='pt',\n",
    "                          truncation=True,\n",
    "                          padding=True)\n",
    "    return torch.stack([\n",
    "        encodings['input_ids'],\n",
    "        encodings['token_type_ids'],\n",
    "        encodings['attention_mask']\n",
    "    ], dim=1).to(device)\n",
    "\n",
    "# 텐서를 구축하는 부분 - y\n",
    "def build_y(labels: List[int], device: torch.device) -> torch.Tensor:\n",
    "    return torch.FloatTensor(labels).unsqueeze(-1).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e373fc-a77b-4f7a-b9b1-cf7e2ed16197",
   "metadata": {},
   "source": [
    "## Sentiment Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d5389f-b169-4d33-8dc6-28d0bb54e2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentClassifier(torch.nn.Module):\n",
    "    def __init__(self, bert: BertModel):\n",
    "        super().__init__()\n",
    "        self.bert = bert\n",
    "        self.hidden_size = bert.config.hidden_size\n",
    "        # TODO 1\n",
    "        self.W_hy = torch.nn.Linear(..., ...)\n",
    "\n",
    "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        :param X: (N, 3, L)\n",
    "        :return: H_all (N, L, H)\n",
    "        \"\"\"\n",
    "        input_ids = X[:, 0]\n",
    "        token_type_ids = X[:, 1]\n",
    "        attention_mask = X[:, 2]\n",
    "        H_all = self.bert(input_ids, token_type_ids, attention_mask)[0]\n",
    "        return H_all\n",
    "\n",
    "    def predict(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        :param X:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # TODO 2\n",
    "        H_all = self.forward(X)\n",
    "        H_cls = ...\n",
    "        y_hat = ...  # (N, H) * (H, 1) -> (N, 1)\n",
    "        return torch.sigmoid(y_hat)\n",
    "\n",
    "    def training_step(self, X: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "        # TODO 3\n",
    "        y_hat = self.predict(X)\n",
    "        loss = ...\n",
    "        return loss.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a8091b-e8bc-4a94-ad31-cb8ef645869e",
   "metadata": {},
   "source": [
    "## Analyser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e457d3aa-ab21-4d18-909b-a6ede707cd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Analyser:\n",
    "    \"\"\"\n",
    "    Bert 기반 감성분석기.\n",
    "    \"\"\"\n",
    "    def __init__(self, classifier: SentimentClassifier, tokenizer: BertTokenizer, device: torch.device):\n",
    "        self.classifier = classifier\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "\n",
    "    def __call__(self, text: str) -> float:\n",
    "        X = build_X(sents=[text], tokenizer=self.tokenizer, device=self.device)\n",
    "        y_hat = self.classifier.predict(X)\n",
    "        return y_hat.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ba24e5-8478-442f-9de0-5d18cb35c8a5",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4584485e-ba13-473c-a64c-8e7add28d19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사전학습된 버트 모델을 로드\n",
    "tokenizer = BertTokenizer.from_pretrained('beomi/kcbert-base')\n",
    "bert = BertModel.from_pretrained('beomi/kcbert-base')\n",
    "\n",
    "# --- have a look at the config --- #\n",
    "print(bert.config)\n",
    "print(bert.config.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771333dd-2e87-41a9-8fe3-2eb3097437b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- hyper parameters --- #\n",
    "EPOCHS = 20\n",
    "LR = 0.0001\n",
    "\n",
    "\n",
    "device = load_device()\n",
    "print(device)\n",
    "\n",
    "# --- build the dataset --- # \n",
    "sents = [sent for sent, _ in DATA]\n",
    "labels = [label for _, label in DATA]\n",
    "X = build_X(sents, tokenizer, device)\n",
    "y = build_y(labels, device)\n",
    "\n",
    "# --- instantiate the classifier --- #\n",
    "classifier = SentimentClassifier(bert)\n",
    "classifier.to(device)  # 모델도 gpu에 올리기. \n",
    "optimizer = torch.optim.Adam(classifier.parameters(), lr=LR)  # 최적화 알고리즘을 선택.\n",
    "\n",
    "# --- 학습시작 --- #\n",
    "for epoch in range(EPOCHS):\n",
    "    loss = classifier.training_step(X, y)\n",
    "    loss.backward()  # 오차 역전파\n",
    "    optimizer.step()  # 경사도 하강\n",
    "    optimizer.zero_grad()  # 기울기 축적방지\n",
    "    print(f\"epoch:{epoch}, loss:{loss.item()} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3d7d13-4b45-4cbe-ba91-02c0670257ba",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9040b5-d9f0-4e3e-b920-e8382b9e124d",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.eval()\n",
    "analyser = Analyser(classifier, tokenizer, device)\n",
    "\n",
    "for sent in TESTS:\n",
    "    print(sent, \"->\", analyser(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identical-indicator",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
