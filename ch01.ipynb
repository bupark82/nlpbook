{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[[ch01]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to NLP "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "당신의 컴퓨터가 무엇을 할 수 있다고 생각합니까?\n",
    "\n",
    "이메일을 보여주고 있습니까? 일부 파일을 수정할 수 있습니까? 엑셀 시트를 돌려볼까요?\n",
    "\n",
    "하지만 컴퓨터가 읽을 수 있다고 말하면 어떻게 될까요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline('sentiment-analysis')\n",
    "classifier('I am reading the greatest NLP book ever!')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... 쓰기:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_generator = pipeline(\"text-generation\")\n",
    "text_generator(\"Welcome to the\", max_length=5, do_sample=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그리고, 가장 인상적으로, 이해한다는 것:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = pipeline(\"question-answering\")\n",
    "context = \"\"\"\n",
    "Natural language processing (NLP) is a subfield of linguistics,\n",
    "computer science, and artificial intelligence concerned with the\n",
    "interactions between computers and human language, in particular\n",
    "how to program computers to process and analyze large amounts of\n",
    "natural language data. The result is a computer capable of\n",
    "\"understanding\" the contents of documents, including the contextual\n",
    "nuances of the language within them. The technology can then accurately\n",
    "extract information and insights contained in the documents as well\n",
    "as categorize and organize the documents themselves.\n",
    "\"\"\"\n",
    "nlp(question=\"What is NLP?\", context=context)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "한때 먼 미래의 환상이었던 것이 여기에 있을 뿐만 아니라 컴퓨터와 인터넷 연결만 있으면 누구나 접근할 수 있습니다. 자연어를 이해하고 의사소통하는 능력, 인류가 존재하는 과정에서 발달시킨 가장 소중한 자산 중 하나, 이제 기계에서 하는 것이 실행 한다는 것입니다.\n",
    "\n",
    "\"물론!\" 당신에게 말합니다. \"기술은 항상 좋아지고 있으며 음성 인식과 Google 번역은 오랜 세월 동안 사용되어 왔습니다!\"\n",
    "\n",
    "그러나 불과 5년 전만 해도 \"NLP\"는 실제 프로덕션 코드베이스보다 TechCrunch 기사에 더 적합한 것이었습니다. 지난 3년 동안 우리는 이 분야에서 기하급수적인 성장을 보았고 *오늘날* 생산에 배치되는 모델은 과거의 가장 모호한 연구 순위표보다 훨씬 우수합니다.\n",
    "\n",
    "그러나 우리는 우리 자신보다 앞서 나가고 있습니다. 더 깊이 파고들기 전에 해당 분야에 대한 높은 수준의 개요부터 시작하겠습니다. 기본 사항을 다룬 후에 더 고급 주제를 소개합니다. 우리의 목표는 책이 끝날 때쯤이면 세상에 _실제 가치_를 추가하는 _실제 응용 프로그램_을 구축할 수 있도록 장별로 NLP 작업에 대한 직관과 경험을 쌓도록 돕는 것입니다.\n",
    "\n",
    "이 장의 전반부에서는 NLP를 정의하고 이 기술의 일부 상용 응용 프로그램을 살펴보고 1950년대에 시작된 이래 이 분야가 어떻게 발전해 왔는지 살펴볼 것입니다.\n",
    "\n",
    "후반부에서는 기업에서 널리 사용되는 고성능 NLP 라이브러리를 소개하고 이를 사용하여 기본 NLP 작업을 수행합니다. 이러한 작업은 초보적이지만 함께 결합하면 컴퓨터가 복잡한 방식으로 자연어 데이터를 처리하고 분석하여 챗봇 및 보이스봇과 같은 놀라운 상용 애플리케이션을 가능하게 합니다.\n",
    "\n",
    "어떤 면에서 기계가 언어를 처리하는 방법을 학습하는 과정은 유아가 단어를 중얼중얼 더듬으면서 언어를 배우기 시작하고 나중에 완전한 문장과 단락으로 말하는 것과 유사합니다. 이 책을 진행하면서 이 장에서 다루는 기본 NLP 작업을 기반으로 구축할 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is NLP?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "자연어 처리가 무엇인지 정의하는 것으로 시작하겠습니다. 다음은 NLP가 일반적으로 정의되는 방식입니다.\n",
    "\n",
    "```asciidoc\n",
    "[quote, Wikipedia]\n",
    "____\n",
    "자연어 처리(NLP)는 컴퓨터와 인간(자연) 언어 간의 상호 작용, 특히 대량의 자연어 데이터를 처리하고 분석하도록 컴퓨터를 프로그래밍하는 방법과 관련된 언어학, 컴퓨터 과학, 정보 공학 및 인공 지능의 하위 분야입니다. \n",
    "\n",
    "자연어 처리의 과제는 종종 음성 인식, 자연어 이해 및 자연어 생성과 관련됩니다.\n",
    "____\n",
    "```\n",
    "\n",
    "이 정의를 풀어봅시다. \"자연어\"라고 하면 프로그래밍 언어가 아니라 \"인간 언어\"를 의미합니다. 자연어는 텍스트 데이터뿐만 아니라 음성 및 오디오 데이터를 의미합니다.\n",
    "\n",
    "좋습니다. 하지만 이제 컴퓨터가 대량의 텍스트, 음성 및 오디오 데이터를 처리할 수 있다면 어떨까요? 이것이 왜 그렇게 중요한가요?\n",
    "\n",
    "언어가 없는 세상을 잠시 상상해 보세요. 문자나 음성으로 어떻게 소통할까요? 책을 읽고, 음악을 듣고, 영화와 TV 쇼를 어떻게 이해할까요? 우리가 알고 있는 삶은 더 이상 존재하지 않을 것입니다. 우리는 정보를 시각적으로 처리할 수 있지만 서로 지식을 공유하거나 의미 있는 방식으로 의사소통할 수 없는 원시인 시대에 갇혀 있을 것입니다. 각주:[인류 역사의 주요 도약 중 하나는 인간(일명 \"자연\") 언어의 형성입니다. 이를 통해 인간은 서로 의사소통하고, 그룹을 형성하고, 개인이 아닌 집단 단위로 활동할 수 있었습니다. ]\n",
    "\n",
    "마찬가지로 기계가 숫자 및 시각적 데이터로만 작업할 수 있지만 자연어를 처리할 수 없다면 기계는 실제 세계에서 사용할 수 있는 응용 프로그램의 수와 다양성이 제한될 것입니다. 자연어 처리 능력이 없다면 기계는 일반적인 인공 지능이나 오늘날 인간 지능과 유사한 어떤 것에 접근할 수 없습니다.\n",
    "\n",
    "다행스럽게도 기계는 이제 자연어 데이터를 합리적으로 잘 처리할 수 있습니다. 컴퓨터가 자연어 데이터로 작업할 수 있는 상대적으로 새로운 기능으로 인해 어떤 상용 응용 프로그램이 가능한지 살펴보겠습니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 인기 있는 애플리케이션"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP의 발전으로 인해 기계는 최소한 기초적인 방식으로 광범위한 자연어 작업을 처리할 수 있습니다. 오늘날 NLP의 몇 가지 일반적인 응용 프로그램은 다음과 같습니다.\n",
    "\n",
    "- **기계 번역:** 기계 번역은 사람의 개입 없이 기계를 사용하여 한 언어에서 다른 언어로 번역하는 프로세스입니다. 지금까지 가장 인기 있는 예는 100개 이상의 언어를 지원하고 매일 5억 명이 넘는 사람들에게 서비스를 제공하는 Google 번역입니다. 2006년 처음 출시되었을 때 Google 번역의 성능은 지금보다 현저하게 나빴습니다. 오늘날 성능은 인간 전문가 수준에 빠르게 접근하고 있습니다.각주:[자세한 내용은 https://www.nytimes.com/2016/12/14/magazine/the-great-ai-awakening에서 2016년 The New York Times 기사를 참조하세요. .html[Google의 신경 기계 번역].]\n",
    "\n",
    "- **음성 인식:** 충격적으로 들릴 수 있지만 음성 인식 기술은 50년이 넘었습니다. 음성 인식 소프트웨어 중 어느 것도 딥 러닝의 등장에 힘입어 아주 최근까지 성능이 좋거나 주류가 되지 못했습니다. 오늘날 Amazon Alexa, Apple Siri, Google Assistant, Microsoft Cortana, 자동차의 디지털 음성 비서 및 기타 소프트웨어는 이제 소프트웨어가 정보를 실시간으로 처리하고 대부분 합리적인 방식으로 대답하십시오. 불과 15년 전만 해도 그러한 기계가 음성을 인식하고 일관된 방식으로 응답하는 능력은 형편없었습니다.\n",
    "\n",
    "- **질문 응답:** 이러한 디지털 어시스턴트가 질문을 하는 인간에게 유쾌한 경험을 제공하려면 음성 인식이 작업의 전반부일 뿐입니다. 소프트웨어는 (a) 음성을 인식하고 (b) 음성이 인식되면 적절한 응답을 검색해야 합니다. 이 후반부는 질문 응답(QA)으로 알려져 있습니다.\n",
    "\n",
    "- **텍스트 요약:** 인간이 매일 하는 가장 일반적인 작업 중 하나는 특히 사무직에서 긴 형식의 문서를 읽고 내용을 요약하는 것입니다. 기계는 이제 이 요약을 수행하여 긴 텍스트 문서의 짧은 요약을 생성할 수 있습니다. 텍스트 요약은 인간의 읽기 시간을 줄여줍니다. 매일 많은 텍스트를 분석하는 인간(예: 변호사, 준법률가, 비즈니스 분석가, 학생 등)은 긴 형식 문서의 기계 생성 짧은 요약을 통해 이동한 다음 요약을 기반으로 관련 문서를 선택할 수 있습니다. 더 자세히 읽으십시오.\n",
    "\n",
    "- **Chatbots:** 최근에 웹사이트를 정독한 적이 있다면 점점 더 많은 사이트에 인간 사용자를 참여시키기 위해 자동으로 참여하는 챗봇이 있다는 것을 깨달았을 것입니다. 챗봇은 일반적으로 친절하고 위협적이지 않은 방식으로 사람을 맞이한 다음 사이트 방문의 목적과 의도를 측정하기 위해 사용자에게 질문을 합니다. 그런 다음 챗봇은 사람의 개입 없이 자동으로 질문에 답하려고 시도합니다. 이러한 챗봇은 이제 디지털 고객 참여를 자동화하고 있습니다.\n",
    "\n",
    "- **텍스트 음성 변환 및 음성 텍스트 변환:** 이제 소프트웨어에서 텍스트를 고품질 오디오로 매우 쉽게 변환할 수 있습니다. 예를 들어 Google Cloud Text-to-Speech는 30개 이상의 언어로 180개 이상의 음성으로 텍스트를 사람과 유사한 음성으로 변환할 수 있습니다. 마찬가지로 Google의 Cloud Speech-to-Text는 120개 이상의 언어로 오디오를 텍스트로 변환하여 진정한 글로벌 서비스를 제공할 수 있습니다.\n",
    "\n",
    "- **Voicebots:** 10년 전에는 자동 음성 에이전트가 투박했습니다. 인간이 상당히 제한적인 방식으로 응답하지 않는 한(예: 예 또는 아니오 유형 응답) 전화기의 음성 에이전트는 정보를 처리할 수 없습니다. 이제 Voiq에서 제공하는 것과 같은 AI 보이스봇은 영업, 마케팅 및 고객 성공 팀에 대한 통화를 늘리고 자동화하는 데 도움이 될 수 있습니다.\n",
    "\n",
    "- **텍스트 및 오디오 생성:** 몇 년 전 텍스트 생성은 템플릿과 규칙 기반 시스템에 의존했습니다. 이것은 적용 범위를 제한했습니다. 이제 소프트웨어는 기계 학습을 사용하여 텍스트와 오디오를 생성할 수 있으므로 응용 범위가 상당히 넓어집니다. 예를 들어 Gmail은 이제 사용자가 작성한 이전 문장을 기반으로 전체 문장을 제안할 수 있으며 사용자가 입력하는 즉시 이를 수행할 수 있습니다. 자연어 생성은 짧은 텍스트(부분 문장)에 가장 적합하지만 머지않아 그러한 시스템이 상당히 좋은 긴 형식의 콘텐츠를 생성할 수 있게 될 것입니다. 자연어 생성의 인기 있는 상업적 응용 프로그램은 데이터베이스 및 데이터 세트의 텍스트 요약을 생성하는 데이터-텍스트 소프트웨어입니다. 데이터-텍스트 소프트웨어에는 데이터 분석과 텍스트 생성이 포함됩니다. 이 분야의 회사에는 Narrative Science 및 Automated Insights가 포함됩니다.\n",
    "\n",
    "- **감정 분석:** 소셜 미디어 콘텐츠가 폭발적으로 증가함에 따라 고객 감정 분석을 자동화하고 트윗, 게시물 및 댓글을 분석하여 긍정적 vs. .슬픈 대 행복한. 이러한 소프트웨어는 감정 AI라고도 합니다.\n",
    "\n",
    "- **정보 추출:** NLP의 한 가지 주요 과제는 구조화되지 않은 문서 및/또는 반구조화된 문서에서 구조화된 데이터를 생성하는 것입니다. 예를 들어 명명된 엔터티 인식 소프트웨어는 주류 뉴스와 같은 긴 형식의 텍스트에서 사람, 조직, 위치, 날짜 및 통화를 추출할 수 있습니다. 정보 추출에는 엔티티 간의 관계를 식별하는 관계 추출도 포함됩니다.\n",
    "\n",
    "지난 10년 동안 기업의 NLP 애플리케이션의 수는 음성 인식과 질문 및 답변에서부터 자체적으로 자연어를 생성할 수 있는 보이스봇 및 챗봇에 이르기까지 폭발적으로 증가했습니다. 이것은 수십 년 전에 현장이 있었던 곳을 감안할 때 상당히 놀랍습니다.\n",
    "\n",
    "NLP의 현재 진행 상황을 파악하기 위해 1950년의 기원부터 NLP가 어떻게 진행되었는지 살펴보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### History "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "자연어 처리 분야는 거의 70년 동안 존재해 왔습니다. 아마도 가장 유명한 것은 Alan Turing이 1950년에 Turing 테스트를 개발하여 이 분야의 토대를 마련한 것입니다. Turing 테스트는 인간과 구별할 수 없는 지능을 보여주는 기계의 능력을 테스트하는 것입니다. 기계가 튜링 테스트를 통과하려면 기계는 인간 평가자가 응답이 인간에 의해 생성되었는지 기계에 의해 생성되었는지 알 수 없도록 인간과 유사한 응답을 생성해야 합니다(즉, 기계의 응답은 인간의 품질임). .footnote:[[Turing 테스트](https://en.wikipedia.org/wiki/Turing_test)에 대한 자세한 내용은 Wikipedia 문서를 참조하세요.]\n",
    "\n",
    "튜링 테스트는 당시 초기 인공 지능 분야에서 중요한 논쟁을 불러일으켰고 연구자들은 오늘날까지 계속되는 검색인 튜링 테스트를 통과할 수 있는 기계의 빌딩 블록 역할을 할 자연어 처리 모델을 개발하도록 자극했습니다.\n",
    "\n",
    "더 넓은 인공 지능 분야와 마찬가지로 NLP는 과대 광고 주기에서 AI 겨울에 이르기까지 많은 호황과 불황을 겪었습니다. 1954년에 Georgetown University와 IBM은 60개 이상의 러시아어 문장을 영어로 자동 번역할 수 있는 시스템을 성공적으로 구축했습니다. 당시 Georgetown University의 연구원들은 기계 번역이 3~5년 내에 해결될 문제라고 생각했습니다. 미국에서의 성공은 또한 소련이 유사한 노력을 시작하도록 자극했습니다. Georgetown-IBM의 성공과 냉전 정신이 결합되어 초기에 NLP에 대한 자금 지원이 증가했습니다.\n",
    "\n",
    "그러나 1966년까지 진전이 정체되었고 컴퓨터 언어학의 진전을 평가하기 위해 설립된 미국 정부 기관인 자동 언어 처리 자문 위원회(ALPAC로 알려짐)는 냉정한 보고서를 발표했습니다. 이 보고서는 기계 번역이 사람의 번역보다 비싸고 정확하지 않으며 느리며 가까운 장래에 사람 수준의 성능에 도달할 가능성이 낮다고 밝혔습니다. 이 보고서로 인해 기계 번역 연구에 대한 자금이 감소했습니다. 보고서 이후 해당 분야의 연구는 거의 10년 동안 거의 중단되었습니다.\n",
    "\n",
    "이러한 좌절에도 불구하고 NLP 분야는 1970년대에 다시 등장했습니다. 1980년대까지 계산 능력은 크게 증가했고 비용은 충분히 낮아져 전 세계의 더 많은 연구자들에게 이 분야를 개방했습니다.\n",
    "\n",
    "1980년대 후반에 IBM의 Thomas J. Watson 연구 센터의 연구원들이 이끄는 최초의 통계적 기계 번역 시스템이 출시되면서 NLP가 다시 주목을 받았습니다. 통계적 기계 번역이 등장하기 전에 기계 번역은 인간이 손으로 만든 언어 규칙에 의존했습니다. 이러한 시스템을 규칙 기반 기계 번역이라고 합니다. 규칙은 기계 번역 시스템이 일반적으로 저지르는 실수를 수정하고 제어하는 ​​데 도움이 되지만 그러한 규칙을 만드는 것은 힘들고 힘든 과정이었습니다. 그 결과 기계 번역 시스템도 불안정해졌습니다. 기계 번역 시스템이 규칙이 개발되지 않은 엣지 케이스 시나리오에 직면하면 기계 번역 시스템이 실패하고 때로는 심각하게 실패합니다.\n",
    "\n",
    "통계적 기계 번역은 사람이 직접 만든 규칙의 필요성을 줄이는 데 도움이 되었습니다. 통계적 기계 번역은 데이터 학습에 훨씬 더 많이 의존했습니다. 병렬 텍스트가 있는 이중 언어 코퍼스를 데이터(즉, 작성된 언어를 제외하고 동일한 두 개의 텍스트)를 사용하여 이러한 시스템은 문장을 작은 하위 집합으로 조각하고 하위 집합을 소스 언어에서 세그먼트별로 번역합니다. 대상 언어. 시스템에 더 많은 데이터(즉, 이중 언어 텍스트 말뭉치)가 있을수록 더 나은 번역이 됩니다.\n",
    "\n",
    "통계적 기계 번역은 2010년대 중반 신경망 기계 번역이 부상할 때까지 가장 널리 연구되고 사용되는 기계 번역 방법으로 남아 있을 것입니다.\n",
    "\n",
    "1990년대까지 이러한 성공으로 연구원들은 텍스트를 넘어 음성 인식으로 확장했습니다. 기계 번역과 같은 음성 인식은 1950년대 초부터 존재했으며 Bell Labs 및 IBM과 같은 초기 성공에 박차를 가했습니다. 그러나 음성 인식 시스템에는 심각한 한계가 있었습니다. 예를 들어 1960년대에 그러한 시스템은 체스를 두기 위해 음성 명령을 받을 수 있었지만 다른 많은 일을 할 수는 없었습니다.\n",
    "\n",
    "1980년대 중반까지 IBM은 음성 인식에 통계적 접근 방식을 적용하고 20,000단어 어휘를 처리할 수 있는 Tangora라는 음성 활성화 타자기를 출시했습니다.\n",
    "\n",
    "DARPA, Bell Labs 및 Carnegie Mellon University도 1980년대 후반까지 비슷한 성공을 거두었습니다. 그때까지 음성 인식 소프트웨어 시스템은 평균적인 인간보다 더 큰 어휘를 가지고 있었고 음성 인식 역사의 이정표인 지속적인 음성 인식을 처리할 수 있었습니다.\n",
    "\n",
    "1990년대에 이 분야의 몇몇 연구자들은 연구실과 대학을 떠나 산업계에서 일했고, 이로 인해 음성 인식 및 기계 번역의 더 많은 상업적 응용이 가능해졌습니다.\n",
    "\n",
    "Google과 같은 오늘날의 NLP 거물들은 2007년에 처음으로 음성 인식 직원을 고용했습니다. 미국 정부도 그 당시에 개입했습니다. NSA는 NSA 분석가의 검색 프로세스를 용이하게 하기 위해 특정 키워드에 대해 대량의 녹음된 대화에 태그를 지정하기 시작했습니다.\n",
    "\n",
    "2010년대 초까지 학계와 산업계의 NLP 연구자들은 NLP 작업을 위한 심층 신경망을 실험하기 시작했습니다. 초기 딥 러닝 주도의 성공은 LSTM(Long Short-Term Memory)이라는 딥 러닝 방법에서 비롯되었습니다. 2015년 Google은 이러한 방법을 사용하여 Google 보이스를 개편했습니다.\n",
    "\n",
    "딥 러닝 방법은 NLP 작업의 성능을 극적으로 향상시켜 공간에 더 많은 비용을 투자했습니다. 이러한 성공으로 일상 생활에서 NLP 소프트웨어가 훨씬 더 깊이 통합되었습니다.\n",
    "\n",
    "예를 들어 2010년대 초반의 자동차에는 제한된 음성 명령 집합을 처리할 수 있는 음성 인식 소프트웨어가 있었습니다. 이제 자동차에는 훨씬 더 광범위한 자연어 명령 세트를 처리할 수 있는 기술이 있어 맥락과 의도를 훨씬 더 명확하게 추론할 수 있습니다.\n",
    "\n",
    "오늘날 돌이켜보면 NLP의 발전은 느리지만 꾸준했습니다. 초기에는 규칙 기반 시스템에서 1980년대에는 통계적 기계 번역으로, 2010년대에는 신경망 기반 시스템으로 이동했습니다. 공간에 대한 학술 연구가 꽤 오랫동안 치열했지만 NLP는 최근에야 주류 주제가 되었습니다. NLP가 오늘날 AI에서 가장 뜨거운 주제 중 하나가 되는 데 도움이 된 지난 몇 년 동안의 주요 변곡점을 살펴보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inflection Points"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP와 컴퓨터 비전은 모두 인공 지능의 하위 분야이지만 컴퓨터 비전은 현재까지 더 성공적인 상업적 성공을 거두었습니다. 컴퓨터 비전은 AlexNet이라는 딥 러닝 기반 솔루션이 컴퓨터 비전 모델의 이전 오류율을 감소시킨 2012년(소위 \"ImageNet\" 순간)에 변곡점을 맞았습니다.\n",
    "\n",
    "2012년부터 컴퓨터 비전은 사진 및 비디오의 자동 태깅, 자율 주행 자동차, 계산원 없는 매장, 안면 인식 기반 장치 인증, 방사선 진단 등과 같은 애플리케이션을 지원해 왔습니다.\n",
    "\n",
    "NLP는 상대적으로 늦게 개화했습니다. NLP는 2014년부터 Amazon Alexa, 개선된 Apple Siri, Google Assistant 및 Microsoft Cortana의 출시로 큰 파장을 일으켰습니다. Google은 또한 2016년에 훨씬 개선된 Google 번역 버전을 출시했으며 이제 챗봇과 보이스봇이 훨씬 더 보편화되었습니다.\n",
    "\n",
    "2018년이 되어서야 NLP는 Transformer 아키텍처를 사용하여 훈련된 대규모 사전 훈련된 언어 모델의 출시와 함께 고유한 ImageNet 순간을 갖게 되었습니다. 그중 가장 주목할만한 것은 2018년 11월에 출시된 Google의 BERT입니다.\n",
    "\n",
    "2019년에는 OpenAI의 GPT-2와 같은 생성 모델이 이전에는 극복할 수 없었던 위업인 이전 콘텐츠를 기반으로 즉석에서 새로운 콘텐츠를 생성하면서 큰 인기를 끌었습니다. 2020년에 OpenAI는 이전의 성공을 기반으로 GPT-3라는 훨씬 더 크고 인상적인 버전을 출시했습니다.\n",
    "\n",
    "2021년 이후로 접어들면서 NLP는 이제 더 이상 AI의 실험적 하위 분야가 아닙니다. 컴퓨터 비전과 함께 NLP는 이제 기업에서 많은 광범위한 기반 응용 프로그램을 가질 준비가 되어 있습니다. 이 책을 통해 회사에서 이러한 응용 프로그램을 구축하는 데 도움이 되는 몇 가지 개념과 도구를 공유할 수 있기를 바랍니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Final Word"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP 작업을 해결하는 단일 접근 방식은 없습니다. 오늘날 지배적인 세 가지 접근 방식은 규칙 기반, 전통적인 기계 학습(통계 기반) 및 신경망 기반입니다.\n",
    "\n",
    "각 접근 방식을 살펴보겠습니다.\n",
    "\n",
    "- **규칙 기반 NLP:** 전통적인 NLP 소프트웨어는 인간이 만든 언어 규칙에 크게 의존합니다. 일반적으로 언어학자인 도메인 전문가가 이러한 규칙을 관리합니다. 이러한 규칙을 정규식 또는 패턴 일치로 생각할 수 있습니다. 규칙 기반 NLP는 좁은 범위의 사용 사례에서 잘 수행되지만 일반적으로 잘 일반화되지 않습니다. 이러한 시스템을 일반화하려면 점점 더 많은 규칙이 필요하므로 규칙 기반 NLP는 다른 NLP 접근 방식에 비해 노동 집약적이고 취약한 솔루션입니다. 다음은 규칙 기반 시스템의 규칙에 대한 예입니다. -ing로 끝나는 단어는 동사이고, -er 또는 -est로 끝나는 단어는 형용사이며, 's로 끝나는 단어는 소유격입니다. 대량의 자연어 데이터를 분석하고 처리할 수 있는 시스템을 만들기 위해 수작업으로 생성합니다. 규칙을 만드는 것은 엄청나게 어렵고 지루한 과정일 뿐만 아니라 그러한 규칙을 사용함으로써 발생하는 많은 오류를 처리해야 합니다. 우리는 각각의 모든 규칙에 대한 모든 코너 케이스를 처리하기 위해 규칙에 대한 규칙을 만들어야 합니다.\n",
    "\n",
    "- **전통적(또는 고전적) 머신 러닝:** 기존 머신 러닝은 규칙보다는 데이터에 더 많이 의존합니다. 통계적 접근 방식을 사용하여 큰 주석 코퍼스를 기반으로 단어의 확률 분포를 그립니다. 인간은 여전히 ​​의미 있는 역할을 합니다. 도메인 전문가는 기계 학습 모델의 성능을 개선하기 위해 기능 엔지니어링을 수행해야 합니다. 기능에는 대문자, 단수 대 복수, 주변 단어 등이 포함됩니다. 이러한 기능을 만든 후에는 NLP 작업을 수행하도록 기존 ML 모델을 교육해야 합니다. 텍스트 분류. 기존 ML은 통계적 접근 방식을 사용하여 특정 기능이나 규칙을 처리 언어에 적용할 시기를 결정하므로 기존 ML 기반 NLP는 규칙 기반 시스템보다 구축 및 유지 관리가 더 쉽습니다. 또한 규칙 기반 NLP보다 더 잘 일반화됩니다.\n",
    "\n",
    "- **신경망:** 신경망은 기존 기계 학습의 단점을 해결합니다. 인간이 기능 엔지니어링을 수행하도록 요구하는 대신 신경망은 표현 학습을 통해 중요한 기능을 \"학습\"합니다. 이러한 신경망이 제대로 작동하려면 방대한 양의 데이터만 있으면 됩니다. 이러한 신경망이 제대로 작동하는 데 필요한 데이터의 양은 상당하지만 오늘날의 인터넷 시대에 데이터를 수집하는 것은 그리 어렵지 않습니다. 신경망을 매우 강력한 함수 근사자 또는 \"규칙\" 생성자로 생각할 수 있습니다. 이러한 규칙과 기능은 인간이 만든 규칙보다 몇 배 더 미묘하고 복잡하므로 자연어 데이터 처리 시 시스템의 보다 자동화된 학습과 일반화가 가능합니다.\n",
    "\n",
    "이 세 가지 중에서 매우 심층적인 신경망(즉, 딥 러닝)의 부상에 힘입은 NLP의 신경망 기반 분기는 가장 강력하고 최근 몇 년 동안 NLP의 많은 주류 상용 응용 프로그램으로 이어진 것입니다.\n",
    "\n",
    "이 책에서는 주로 NLP에 대한 신경망 기반 접근 방식에 초점을 맞추지만 전통적인 머신 러닝 접근 방식도 살펴볼 것입니다. 전자는 많은 NLP 작업에서 최첨단 성능을 가지고 있지만 전통적인 기계 학습은 여전히 ​​상용 응용 프로그램에서 활발하게 사용됩니다.\n",
    "\n",
    "규칙 기반 NLP에 크게 초점을 맞추지는 않겠지만 수십 년 동안 사용되어 왔기 때문에 규칙 기반 NLP에서 다른 리소스를 찾는 데 어려움이 없을 것입니다. 규칙 기반 NLP에는 다른 두 가지 접근 방식 사이에 여지가 있지만 일반적으로 엣지 케이스만 처리합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic NLP"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 NLP를 정의하고, 오늘날 유행하는 응용 프로그램을 탐색하고, NLP의 역사와 변곡점을 다루고, NLP 작업을 해결하기 위한 다양한 접근 방식을 명확히 했으므로 NLP에서 가장 기본적인 작업을 수행하여 여정을 시작하겠습니다.\n",
    "\n",
    "우리는 이러한 작업을 수행하기 위해 NLP의 상용 응용 프로그램인 SpaCy에서 사용하기 위해 가장 널리 사용되는 오픈 소스 라이브러리 중 하나를 활용할 것입니다.\n",
    "\n",
    "SpaCy를 사용하기 전에 이러한 가장 기본적인 NLP 작업이 무엇인지 논의해 봅시다. 장 소개에서 말했듯이 이러한 기본 NLP 작업은 초등학생에게 언어의 기본을 가르치는 것과 유사하게 매우 초보적입니다. 그러나 이러한 기본 NLP 작업이 함께 결합되면 더 복잡한 작업을 수행하는 데 도움이 되며 궁극적으로 오늘날 주요 NLP 응용 프로그램을 강화합니다.\n",
    "\n",
    "우리와 같은 기계는 뛰기 전에 걸어야 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define NLP Tasks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 장의 앞부분에서 다음을 포함하여 오늘날 유행하는 여러 NLP 응용 프로그램을 살펴보았습니다.\n",
    "\n",
    ".NLP 애플리케이션\n",
    "* 기계 번역\n",
    "* 음성 인식\n",
    "* 질의 응답\n",
    "* 텍스트 요약\n",
    "* 챗봇\n",
    "* 텍스트 음성 변환 및 음성 텍스트 변환\n",
    "* 보이스봇\n",
    "* 텍스트 및 오디오 생성\n",
    "* 감정 분석\n",
    "* 정보 추출\n",
    "\n",
    "기계가 이러한 복잡한 애플리케이션을 수행하려면 몇 가지 더 작고 한 입 크기의 NLP 작업을 수행해야 합니다. 즉, 성공적인 상용 NLP 애플리케이션을 구축하려면 해당 애플리케이션의 빌딩 블록 역할을 하는 NLP 작업을 마스터해야 합니다.\n",
    "\n",
    "최신 신경망 기반 NLP 모델은 신경망 훈련을 통해 이러한 \"작업\"을 자동으로 수행한다는 점에 유의해야 합니다. 즉, 신경망은 이러한 작업 중 일부를 수행하는 방법을 자체적으로 학습합니다. 운영자인 우리는 이러한 작업을 명시적으로 수행할 필요가 없습니다.\n",
    "\n",
    "이러한 이유로 이러한 작업은 약간 구식이지만 기계가 자연 언어로 작업하는 방법을 배우는 방법과 비신경망 기반 NLP 모델로 작업하는 방법에 대한 더 큰 직관을 구축하는 데 오늘날에도 여전히 관련이 있습니다. 고전적인 비신경 네트워크 기반 NLP는 오늘날 최첨단 연구에서 선호되지 않는 경우에도 기업에서는 여전히 일반적입니다. 이러한 이유로 이러한 작업을 배우는 것은 가치가 있습니다.\n",
    "\n",
    "더 이상 고민하지 않고 다음은 이러한 NLP 작업 중 일부입니다.\n",
    "\n",
    ".NLP 작업\n",
    "\n",
    "- **토큰화:** 토큰화는 텍스트를 단어, 문장 부호, 기호 등과 같은 최소한의 의미 있는 단위로 나누는 과정입니다. 예를 들어 \"We live in Paris\"라는 문장은 4개의 토큰으로 토큰화할 수 있습니다. , 파리에서. 토큰화는 일반적으로 모든 NLP 프로세스의 첫 번째 단계입니다. 기계가 다른 요소의 맥락에서 각 요소를 분석할 수 있도록 기계가 자연어 데이터를 가장 기본적인 요소(또는 토큰)로 분해해야 하기 때문에 토큰화는 필요한 단계입니다. 그렇지 않으면 기계가 텍스트나 오디오의 긴 왜곡을 하나의 단일 요소인 것처럼 분석해야 하므로 기계가 다루기 힘든 문제가 됩니다. 언어 초보 학생이 정보를 단어 단위로 학습하고 처리하기 위해 문장을 더 작은 비트로 분해하는 것처럼 기계도 동일한 작업을 수행해야 합니다. 복잡한 수치 계산에서도 기계는 문제를 기본 요소로 분해하여 두 집합의 숫자에 대한 덧셈, 뺄셈, 곱셈 및 나눗셈과 같은 작업을 수행합니다. 기계의 주요 이점은 인간이 할 수 없는 속도와 규모로 이 작업을 수행할 수 있다는 것입니다. 토큰화가 텍스트를 의미 있는 최소 단위로 분해한 후 기계는 각 단위에 메타데이터를 할당하여 다른 단위의 컨텍스트에서 각 단위를 처리하는 방법에 대한 자세한 정보를 기계에 제공해야 합니다.\n",
    "\n",
    "- **품사 태깅:** 품사(POS) 태깅은 토큰에 명사, 대명사, 동사, 부사, 형용사, 접속사, 전치사, 감탄사 등의 단어 유형을 부여하는 과정입니다. . \"We live in Paris\"의 품사는 대명사, 동사, 전치사 및 명사입니다. 이 품사 태깅은 각 토큰에 약간 더 많은 메타데이터를 제공하여 시스템이 각 토큰과 다른 모든 토큰 간의 관계를 더 쉽게 할당할 수 있도록 합니다. \"I kick the ball\"이라는 문장에서 \"I\"와 \"ball\"은 모두 명사이고 \"kick\"은 동사입니다. 이 메타데이터를 사용하여 \"kick\"이 어떻게든 \"I\"와 \"ball\"을 연결하여 단어 간의 관계를 형성할 수 있음을 추론할 수 있습니다. 이것이 품사 부분이 중요한 이유입니다. 어떤 단어는 명사이고 다른 단어는 동사라는 사실을 모르면 기계는 토큰 간의 관계를 매핑할 수 없습니다.\n",
    "\n",
    "- **Dependency Parsing:** Dependency Parsing에는 개별 토큰 간의 관계에 레이블을 지정하고 구문 구조를 문장에 할당하는 작업이 포함됩니다. 관계에 레이블이 지정되면 전체 문장을 토큰 세트 간의 일련의 관계로 구성할 수 있습니다. 기계가 텍스트 사이의 고유한 구조를 식별하면 기계가 텍스트를 처리하기가 더 쉽습니다. 문장의 모든 단어가 순서 없이 제시되고 문법 규칙에 대한 사전 지식이 없다면 문장을 이해하는 것이 얼마나 어려울지 생각해 보십시오. 거의 같은 방식으로 기계가 종속성 구문 분석을 수행할 때까지는 토큰으로 변환한 텍스트의 구조에 대해 거의 또는 전혀 알지 못합니다. 구조가 명확해지면 텍스트 처리가 조금 더 쉬워집니다. 종속성 구문 분석은 까다로울 수 있으므로 종속성 구문 분석을 이해하는 가장 좋은 방법은 구문 분석 트리를 사용하여 관계를 시각화하는 것입니다. AllenNLP에는 훌륭한 https://demo.allennlp.org/dependency-parsing/[종속성 구문 분석 데모]가 있으며 그림 1-1에서 종속성 그래프를 생성하는 데 사용했습니다. 이 종속성 그래프를 통해 토큰 간의 관계를 시각화할 수 있습니다. 그림에서 알 수 있듯이 \"We\"는 인칭 대명사(PRP)이자 비 3인칭 단수 현재 동사(VBP)인 \"live\"의 명사 주어(NSUBJ)입니다. \"Live\"는 \"in Paris\"라는 전치사구(PREP)와 연결됩니다. \"in\"은 전치사(IN)이고 \"Paris\"는 전치사(POBJ)의 목적어이며 그 자체가 단수 고유 명사(NNP)입니다. 이러한 관계는 모델링하기가 매우 복잡하며 모든 언어의 진정한 마스터가 되기가 매우 어려운 이유 중 하나입니다. 우리 중 대부분은 수년간의 경험을 통해 언어를 배웠기 때문에 즉석에서 문법 규칙을 적용합니다. 기계는 동일한 유형의 분석을 수행하지만 자연어 처리를 수행하려면 이러한 작업을 엄청나게 빠른 속도로 차례로 처리해야 합니다.\n",
    "\n",
    "![Dependency parsing](images/hulp_0101.png)\n",
    "\n",
    "- **청킹(Chunking):** 청킹은 관련 토큰을 단일 토큰으로 결합하여 관련 명사 그룹, 관련 동사 그룹 등을 생성하는 것을 포함합니다. 예를 들어 \"New York City\"는 세 개의 개별 토큰/청크가 아닌 단일 토큰/청크로 취급될 수 있습니다. 토큰. 청킹은 이를 가능하게 하는 프로세스입니다. 기계가 원본 텍스트를 토큰으로 나누고, 품사를 식별하고, 각 토큰이 텍스트의 다른 토큰과 어떻게 관련되어 있는지 태그를 지정하면 청킹을 수행하는 것이 중요합니다. Chunking은 유사한 토큰을 함께 결합하여 텍스트 분석의 전체 프로세스를 좀 더 쉽게 수행할 수 있도록 합니다. 예를 들어 \"New\", \"York\" 및 \"City\"를 세 개의 개별 토큰으로 취급하는 대신 관련이 있다고 추론하고 단일 그룹(또는 청크)으로 그룹화할 수 있습니다. 그런 다음 청크를 텍스트의 다른 청크와 연관시킬 수 있습니다. 전체 토큰 세트에 대해 이것을 다운하면 작업할 토큰 및 청크 세트가 훨씬 더 작아집니다.\n",
    "\n",
    "- **Lemmatization:** Lemmatization은 단어를 단어의 기본 형태로 변환하는 프로세스입니다. 예를 들어 원형 복원은 말을 말로, 잠에서 잠으로, 가장 큰 것을 큰 것으로 변환합니다. 원형 복원을 사용하면 시스템에서 수행해야 하는 텍스트 처리 작업을 단순화할 수 있습니다. 기본 단어의 변형으로 작업하는 대신 원형 복원을 수행한 후 기본 단어로 직접 작업할 수 있습니다.\n",
    "\n",
    "- **스테밍(Stemming):** 스테밍은 표제어 추출과 관련된 프로세스이지만 더 간단합니다. 어간 추출은 단어를 단어 어간으로 줄입니다. 어간 추출 알고리즘은 일반적으로 규칙 기반입니다. 예를 들어, big이라는 단어는 big으로 줄어들지만, slept라는 단어는 전혀 줄어들지 않습니다. 어간 추출은 때때로 무의미한 하위 단어를 생성하며 이러한 이유로 어간 추출보다 표제어 추출을 선호합니다. 원형 복원은 사전에 따라 단어를 기본 또는 정식 형식으로 반환합니다. 그러나 스테밍에 비해 비용이 많이 드는 프로세스입니다. 표제어 추출을 잘 수행하려면 단어의 품사를 알아야 합니다.\n",
    "\n",
    "> 참고: 토큰화, 품사 태그 지정, 종속성 구문 분석, 청킹, 표제어 지정 및 형태소 분석은 다운스트림 NLP 애플리케이션을 위한 자연어를 처리하는 작업입니다. 즉, 이러한 작업은 목적을 위한 수단입니다. 기술적으로 다음 두 \"작업\"(엔티티 인식 및 엔터티 연결)은 자연어 작업이 아니라 NLP 애플리케이션에 더 가깝습니다. 명명된 엔터티 인식 및 엔터티 연결은 목적을 위한 수단이 아니라 목적 자체가 될 수 있습니다. 그러나 다운스트림 NLP 응용 프로그램에도 사용되므로 여기서 \"작업\" 섹션에 포함합니다.\n",
    "\n",
    "- **Named Entity Recognition:** Named Entity Recognition(NER)은 사람, 조직, 위치, 날짜, 통화 등과 같이 알려진 개체(또는 개체)에 레이블을 지정하는 프로세스입니다. \"We live in Paris\"에서 파리는 위치로 표시됩니다. NER은 매우 강력합니다. 이를 통해 기계는 명명된 엔터티 태그로 가장 중요한 토큰에 태그를 지정할 수 있으며 이는 NLP의 정보 검색 애플리케이션에 매우 중요합니다. 예를 들어 일련의 문서에서 조지 W. 부시 전 미국 대통령을 검색하려는 경우 기계가 명명된 엔터티 인식을 사용하여 모든 문서의 모든 사람에 태그를 지정한 다음 이 사람 목록 내에서 검색합니다. 추가 조사를 위해 관련 문서 세트를 찾습니다.\n",
    "\n",
    "- **엔티티 연결(Entity Linking):** 엔터티 연결은 한 형식의 텍스트를 다른 형식으로 연결하여 외부 데이터베이스에 대한 엔터티를 명확하게 하는 프로세스입니다. 이는 엔터티 해결 애플리케이션(예: 데이터 중복 제거)과 정보 검색 애플리케이션 모두에 중요합니다. George W. Bush의 예에서 우리는 George W. Bush의 모든 경우를 George W. Bush로 해결하고 싶지만 George H. W. Bush, George W. Bush의 아버지이자 전 미국 대통령은 해결하지 않기를 원할 것입니다. 이 결의안과 부시 대통령의 올바른 버전에 대한 연결은 까다롭고 까다로운 과정이지만 주어진 모든 텍스트 컨텍스트에서 기계가 수행할 수 있는 과정입니다. 기계가 엔터티 인식 및 연결을 수행하면 정보 검색은 오늘날 가장 상업적으로 관련된 NLP 응용 프로그램 중 하나가 됩니다.\n",
    "\n",
    "이것은 가장 기본적인 NLP 작업에 대한 빠르고 간단한 개요입니다. 이러한 작업을 더 자세히 조사하고 싶을 것입니다. 온라인에서 사용할 수 있는 충분한 리소스가 있습니다. 그러나 지금은 시작하기에 충분한 정보입니다.\n",
    "\n",
    "이제 보다 야심 찬 NLP 응용 프로그램의 구성 요소 역할을 하는 기본 NLP 작업을 알았으므로 오픈 소스 NLP 라이브러리 SpaCy를 사용하여 이러한 기본 NLP 작업 중 일부를 수행해 보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Programming Environment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기본 NLP 작업을 수행하려면 먼저 프로그래밍 환경을 설정해야 합니다.\n",
    "\n",
    "이 책에서는 오늘날 데이터 과학자가 사용할 수 있는 가장 사용하기 쉬운 프로그래밍 환경 중 하나인 [Google의 Colaboratory](https://colab.research.google.com/notebooks/welcome.ipynb)를 사용합니다. Google Colab은 전적으로 클라우드에서 실행되는 무료 Jupyter 노트북 환경입니다. 2장에서는 Google Colab 및 대체 프로그래밍 환경에 대해 자세히 설명합니다.\n",
    "\n",
    "GitHub를 코딩 저장소로 사용할 것입니다. 각주:[GitHub에 대한 자세한 내용은 [GitHub 웹사이트](https://github.com/)를 방문하세요. 그리고 [GitHub과 통합하는 방법에 대한 Google Colab의 지침](https://colab.research.google.com/github/googlecolab/colabtools/blob/master/notebooks/colab-github-demo.ipynb).]\n",
    "\n",
    "컴퓨터에서 로컬로 코드를 실행하려는 경우 Github 리포지토리에 로컬 환경을 설정하는 지침이 있습니다.\n",
    "\n",
    "이를 통해 기본 NLP 작업 코딩을 시작하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SpaCy, fast.ai, and Hugging Face"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 책에서는 SpaCy, fast.ai, Hugging Face 등 3대 메이저 기업에서 제공하는 오픈소스 소프트웨어 라이브러리를 사용하여 자연어 처리를 수행할 것이다. 이 라이브러리는 고수준이며 그렇지 않으면 수행해야 하는 많은 저수준 작업을 추상화합니다. 이러한 라이브러리를 NLP를 빠르게 적용할 수 있는 아름다운 래퍼로 생각하세요. 세 가지 라이브러리 모두 성능이 뛰어나고 상업적으로 실행 가능하며 세 가지 중 하나를 선택하여 자신의 응용 작업을 수행할 수 있습니다. 세 가지를 모두 선택할 필요는 없습니다. 즉, 각각의 강점과 약점이 있기 때문에 세 가지 모두에 정통한 것이 현명하며 때로는 다른 것보다 NLP의 최신 발전을 더 빨리 채택할 수 있습니다. 이 장에서 SpaCy로 진행하기 전에 세 가지를 빠르게 소개하겠습니다. 다음 장에서는 fast.ai와 Hugging Face를 다루겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SpaCy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2015년에 처음 출시된 [SpaCy](https://spacy.io/)는 Python과 Cython을 모두 활용하여 놀랍도록 빠른 성능을 제공하는 NLP용 오픈 소스 라이브러리입니다. SpaCy 이전에는 [Natural Language Toolkit(NLTK)](https://www.nltk.org/)이 연구자들 사이에서 최고의 NLP 라이브러리였지만 NLTK는 오래되었고(처음에는 2001년에 출시됨) 확장성이 좋지 않았습니다. SpaCy는 상업 청중을 대상으로 한 최초의 현대 NLP 라이브러리였습니다. 생산 규모 확장을 염두에 두고 제작되었습니다. 이제 64개 이상의 언어와 TensorFlow 및 PyTorch를 모두 지원하는 기업의 NLP 애플리케이션용 라이브러리 중 하나입니다.\n",
    "\n",
    "2021년 이전에 SpaCy 2.x는 업계를 선도하는 변환기 기반 모델이 아니라 이 책의 뒷부분에서 다룰 순환 신경망(RNN)에 의존했습니다. 그러나 2021년 1월 현재 SpaCy는 최신 변환기 기반 파이프라인도 지원하여 오늘날 사용 중인 주요 NLP 라이브러리 사이에서 입지를 확고히 했습니다.\n",
    "\n",
    "SpaCy의 창작자이자 모회사인 [Explosion AI](http://explosion.ai/)도 [Prodigy](https://prodi.gy/)라는 훌륭한 주석 플랫폼을 제공하며, 3장에서 사용할 것입니다. 3개 라이브러리 중에서 SpaCy는 제작자가 지난 6년 이상 동안 만들고 지원한 모든 통합을 고려할 때 가장 성숙하고 확장성이 뛰어납니다. 오늘날 생산 용도에 가장 적합한 제품입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### fast.ai"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[fast.ai](https://www.fast.ai/)(회사)는 2018년에 PyTorch를 기반으로 구축된 fastai라는 오픈 소스 라이브러리를 출시했습니다. 회사인 fast.ai는 기계 학습에 대한 보다 실용적인 입문을 원하는 코더에게 대규모 공개 온라인 과정(MOOC)을 제공하여 명성을 쌓았으며 fastai 라이브러리는 이러한 정신을 반영합니다. fastai 라이브러리에는 코더가 최신 결과를 빠르고 쉽게 생성할 수 있는 높은 수준의 구성 요소가 있습니다. 동시에 fastai에는 연구원이 사용자 지정 문제를 해결하기 위해 혼합하고 일치시킬 수 있는 낮은 수준의 구성 요소가 있습니다. fastai의 제작자는 NLP의 첫 번째 전이 학습 방법 중 하나인 [ULMFiT](https://arxiv.org/abs/1801.06146)도 만들었습니다. 2장에서 사용할 것입니다. 코스 작업과 비디오를 원하는 사람들을 위해 빠르고 사용하기 쉬운 라이브러리와 함께 fastai는 훌륭한 옵션입니다. 그러나 SpaCy와 Hugging Face보다 덜 성숙하고 프로덕션 작업에 적합하지 않습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hugging Face"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2016년에 설립된 [Hugging Face](https://huggingface.co/)는 이 분야에서 가장 최근에 등장한 회사이지만 현재 세 곳 중 가장 자금을 많이 받고 빠르게 성장하고 있습니다. 이 회사는 2021년 3월에 4,000만 달러 규모의 시리즈 B를 모금했습니다. Hugging Face는 NLP에만 집중하고 실무자가 최신 변압기를 사용하여 NLP 애플리케이션을 구축할 수 있도록 지원하기 위해 만들어졌습니다. Transformers라는 라이브러리는 PyTorch 및 TensorFlow용으로 구축되었으며 100개 이상의 언어를 지원합니다. 실제로 개발 및 배포를 위해 PyTorch 및 Tensorflow에서 매우 원활하게 이동할 수 있습니다. Hugging Face에는 NLP 모델 생산을 위한 파이프라인 API도 있습니다. 우리는 세 개의 라이브러리 중에서 Hugging Face의 미래에 대해 가장 기대하고 있으며 충분한 시간을 들여 이에 익숙해질 것을 적극 권장합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform NLP Tasks using SpaCy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 NLP 작업에 SpaCy를 사용하겠습니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "먼저 spaCy를 설치합니다. spaCy 설치 방법에 대한 자세한 내용은 [SpaCy 공식 웹사이트](https://spacy.io/usage)를 방문하세요."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "아직 spaCy를 설치하지 않았다면 이 명령으로 필요한 모든 것을 얻을 수 있습니다. 노트북에서 실행하는 경우 이전에 수행한 것처럼 각 행 앞에 `!` 문자를 붙입니다.\n",
    "\n",
    "```bash\n",
    "pip install -U spacy[cuda110,transformers,lookups]==3.0.3\n",
    "pip install -U spacy-lookups-data==1.0.0\n",
    "pip install cupy-cuda110==8.5.0\n",
    "python -m spacy download en_core_web_trf\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download Pretrained Language Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SpaCy에는 기본적으로 사용할 수 있도록 사전 훈련된 언어 모델이 있습니다. 사전 훈련된 모델은 이미 많은 데이터에 대해 훈련되었으며 추론을 수행할 준비가 된 모델입니다.\n",
    "\n",
    "이러한 사전 훈련된 언어 모델은 기본 NLP 작업을 해결하는 데 도움이 되지만 고급 사용자는 선택한 보다 구체적인 데이터에서 사전 훈련된 모델을 미세 조정할 수 있습니다. 이것은 당면한 특정 작업에 대해 훨씬 더 나은 성능을 제공할 것입니다.\n",
    "\n",
    "미세 조정은 사전 훈련된 모델을 가져와서 사용자의 도메인과 관련된 보다 구체적인 텍스트 코퍼스에서 좀 더 훈련(즉, 모델 미세 조정)하는 프로세스입니다.각주:[이 작업을 수행하는 작업은 하나의 작업을 위해 개발된 모델을 두 번째 작업에 대한 모델의 시작점으로 사용하는 것을 전이 학습이라고 합니다.] 예를 들어, 우리가 금융 분야에서 일했다면 재무 문서에서 사전 훈련된 일반 언어 모델을 미세 조정하기로 결정할 수 있습니다. 재무 관련 언어 모델을 생성합니다. 이 재무 관련 언어 모델은 일반적인 사전 훈련된 언어 모델에 비해 재무 관련 NLP 작업에서 훨씬 더 나은 성능을 보입니다.\n",
    "\n",
    "SpaCy는 사전 훈련된 언어 모델을 핵심 모델과 스타터 모델의 두 그룹으로 나눕니다. 핵심 모델은 범용 모델이며 기본 NLP 작업을 해결하는 데 도움이 됩니다. 스타터 모델은 전이 학습에 유용한 기본 모델입니다. 이러한 모델에는 자체 모델을 초기화하고 미세 조정하는 데 사용할 수 있는 사전 훈련된 가중치가 있습니다. 핵심 모델을 바로 사용할 수 있는 모델로, 기본 모델을 DIY 스타터 키트로 생각하십시오.\n",
    "\n",
    "바로 사용할 수 있는 핵심 모델을 사용하여 기본 NLP 작업을 수행합니다. 먼저 핵심 모델을 가져오겠습니다 각주:[spaCy 언어 모델은 일반적으로 NLP 문헌에서 \"언어 모델\"이라고 부르는 것과 다릅니다. 언어 모델링에 대한 자세한 내용은 [[ch02]].]를 참조하십시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import spacy and download language model\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# Download data\n",
    "!aws s3 cp s3://applied-nlp-book/data/ data --recursive --no-sign-request\n",
    "!aws s3 cp s3://applied-nlp-book/models/ag_dataset/ models/ag_dataset --recursive --no-sign-request\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 첫 번째 NLP 작업인 토큰화를 수행해 보겠습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "토큰화는 모든 NLP 작업이 시작되는 곳입니다. 기계가 눈에 보이는 텍스트를 처리하기 전에 텍스트를 한 입 크기의 토큰으로 분해해야 합니다. 토큰화는 텍스트를 단어, 문장 부호 등으로 분할합니다.\n",
    "\n",
    "SpaCy는 데이터(예: `nlp(SENTENCE)`)에서 언어 모델을 실행할 때 전체 NLP 파이프라인을 자동으로 실행하지만 토크나이저만 분리하기 위해 `nlp.tokenizer(SENTENCE)`를 사용하여 토크나이저만 호출합니다.\n",
    "\n",
    "그런 다음 토큰의 길이와 개별 토큰을 인쇄합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "sentence = nlp.tokenizer(\"We live in Paris.\")\n",
    "\n",
    "# Length of sentence\n",
    "print(\"The number of tokens: \", len(sentence))\n",
    "\n",
    "# Print individual words (i.e., tokens)\n",
    "print(\"The tokens: \")\n",
    "for words in sentence:\n",
    "    print(words)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "토큰의 길이는 5이고 개별 토큰은 `\"We\"`, `\"live\"`, `\"in\"`, `\"Paris\"`, `\".\"`입니다. 문장 끝의 마침표는 자체 토큰입니다.\n",
    "\n",
    "spaCy 토크나이저는 새 줄(`\"\\n\"`), 탭(`\"\\t\"`) 및 단일 공백(`\" \"`) 너머의 공백 문자를 토큰으로 취급합니다.\n",
    "\n",
    "약간 더 복잡한 예제에서 토크나이저를 사용해 봅시다.\n",
    "\n",
    "공개적으로 사용 가능한 Jeopardy 질문을 로드한 다음 몇 가지 질문에 대해 전체 SpaCy 언어 모델을 실행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# Import Jeopardy Questions\n",
    "data = pd.read_csv(cwd+'/data/jeopardy_questions/jeopardy_questions.csv')\n",
    "data = pd.DataFrame(data=data)\n",
    "\n",
    "# Lowercase, strip whitespace, and view column names\n",
    "data.columns = map(lambda x: x.lower().strip(), data.columns)\n",
    "\n",
    "# Reduce size of data\n",
    "data = data[0:1000] \n",
    "\n",
    "# Tokenize Jeopardy Questions\n",
    "data[\"question_tokens\"] = data[\"question\"].apply(lambda x: nlp(x))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "처음 1,000개의 Jeopardy 질문에 대해 이제 토큰을 만들었습니다. 즉, 1,000개의 Jeopardy 질문 각각에 대한 토큰을 만들었습니다.\n",
    "\n",
    "모든 것이 제대로 작동하는지 확인하기 위해 첫 번째 질문과 생성된 토큰을 살펴보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View first question\n",
    "example_question = data.question[0]\n",
    "example_question_tokens = data.question_tokens[0]\n",
    "print(\"The first questions is:\")\n",
    "print(example_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print individual tokens of first question\n",
    "print(\"The tokens from the first question are:\")\n",
    "for tokens in example_question_tokens:\n",
    "    print(tokens)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이것은 컴퓨터가 수행하는 첫 번째 기본 NLP 작업입니다. 이제 다른 NLP 작업으로 이동할 수 있습니다. 잘하셨어요!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part-of-speech Tagging"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "토큰화 후 기계는 각 토큰의 품사와 같은 관련 메타데이터로 각 토큰에 태그를 지정해야 합니다. 이것이 우리가 지금 수행할 것입니다.\n",
    "\n",
    "전체 SpaCy 언어 모델을 Jeopardy 질문에 적용했기 때문에 생성된 토큰에는 이미 우리가 관심을 갖는 많은 의미 있는 속성/메타데이터가 있습니다.\n",
    "\n",
    "SpaCy는 미리 로드된 통계 모델을 사용하여 각 토큰의 품사를 예측합니다. 이전에 `spacy.load(\"en_core_web_sm\")` 코드를 사용하여 영어 통계 모델을 로드했습니다.\n",
    "\n",
    "첫 번째 질문에서 토큰의 품사(POS) 태깅 속성을 살펴보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Part-of-speech tags for tokens in the first question\n",
    "print(\"Here are the Part-of-speech tags for each token in the first question:\")\n",
    "for token in example_question_tokens:\n",
    "    print(token.text,token.pos_, spacy.explain(token.pos_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first token \"For\" is marked as an adposition (e.g., in, to, during), the second token \"the\" is a determiner (e.g., a, an, the), the third token \"last\" is an adjective, the fourth token \"8\" is a numeral, the fifth token \"years\" is a noun, and so on.\n",
    "\n",
    "Figure 1-2 displays the full list of all possible POS tags, including descriptions and examples of each.footnote:[Please visit the [SpaCy POS documentation](https://spacy.io/api/annotation) for more.]\n",
    "\n",
    "![Part-of-speech Tags](images/hulp_0102.png)\n",
    "\n",
    "Now that we have used the tokenizer to create tokens for each sentence and part-of-speech tagging to tag each token with meaningful attributes, let's label each token's relationship with other tokens in the sentence. In other words, let's find the inherent structure among the tokens given the part-of-speech metadata we have generated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dependency Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dependency parsing is the process to find these relationships among the tokens. Once we have performed this step, we will be able to visualize the relationships using a dependency parsing graph.\n",
    "\n",
    "First, let's view the depenency parsing tags for each of the tokens in the first question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Dependency Parsing tags for tokens in the first question\n",
    "for token in example_question_tokens:\n",
    "    print(token.text,token.dep_, spacy.explain(token.dep_))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "첫 번째 토큰 \"For\"는 전치사 수식어로 표시되고, 두 번째 토큰 \"the\"는 한정사, 세 번째 토큰 \"last\"는 형용사 수식어, 네 번째 토큰 \"8\"은 숫자 수식어, 다섯 번째 토큰 \"years\"로 표시됩니다. \"는 전치사의 목적어 등입니다.\n",
    "\n",
    "그림 1-3 및 1-4는 각각의 설명 및 예를 포함하여 가능한 모든 구문 종속성 태그를 나열합니다.주:[[SpaCy documentation](https://spacy.io/api/annotation)를 방문하십시오.]\n",
    "\n",
    "![Syntactic Dependency Parsing Labels Part 1](images/hulp_0103.png)\n",
    "\n",
    "![Syntactic Dependency Parsing Labels Part 2](images/hulp_0104.png)\n",
    "\n",
    "이러한 태그는 토큰 간의 관계를 정의하는 데 도움이 됩니다. 이러한 태그를 사용하여 문장을 구성하는 토큰 간의 관계 구조를 이해할 수 있습니다.\n",
    "\n",
    "종속성 구문 분석은 풀기 어렵기 때문에 spaCy의 내장 시각화 도우미를 사용하여 토큰 전체의 종속성을 더 잘 이해해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the dependency parse\n",
    "from spacy import displacy\n",
    "\n",
    "displacy.render(example_question_tokens, style='dep',\n",
    "                jupyter=True, options={'distance': 120})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그림 1-5는 구문 분석된 문장의 첫 번째 부분을 표시합니다.\n",
    "\n",
    "![Dependency Parsing Example - Part 1](images/hulp_0105.png)\n",
    "\n",
    "전치사구에서 \"For\"와 \"years\"의 중요성에 주목하세요. 여러 토큰이 이 둘에 매핑됩니다.\n",
    "\n",
    "그림 1-6은 구문 분석된 문장의 두 번째 부분을 보여줍니다.\n",
    "\n",
    "![Dependency Parsing Example - Part 2](images/hulp_0106.png)\n",
    "\n",
    "토큰 \"was\"는 명목상의 주어 \"Galileo\"와 두 개의 전치사구인 \"under house arrest\"와 \"for espousing this man's theory\"에 연결됩니다.\n",
    "\n",
    "이 그림은 특정 토큰을 그룹화하는 방법과 토큰 그룹이 서로 관련되는 방법을 보여줍니다. 이것은 자연어 처리에서 필수적인 단계입니다. 먼저 기계는 문장을 토큰으로 나눕니다. 그런 다음 메타데이터를 각 토큰(예: 품사)에 할당한 다음 서로 간의 관계를 기반으로 토큰을 연결합니다.\n",
    "\n",
    "관련된 토큰을 그룹화하는 또 다른 형태인 청킹으로 넘어가겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chunking"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음 문장에 청킹을 수행해 봅시다.: \"부모님은 뉴욕에 사세요.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print tokens for example sentence without chunking\n",
    "for token in nlp(\"My parents live in New York City.\"):\n",
    "    print(token.text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "청킹은 관련 토큰을 단일 토큰으로 결합합니다.\n",
    "\n",
    "청킹을 사용하면 spaCy 언어 모델은 인간이 머릿속에서 문장을 구문 분석할 때와 같이 명사 청크로 \"나의 부모님\"과 \"뉴욕 시\"를 식별합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print chunks for example sentence\n",
    "for chunk in nlp(\"My parents live in New York City.\").noun_chunks:\n",
    "      print(chunk.text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "관련 토큰을 청크로 그룹화하면 기계가 문장을 더 쉽게 처리할 수 있습니다. 각 토큰을 개별적으로 보는 대신 기계는 이제 특정 토큰이 다른 토큰과 관련되어 있음을 인식하며 이는 자연어 처리의 필수 단계입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 한 단계 더 나아가 표제어화를 수행해 보겠습니다. 표제어 추출은 단어를 단어의 기본(또는 정식) 형식으로 변환하는 프로세스입니다. 예를 들어, 말 대 말, 잤다가 잠자기, 가장 큰 것에서 큰 것. 품사 태깅, 종속성 구문 분석 및 청킹과 마찬가지로 표제어 정리는 시스템이 토큰을 \"처리\"하는 데 도움이 됩니다. 표제어 정리를 사용하면 시스템에서 일부 토큰을 가장 기본적인 형식으로 변환하여 토큰을 단순화할 수 있습니다.\n",
    "\n",
    "형태소 분석은 관련 개념이지만 형태소 분석이 더 간단합니다. 형태소 분석은 종종 규칙 기반 접근 방식을 사용하여 단어를 단어 형태로 줄입니다.\n",
    "\n",
    "원형 복원은 더 어려운 프로세스이지만 일반적으로 결과가 더 좋습니다. 형태소 분석은 때때로 무의미한(단어가 아닌) 출력을 생성합니다. 실제로 spaCy는 형태소 분석도 지원하지 않습니다. 표제어만 지원합니다.\n",
    "\n",
    "우리는 토큰의 원본 버전과 원형 변환 버전을 나란히 저장하고 보기 위해 DataFrame을 생성할 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Lemmatization for tokens in the first question\n",
    "lemmatization = pd.DataFrame(data=[], \\\n",
    "  columns=[\"original\",\"lemmatized\"])\n",
    "i = 0\n",
    "for token in example_question_tokens:\n",
    "    lemmatization.loc[i,\"original\"] = token.text\n",
    "    lemmatization.loc[i,\"lemmatized\"] = token.lemma_\n",
    "    i = i+1\n",
    "\n",
    "lemmatization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, words such as \"years\", \"was\", and \"espousing\" are lemmatized to their base forms. The other tokens are already their base forms, so the lemmatized output is the same 보시다시피 \"years\", \"was\" 및 \"espousing\"과 같은 단어는 기본 형식으로 표제어 정리됩니다. 다른 토큰은 이미 기본 형식이므로 표제어화된 출력은 원본과 동일합니다. Lemmatization은 가능한 경우 토큰을 가장 간단한 형태로 단순화하여 기계가 문장을 구문 분석하는 프로세스를 단순화합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Named Entity Recognition"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "지금까지 수행한 모든 작업(토큰화, 품사 태그 지정, 종속성 구문 분석, 청킹 및 표제어 지정)을 함께 사용하면 기계에서 더 복잡한 NLP 작업을 수행할 수 있습니다.\n",
    "\n",
    "복잡한 NLP 작업의 한 예는 개체 인식(NER)이라는 이름입니다. 명명된 엔터티 인식은 자연어로 주목할만한 엔터티를 구문 분석하고 적절한 클래스 레이블로 레이블을 지정합니다. 예를 들어, NER은 \"Person\" 레이블이 있는 사람 이름과 \"Location\" 레이블이 있는 도시 이름에 레이블을 지정합니다.\n",
    "\n",
    "NER은 기계가 우리가 다룬 이전 NLP 작업에서 생성된 메타데이터를 사용하여 텍스트 분류를 수행할 수 있기 때문에 가능합니다. 이전 NLP 작업의 메타데이터가 없으면 사람의 이름을 \"사람\"으로, 도시 이름을 \"위치\"로 분류하는 기능이 충분하지 않기 때문에 머신이 NER을 수행하는 데 매우 어려움을 겪을 것입니다.\n",
    "\n",
    "NER은 많은 조직에서 많은 양의 문서를 처리해야 하기 때문에 가치 있는 NLP 작업이며, 적절한 클래스 레이블로 주목할만한 엔터티에 레이블을 지정하는 간단한 작업은 특히 정보 검색 작업의 경우 텍스트 정보를 분석하는 의미 있는 첫 번째 단계입니다. 예를 들어 필요한 정보를 최대한 빨리 찾는 것).\n",
    "\n",
    "이러한 문서에는 계약서, 임대차 계약서, 부동산 구매 계약서, 재무 보고서, 뉴스 기사 등이 포함됩니다. 명명된 개체를 인식하기 전에는 인간이 그러한 개체에 손으로 레이블을 지정해야 했습니다(많은 회사에서 여전히 수행함). 이제 명명된 엔터티 인식(\"NER\"라고도 함)은 이 작업을 수행하는 알고리즘 방식을 제공합니다.\n",
    "\n",
    "SpaCy의 NER 모델은 많은 유형의 주목할만한 개체(\"실제 개체\")에 레이블을 지정할 수 있습니다. 그림 1-7은 spaCy 모델이 인식할 수 있는 현재 엔터티 유형 집합을 보여줍니다.\n",
    "\n",
    "![spaCy NER Entity Types](images/hulp_0107.png)\n",
    "\n",
    "NER은 본질적으로 분류 모델이라는 점에 유의하는 것이 매우 중요합니다. NER 모델은 관심 토큰 주변의 토큰 컨텍스트를 사용하여 관심 토큰의 엔터티 유형을 예측합니다. NER은 통계 모델이며 모델이 훈련한 데이터 모음이 중요합니다. 더 나은 성능을 위해 엔터프라이즈에서 이러한 모델의 개발자는 기본 NER 모델에 비해 더 나은 성능을 달성하기 위해 특정 문서 코퍼스에서 기본 NER 모델을 미세 조정합니다.\n",
    "\n",
    "spaCy NER 모델을 사용해 봅시다. [Wikipedia 기사](https://en.wikipedia.org/wiki/George_Washington)에서 미국 초대 대통령 조지 워싱턴을 설명하는 첫 문장에 대해 NER을 수행합니다.\n",
    "\n",
    "문장은 다음과 같습니다. 조지 워싱턴은 미국의 정치 지도자, 군 장군, 정치가, 건국의 아버지로 1789년부터 1797년까지 미국의 초대 대통령을 역임했습니다.\n",
    "\n",
    "위에서 볼 수 있듯이 조지 워싱턴과 미국을 포함하여 여기에서 인식해야 할 몇 가지 실제 개체가 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print NER results\n",
    "example_sentence = \"George Washington was an American political leader, \\\n",
    "military general, statesman, and Founding Father who served as the \\\n",
    "first president of the United States from 1789 to 1797.\\n\"\n",
    "\n",
    "print(example_sentence)\n",
    "\n",
    "print(\"Text Start End Label\")\n",
    "doc = nlp(example_sentence)\n",
    "for token in doc.ents:\n",
    "    print(token.text, token.start_char, token.end_char, token.label_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "출력에는 네 가지 요소가 있습니다. 첫째, 엔터티를 구성하는 텍스트입니다. 텍스트는 단일 토큰이거나 전체 엔터티를 구성하는 토큰 집합일 수 있습니다. 둘째, 문장에서 텍스트의 시작 위치입니다. 셋째, 문장에서 텍스트의 끝 위치입니다. 넷째, 엔터티의 레이블입니다.\n",
    "\n",
    "NER의 가치를 더욱 명확하게 하기 위해 spaCy의 내장 시각화 도구를 사용하여 관련 항목 레이블로 이 문장을 시각화해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize NER results\n",
    "displacy.render(doc, style='ent', jupyter=True, options={'distance': 120})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그림 1-8에서 볼 수 있듯이 spaCy NER 모델은 엔티티에 레이블을 지정하는 작업을 훌륭하게 수행합니다. \"George Washington\"은 사람이고 텍스트는 색인 0에서 시작하여 색인 17에서 끝납니다. 그의 국적은 \"미국인\"입니다. \"첫 번째\"는 서수로 표시되고 \"미국\"은 지정학적 개체이며 \"1789년에서 1797년\"은 날짜입니다.\n",
    "\n",
    "![Visualize NER Results](images/hulp_0108.png)\n",
    "\n",
    "문장은 엔터티 유형에 따라 색상으로 구분된 레이블로 아름답게 렌더링됩니다. 이것은 강력하고 의미 있는 NLP 작업입니다. 사람 없이 이 기계 기반 라벨링을 대규모로 수행하는 것이 어떻게 많은 텍스트 데이터로 작업하는 기업에 많은 가치를 추가할 수 있는지 확인할 수 있습니다. 물론, 처음부터 그러한 모델을 훈련시키려면 텍스트 데이터에 주석을 다는 많은 사람이 필요합니다. 그리고 프로덕션에서 엣지 케이스를 처리하려면 루프에 사람이 필요할 수 있습니다. 당신은 결코 정말로 인간으로부터 자유로울 수 없지만 아마도 궁극적으로 대부분 인간이 없는 프로세스에 도달할 수 있을 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Named Entity Linking"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "엔터프라이즈에서 복잡하지만 매우 유용한 또 다른 NLP 작업은 엔터티 연결(NEL)이라는 이름입니다. 명명된 엔터티 연결은 텍스트 엔터티를 지식 기반의 고유 식별자로 확인합니다. 즉, NEL은 원본 텍스트의 엔터티를 지식 데이터베이스의 표준 버전으로 확인합니다. 이름이 지정된 모든 항목을 Google 지식 그래프에 연결해 봅시다. 이 명명된 엔터티 연결을 수행하기 위해 Google Knowledge Graph API를 호출합니다.각주:[이 API를 수행하려면 [Google Knowledge Graph API 키](https://developers.google.com/knowledge-graph)가 필요합니다. 자신의 기계를 호출하십시오. 설명을 위해 자체 API 키를 사용하여 이를 수행합니다.]\n",
    "\n",
    "다음은 이 API 호출을 수행하는 함수입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import requests\n",
    "\n",
    "# Define Google Knowledge Graph API Result function\n",
    "def returnGraphResult(query, key, entityType):\n",
    "    if entityType==\"PERSON\":\n",
    "        google = f\"https://kgsearch.googleapis.com/v1/entities:search\\\n",
    "         ?query={query}&key={key}\"\n",
    "        resp = requests.get(google)\n",
    "        url = resp.json()['itemListElement'][0]['result']\\\n",
    "         ['detailedDescription']['url']\n",
    "        description = resp.json()['itemListElement'][0]['result']\\\n",
    "         ['detailedDescription']['articleBody']\n",
    "        return url, description\n",
    "    else:\n",
    "        return \"no_match\", \"no_match\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "George Washington 예제에서 엔터티 연결을 수행해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Wikipedia descriptions and urls for entities\n",
    "# You can un-comment this and run the code after you obtain your own Google Knowledge Graph API key\n",
    "'''\n",
    "for token in doc.ents:\n",
    "    url, description = returnGraphResult(token.text, key, token.label_)\n",
    "    print(token.text, token.label_, url, description)\n",
    "'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음은 출력입니다.\n",
    "\n",
    "- George Washington:: PERSON https://en.wikipedia.org/wiki/George_Washington 조지 워싱턴은 미국의 정치 지도자, 장군, 정치가, 건국의 아버지였으며 1789년부터 미국의 초대 대통령을 역임하기도 했습니다. 1797.\n",
    "- 미국: NORP no_match no_match\n",
    "- 첫 번째:: ORDINAL no_match no_match\n",
    "- 미국:: GPE no_match no_match\n",
    "- 1789년 ~ 1797년:: DATE no_match no_match\n",
    "\n",
    "보시다시피 George Washing은 개인이며 \"George Washington\" Wikipedia URL 및 설명에 성공적으로 연결되어 있습니다. 나머지는 엔터티 유형 PERSON이 아니며 연결되지 않습니다. 원하는 경우 미국과 같은 다른 명명된 엔터티를 관련 Wikipedia 기사에 연결할 수도 있습니다.\n",
    "\n",
    "명명된 엔터티 연결은 기업에서 많은 사용 사례를 가지고 있습니다. 특히 정보를 분류 체계에 연결해야 할 필요성이 계속해서 제기되기 때문입니다(예: 주식 시세 표시기, 의약품, 상장 기업, 소비자 제품 등을 표준 버전에 연결). 분류 또는 지식 기반)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 장에서 우리는 NLP를 정의하고 오늘날 기업에서 널리 사용되는 일부 상용 응용 프로그램을 포함하여 그 기원을 다루었습니다. 그런 다음 몇 가지 기본 NLP 작업을 정의하고 SpaCy라는 고성능 NLP 라이브러리를 사용하여 수행했습니다. 이 장에서 배운 내용을 연마하려면 온라인에서 사용할 수 있는 문서 검토를 포함하여 SpaCy를 사용하는 데 더 많은 시간을 할애해야 합니다.\n",
    "\n",
    "우리가 수행한 작업은 매우 기본적이지만 함께 결합하면 토큰화, 품사 태그 지정, 종속성 구문 분석, 청킹 및 표제어 지정과 같은 NLP 작업을 통해 컴퓨터가 명명된 엔터티 인식과 같은 훨씬 더 복잡한 NLP 작업을 수행할 수 있습니다. 엔터티 연결. 이러한 작업에 대한 연습이 기계가 어떻게 자연어를 풀고 처리하여 공간의 일부를 이해할 수 있는지에 대한 직관을 구축하는 데 도움이 되었기를 바랍니다.\n",
    "\n",
    "오늘날 대부분의 복잡한 NLP 애플리케이션은 실무자가 이러한 작업을 수동으로 수행할 필요가 없습니다. 오히려 신경망은 이러한 \"작업\"을 스스로 수행하는 방법을 배웁니다. 다음 장에서는 Transformer 아키텍처와 fastai 및 Hugging Face의 사전 훈련된 대규모 언어 모델을 사용하여 오늘날 NLP를 시작하고 실행하는 것이 얼마나 쉬운지 보여 주는 최첨단 접근 방식에 대해 자세히 알아볼 것입니다. 이 책의 뒷부분에서 우리는 기본(이 장에서 간략하게 소개한)으로 돌아가서 NLP에 대한 기본 지식을 더 많이 쌓도록 도울 것입니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
