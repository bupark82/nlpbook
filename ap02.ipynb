{
 "cells": [
  {
   "cell_type": "raw",
   "id": "gorgeous-allah",
   "metadata": {},
   "source": [
    "[appendix]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "improved-defense",
   "metadata": {},
   "source": [
    "# CUDA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "10c63dd5",
   "metadata": {},
   "source": [
    "Throughout the book, we've mostly been using PyTorch or tools built on top of it, such as fastai and Hugging Face transformers. When we first introduced it in this book, we pitched PyTorch as a low-level framework, where you build architectures and write training loops \"from scratch\" using your knowledge of linear algebra.\n",
    "\n",
    "But PyTorch may not be the lowest level of abstraction you deal with in machine learning.\n",
    "\n",
    "PyTorch itself is written in C++, to which the CUDA language is an extension. CUDA is self-described as a \"programming model\" that allows you to write code for Nvidia GPUs. When writing your C++ code, you include certain functions called \"CUDA Kernels\" that perform a portion of the work on the GPU.\n",
    "\n",
    "> Who's That Pokemon? CUDA Kernels\n",
    "\n",
    "> A kernel is a function that is compiled for and designed to run on special accelerator hardware like GPUs (graphic processing units), FPGAs (field-programmable gate arrays), and ASICs (application-specific integated circuits). They are generally written by engineers who are very familiar with the hardware architecture, and are extensively tuned to perform a single task very well, such as matrix multiplication or convolution. CUDA kernels are kernels run on devices that use CUDA-Nvidia's GPUs and accelerators.\n",
    "\n",
    "PyTorch and many other deep learning frameworks use a handful of CUDA kernels to implement their backend, and then build a higher-level interface to a language like Python. This allows you to run super-fast, hand-tuned code on specialized hardware that experts have spent years optimizing without having to think about memory, pointers, threads, etc.\n",
    "\n",
    "There are many ohter similiar platforms, like AMS's ROCm, SYCL (an open source alternative from the Khronos Group), and, with AI hardware startups showing up in every nook and corner, many more.\n",
    "\n",
    "But CUDA is, by far, the most mature and well-developed GPU programming interface available today. In fact, it's mostly the reason that we're all forced to use Nvidia's GPUs-its software stack is just so much better than everyone else's, which makes it easier to develop libraries like PyTorch on top of it.\n",
    "\n",
    "Unless you have the bandwidth, it's not always a great idea to look for kerne-level improvements. This is probably very low on the list of things you should do if youre focus is on deploying an NLP application using existing tools and technology.\n",
    "\n",
    "But...it is useful to understand how such a critical component of the infrastructure that powers deep learning today works, and it's certainly interesting and fun. An understanding of some of the idears in CUDA may also help you debug obscure errors in you deep learning framework, and can help you make more informed purchasing decisions for hardware."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "romantic-saturn",
   "metadata": {},
   "source": [
    "## Threads and Thread Blocks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "77d917c6",
   "metadata": {},
   "source": [
    "The fundamental atom of CUDA is the thread. A thread represents a single unit of execution of a computation. Every instruction that runs in a single thread will be executed sequentially. To get massive parallelism, CUDA devices usually have a lot of threads, which all run independently.\n",
    "\n",
    "Crucially, communication between threads is hard (even on regular CPUs), and so we try to avoid this as much as possible. If you don't believe this, try to get a hundred people to agree on whether or not pineapples belong on pizza. It's hard, which is why CUDA attempts to sidestep the problem to a large degree, and is much better suited for problems that are embarrasingly parallel.\n",
    "\n",
    "> yes, \"embarrassingly parallel\" is a somewhat widely accepted technical term that you'll likely hear in a few situations. In general, it means taht the problem you're trying to solve is composed of multiple smaller tasks that don't depend on each ohter. This is true in deep learning, where we have natural parallelism across hyperparameter sets, samples in a training batch, and even across tokens in a sequence for transformers.\n",
    "\n",
    "Threads in CUDA are arranged into what are called blocks, which are themselves arranged into `grids`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suited-album",
   "metadata": {},
   "source": [
    "## Writing CUDA Kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caroline-concept",
   "metadata": {},
   "source": [
    "## CUDA in Practice"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f9d037ef",
   "metadata": {},
   "source": [
    "Writing CUDA kernels, profiling them, and tweaking your code can be fun, but you don't always need to work at this level of abstraction to extract the benefits of CUDA. The examples we showed you are much simpler than the CUDA code that is currently deployed in the real world.\n",
    "\n",
    "In Python, when we want to do matrix multiplication, we look up the docs. Maybe there are a few syntax variations, like a.matmul(b), matmul(a,b), and a@b, but that's about it. We generally don't give these methods too much thoughts.\n",
    "\n",
    "CUDA is on an entirely different plane of existence. There are multiple competing matrix multiplication algorithms, with complex heuristics for deciding which kernel to call in which scenarios. The implementation of martix multiplication that's used can vary significantly depending on the shape of the matrices, memory bandwidth, and other hardware-specific details.\n",
    "\n",
    "Thankfully, there's a slightly better abstraction layer for general-purpose GPU code: CUDA libraries. This includes CuFFT, cuDNN, cuSPARE, and more. The CUDA libraries contain highly optimized implementations of the most common algorithms you might want to run on a GPU, like convolution, Fourier transforms, matrix multiplicaiton, and more.\n",
    "\n",
    "There's also the PyTorch C++ library, libtorch, which provides even higher-level primitives like torch::Tensor. PyTorch C++ code looks surprisingly similar to PyTorch code in Python. Here's an example of a layer from the official guide (https://oreil.ly/IEdxH) to custom extensions that the PyTorch documentation refers to as long log-term memory (LLTM):\n",
    "\n",
    "```\n",
    "#include <vector>\n",
    "std::vector<at::Tensor> lltm_forward(\n",
    "    torch::Tensor input,\n",
    "    torch::Tensor weights,\n",
    "    torch::Tensor bias,\n",
    "    torch::Tensor old_h,\n",
    "    torch::Tensor old_cell\n",
    ") {\n",
    "    auto X = torch::cat({old_h, input}, /*dim=*/1);\n",
    "\n",
    "    auto gate_weights = torch::addmm(bias, X, weights.transpose(0,1));\n",
    "    auto gates = gate_weights.chunk(3, /*dim=*/1);\n",
    "\n",
    "    auto input_gate = torch::sigmoid(gates[0]);\n",
    "    auto output_gate = torch::sigmoid(gates[1]);\n",
    "    auto candidate_cell = torch::elu(gates[2], /*alpha=*/1.0);\n",
    "\n",
    "    auto new_cell = old_cell + candidate_cell * input_gate;\n",
    "    auto new_h = torch::tanh(new_cell) * output_gate;\n",
    "\n",
    "    return  {\n",
    "        new_h,\n",
    "        new_cell,\n",
    "        input_gate,\n",
    "        output_gate,\n",
    "        candidate_cell,\n",
    "        X,\n",
    "        gate_weights\n",
    "    };\n",
    "}\n",
    "```\n",
    "This is still much higher-level than the pointer manipulation you'll do in CUDA, but it can actually be very useful if you need to implement a new custom layer and find that cobbling up Python code incurs a significant performance penalty. The act of simply writing your layers with libtorch, linking into Python, and using that instead can produce a noticeable speed improvement, and this an optimization that may difinitely be worth your time.\n",
    "\n",
    "If you want take the first steps toward writing low-level GPU code in practice, but don't want to burn your prcious hours trying to figure out what the most efficient access pattern is for a half-precision Fourier transform in shared memory, CUDA libraries and libtorch are wonderful tools that you can use as you craft your next NLP creation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
