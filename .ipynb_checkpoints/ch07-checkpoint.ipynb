{
 "cells": [
  {
   "cell_type": "raw",
   "id": "professional-partnership",
   "metadata": {},
   "source": [
    "[[ch07]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reflected-blackjack",
   "metadata": {},
   "source": [
    "# Tranformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0917d8-c793-4166-9af7-24d76991d8dd",
   "metadata": {},
   "source": [
    "In the previous chapter, we covered RNNs, the modeling architecture in vogue in NLP until the Transformer architecture gained prominence.\n",
    "\n",
    "Transformers are the workhorse of modern NLP. The original architecture, first propsed in 2017, has taken the (deep learning) world by storm. Since then, NLP literature has been inundated with all sorts of new architectures that are broadly classified into either Sesame street characters or words that end with \"-former\"\n",
    "\n",
    "In this chapter, we'll look at that very architecture-the transformer-in detail. We'll analyze the core innovations and explore a hot new category of neural network layers: the attention mechanism."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floating-spread",
   "metadata": {},
   "source": [
    "## Building a Transformer from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fb3884-e59d-441a-acd9-60f0681fb626",
   "metadata": {},
   "source": [
    "In Chapter 2 and 3, we explored how to use transformers in practice and how to leverage pretrained transformers to solve complex NLP problems. Now we're going to take a deep dive into the architecture itself and learn how transformers work from first principles.\n",
    "\n",
    "What does \"first principles\" mean? Well, for starters, it means we're not allowed to use the Hugging Face Transformers library. We've raved about it plenty in this book already, so it's about time we take a break from that and see how things actually work under the hood. For this chapter, we're going to be using raw PyTorch instead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b731210-e8e9-483e-9534-413372746037",
   "metadata": {},
   "source": [
    "PyTorch, being a fully fledged deep learning library that most researchers use, naturally has an implementation of the extremly popular transformer architecture, just like a Hugging Face library does. This version, though, exposed as an nn.Module, is much more DIY and is meant to be used with the other familiar PyTorch tools like dataloaders, optimizers, etc.\n",
    "\n",
    "As we've mentioned before, one of the best ways to see what any deep learning related class/function does is by looking at the type signature and he dimensionality of the inputs and outputs. So let's do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "everyday-baking",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detailed-wilson",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Transformer()\n",
    "model.encoder.layers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a901e9-3427-4f0e-a7c4-4f401c61ceda",
   "metadata": {},
   "outputs": [],
   "source": [
    "TransformerEncoderLayer(\n",
    "    (self_attn): MultiheadAttention(\n",
    "        (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
    "    )\n",
    "    (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
    "    (dropout): Dropout(p=0.1, inplace=False)\n",
    "    (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
    "    (norm1): LayerNorm(512,), eps=1e-0.5, elementwise_affine=True)\n",
    "    (norm2): LayerNorm(512,), eps=1e-0.5, elementwise_affine=True)\n",
    "    (dropout1): Dropout(p=0.1, inplace=False)\n",
    "    (dropout2): Dropout(p=0.1, inplace=False)\n",
    ")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d6f291-30f6-48a3-a319-297d0578dbf8",
   "metadata": {},
   "source": [
    "At the outset, there doesn't seem to be too much to take away from this. It's a fairly standard PyTorch nn.Module with the standard forward() function defined for us. In principle, we could just plug it into our training pipeline and carry on. in fact, this is essentially what we did in Chapter 2. But let's try to understand the exact components of this module.\n",
    "\n",
    "Of particular interest is the MultiheadAttention layer. Most of the other layers, like Dropout, Linear, and LayerNorm, are things you'd expect to see in nontransformer models as well.This particular implementation of he transformer by PyTorch (with no additional configuration parameters), exactly matches the specification of the architecture in the original paper (shown in Figure 7-1) which, coincidentally, is titled \"Attention Is All you Need.\"\n",
    "\n",
    "In short, it's safe to say that the most important component of this Transformer class is the MultiheadAttention layer. So it makes sense to take some time to understand what that is and how it works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "formed-hygiene",
   "metadata": {},
   "source": [
    "## Attention Mechanisms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8824eb-e36f-4aaf-b75c-689242df81ba",
   "metadata": {},
   "source": [
    "An attention mechanism is a layer in a deep neural network. Its job, while still open to interpretation, is to learn long-range, \"global\" features. An attention mechanism acts as what we like to call an \"information router\" that decides what components of the input sequence of embedding vectors contribute to a single output vector. This idea will become more clear as we actually work through the details.\n",
    "\n",
    "We're just as excited to talk about attention as the other couple thousand people that attended NeurIPS within that last year, but before we de, we should mention that an important theme to pay attention to is the computaional complexity of the operations involved.Think about how many dot products/matrix multiplications you see and the size of the tensors involved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "descending-chassis",
   "metadata": {},
   "source": [
    "### Dot Product Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a1493f-17c5-450c-b817-6aeb2176506e",
   "metadata": {},
   "source": [
    "OK, strictly speaking, we don't think we've seen this type of attention actually being applied in real networks. Scaled dot product attention is usually just talked about as a component of the next thing we'll discuss: Multi-Head Self-Attention.\n",
    "\n",
    "The most important question you need to ask in the world of exotic attention mechanisms is this: how, exactly, do you measure the similarity between things? This core idea, shrouded in a veil of linear algebra and bucket-loads of GPUs, is what drives the fundamental behavior of neural nets in NLP today.\n",
    "\n",
    "And the scaled dot product uses probably one of the simplest and most intuitive methods of measureing similarity-the dot product.\n",
    "\n",
    "You should be familiar with this, but let's do a quick recap. The dot product is an operation that takes two vectors, multiples them element-wise, and then adds up the results. This measures similarity becasue if the two vectors that we're \"dot-producting\" have similar components, the product of their elements will be large, and vice versa (in the sense that vectors with dissimilar components will have a small dot product).\n",
    "\n",
    "But the real question is, what exactly are we taking the dot product of? To answer this question, let's focus on how these attention mechanisms are implemented in transformers(see Figure 7-1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf9af7f-1ec0-40a4-bfb9-687a39dd6b83",
   "metadata": {},
   "source": [
    "![image](images/transformer_layer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba00a38e-7bb4-4c32-8c16-767e4c4228c7",
   "metadata": {},
   "source": [
    "A typical transformer takes in sequences of word vectors as input, and at each layer, transforms (and no, we don't think that's how they got their name) them into another sequence of vectors, which we call the hidden representation/state.\n",
    "\n",
    "So at each hidden layer in the network, we have sequences of vectors that we want to \"attend\" over. See Figure 7-2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec6f4c9-77f6-40c9-b726-c3259af20c7c",
   "metadata": {},
   "source": [
    "![image](images/dotproduct_1-768x412.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c95b01c-9294-477d-965b-f383bc5bc6fa",
   "metadata": {},
   "source": [
    "Now, here's the important bit, so pay attention (pun very much intented).\n",
    "\n",
    "What we're going to do is transform each one of these hidden state vectors into three seperate, completely independent vectors-the query, the key, and the value.\n",
    "\n",
    "We do this transformation via a simple matrix multiply, and the dimensions of these vectors are up to us. The only restriction is that the query and key vectors need to have the same dimensions (since we're going to take the dot product between them):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blocked-acoustic",
   "metadata": {},
   "source": [
    "### Scaled Dot Product Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dimensional-seventh",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "breeding-middle",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_dots = [\n",
    "    np.dot(np.random.randn(10),\n",
    "           np.random.randn(10))\n",
    "    for i in range(100)]\n",
    "np.mean(np.absolute(small_dots))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loved-edmonton",
   "metadata": {},
   "outputs": [],
   "source": [
    "large_dots = [np.dot(np.random.randn(10000),\n",
    "                     np.random.randn(10000))\n",
    "              for i in range(100)]\n",
    "np.mean(np.absolute(large_dots))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extensive-plastic",
   "metadata": {},
   "source": [
    "### Multi Head Self Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vanilla-fruit",
   "metadata": {},
   "source": [
    "### Adaptive Attention Span"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tired-helicopter",
   "metadata": {},
   "source": [
    "### Persistent Memory/All-Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stuffed-trigger",
   "metadata": {},
   "source": [
    "### Product-Key Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "binding-vacuum",
   "metadata": {},
   "source": [
    "## Transformers for Computer Vision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77022a5d-a34f-4c1f-87a1-4e537eb4d3ce",
   "metadata": {},
   "source": [
    "# Fine-tuning BERT for sentiment analysis\n",
    "\n",
    "## Contents\n",
    "1. 학습데이터 확인\n",
    "2. 보조함수 확인\n",
    "3. SenitmentClassifier 확인\n",
    "4. Analyser 확인\n",
    "5. 학습 과정 확인\n",
    "6. tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a19e501-b550-427c-b4dd-e328536b7133",
   "metadata": {},
   "source": [
    "## 학습 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e7b518-8c06-4a6a-a661-1efc6b058d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install torch\n",
    "!pip3 install transformers\n",
    "from typing import List, Tuple\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "DATA: List[Tuple[str, int]] = [\n",
    "    # 긍정적인 문장 - 1\n",
    "    (\"난 너를 좋아해\", 1),\n",
    "    # --- 부정적인 문장 - 레이블 = 0\n",
    "    (\"난 너를 싫어해\", 0)\n",
    "]\n",
    "\n",
    "TESTS = [\n",
    "    \"나는 자연어처리가 좋아\",\n",
    "    \"나는 자연어처리가 싫어\",\n",
    "    \"나는 너가 좋다\",\n",
    "    \"너는 참 좋다\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1c9831-f8a8-477e-b7b5-9277b590a3a5",
   "metadata": {},
   "source": [
    "## 보조함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9ae08b-1d45-4d5c-a6a2-00d404deb786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cuda를 사용할 수 있는지를 체크, 사용가능하다면 cuda로 설정된 device를 출력.\n",
    "def load_device() -> torch.device:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    return device\n",
    "\n",
    "\n",
    "# 텐서를 구축하는 부분 - X\n",
    "def build_X(sents: List[str], tokenizer: BertTokenizer, device: torch.device) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    return X (N, 3, L).  N, 0, L -> input_ids / N, 1, L -> token_type_ids / N, 2, L -> attention_mask\n",
    "    \"\"\"\n",
    "    encodings = tokenizer(text=sents,\n",
    "                          add_special_tokens=True,\n",
    "                          return_tensors='pt',\n",
    "                          truncation=True,\n",
    "                          padding=True)\n",
    "    return torch.stack([\n",
    "        encodings['input_ids'],\n",
    "        encodings['token_type_ids'],\n",
    "        encodings['attention_mask']\n",
    "    ], dim=1).to(device)\n",
    "\n",
    "# 텐서를 구축하는 부분 - y\n",
    "def build_y(labels: List[int], device: torch.device) -> torch.Tensor:\n",
    "    return torch.FloatTensor(labels).unsqueeze(-1).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e373fc-a77b-4f7a-b9b1-cf7e2ed16197",
   "metadata": {},
   "source": [
    "## Sentiment Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d5389f-b169-4d33-8dc6-28d0bb54e2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentClassifier(torch.nn.Module):\n",
    "    def __init__(self, bert: BertModel):\n",
    "        super().__init__()\n",
    "        self.bert = bert\n",
    "        self.hidden_size = bert.config.hidden_size\n",
    "        # TODO 1\n",
    "        self.W_hy = torch.nn.Linear(..., ...)\n",
    "\n",
    "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        :param X: (N, 3, L)\n",
    "        :return: H_all (N, L, H)\n",
    "        \"\"\"\n",
    "        input_ids = X[:, 0]\n",
    "        token_type_ids = X[:, 1]\n",
    "        attention_mask = X[:, 2]\n",
    "        H_all = self.bert(input_ids, token_type_ids, attention_mask)[0]\n",
    "        return H_all\n",
    "\n",
    "    def predict(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        :param X:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # TODO 2\n",
    "        H_all = self.forward(X)\n",
    "        H_cls = ...\n",
    "        y_hat = ...  # (N, H) * (H, 1) -> (N, 1)\n",
    "        return torch.sigmoid(y_hat)\n",
    "\n",
    "    def training_step(self, X: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "        # TODO 3\n",
    "        y_hat = self.predict(X)\n",
    "        loss = ...\n",
    "        return loss.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a8091b-e8bc-4a94-ad31-cb8ef645869e",
   "metadata": {},
   "source": [
    "## Analyser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e457d3aa-ab21-4d18-909b-a6ede707cd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Analyser:\n",
    "    \"\"\"\n",
    "    Bert 기반 감성분석기.\n",
    "    \"\"\"\n",
    "    def __init__(self, classifier: SentimentClassifier, tokenizer: BertTokenizer, device: torch.device):\n",
    "        self.classifier = classifier\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "\n",
    "    def __call__(self, text: str) -> float:\n",
    "        X = build_X(sents=[text], tokenizer=self.tokenizer, device=self.device)\n",
    "        y_hat = self.classifier.predict(X)\n",
    "        return y_hat.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ba24e5-8478-442f-9de0-5d18cb35c8a5",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4584485e-ba13-473c-a64c-8e7add28d19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사전학습된 버트 모델을 로드\n",
    "tokenizer = BertTokenizer.from_pretrained('beomi/kcbert-base')\n",
    "bert = BertModel.from_pretrained('beomi/kcbert-base')\n",
    "\n",
    "# --- have a look at the config --- #\n",
    "print(bert.config)\n",
    "print(bert.config.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771333dd-2e87-41a9-8fe3-2eb3097437b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- hyper parameters --- #\n",
    "EPOCHS = 20\n",
    "LR = 0.0001\n",
    "\n",
    "\n",
    "device = load_device()\n",
    "print(device)\n",
    "\n",
    "# --- build the dataset --- # \n",
    "sents = [sent for sent, _ in DATA]\n",
    "labels = [label for _, label in DATA]\n",
    "X = build_X(sents, tokenizer, device)\n",
    "y = build_y(labels, device)\n",
    "\n",
    "# --- instantiate the classifier --- #\n",
    "classifier = SentimentClassifier(bert)\n",
    "classifier.to(device)  # 모델도 gpu에 올리기. \n",
    "optimizer = torch.optim.Adam(classifier.parameters(), lr=LR)  # 최적화 알고리즘을 선택.\n",
    "\n",
    "# --- 학습시작 --- #\n",
    "for epoch in range(EPOCHS):\n",
    "    loss = classifier.training_step(X, y)\n",
    "    loss.backward()  # 오차 역전파\n",
    "    optimizer.step()  # 경사도 하강\n",
    "    optimizer.zero_grad()  # 기울기 축적방지\n",
    "    print(f\"epoch:{epoch}, loss:{loss.item()} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3d7d13-4b45-4cbe-ba91-02c0670257ba",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9040b5-d9f0-4e3e-b920-e8382b9e124d",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.eval()\n",
    "analyser = Analyser(classifier, tokenizer, device)\n",
    "\n",
    "for sent in TESTS:\n",
    "    print(sent, \"->\", analyser(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identical-indicator",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
