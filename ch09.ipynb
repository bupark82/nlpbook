{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[[ch09]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tools of the Trade"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the preceding section, we covered all the foundational elements of NLP and how to develop NLP models. Starting with this chapter, we'll cover what you should begin to think about as you come out of the wonderful world of training magnificent models on carefully curated datasets and into the mess that is the real world.\n",
    "\n",
    "In this chapter specifically, we will discuss mainstream machine learning software and the choices you will face as you decide what to include in your stack. Then, in Chapter 10, we build custom web apps for machine learning and data science using an easy-to-use open source Python library called Streamlit (https://streamlit.io), and we will conclude this section (in Chapter 11) with model deployment at scale using software from the industry leader, Databricks (https://databricks.com). By the end of these three chapters, you will have a good understanding of how to productionize machine learning models as web apps, APIs, and machine learning pipelines.\n",
    "\n",
    "Let's start with a topic many developers love spending inordinate amounts of time arguing over: tools.\n",
    "\n",
    "People who should probably be spending their time coding, love hashing out the standard TensorFlow versus PyTorch or best programming language debates on endlessly long Twitter threads, but we want to take a step back and talk about some of the more practical decisions you'll have to wrestle with in the real world. After all, \"applied\" is in the title of this book.\n",
    "\n",
    "Here are a few obligatory disclaimers:\n",
    "\n",
    "- It is almost certain that what we recommend today will become outdated over time. Instead of being overly prescriptive with our advice, we want to help you develop intuition for what matters when you make decisions about to include in your tech stack.\n",
    "\n",
    "- We recoginze that you probably have your own set of restrictions-for example, your company may already have a set of tools you are obligated to use. Or, you might be part of a large team where the choice of programming language, cloud provider, etc., has already been made for you. But hopefully, this chapter will still provide you with a sense of what else is out there.\n",
    "\n",
    "- Making choices for your tech stack can be overwhelming. There are so many different providers offering similar competing services, and the prices and features they offer change frequently. This makes picking the absolute best a nearly impossible exercise. The sheer variety of competing providers can lead to decision fatigue. We will try to do our best to keep the choices you have to make to a minimum. In fact, we will go a step further. We, the authors of this book, will pick our own favorite tools to work with when building NLP applications!\n",
    "\n",
    "- The list here is neither comprehensive nor definitive (nor is it in any particular order or ranking). The tools we list here are simply the ones that we, the authors, have found useful, popular, or interesting. The decision on what to use is, as always, up to you.\n",
    "\n",
    "- What may work for one person in the field may not work for you. Take our suggestions with a grain of salt, and think critically about what makes the most sense for you.\n",
    "\n",
    "- In the end, what's most important is not what tools you use but how you use them. In fact, you'll find that a lot of the deep learning frameworks, programming languages, etc., are often very similar, and it's not too hard to learn one once you've learned another.\n",
    "\n",
    "We've split the tools info a few categories and have listed a few under each. At the end of each section, you'll find two specific recommendations, labeled \"Ankur's Pick\" and \"Ajay's Pick\" in the classic style of The Motley Fool (https://www.fool.com). These are our individual personal favorites:\n",
    "\n",
    "Ankur's picks  \n",
    "These wil trend to be more production-oriented, with a focus on tools that are stable and popular in industry, and that scale well.\n",
    "\n",
    "Ajay's picks  \n",
    "These will be more experimental and research-oriented. These tools are designed for rapid experimentation and prototyping and will help you stay on the bleeding edge of modern research.\n",
    "\n",
    "By the end of this chapter, you should be well aquainted with the landscape of tools available to you as you build NLP applications, both to prototyping and to deploy in production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Frameworks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with the deep learning frameworks. These frameworks are the core building blocks for nearly all of NLP (that's relevant to us), and we will use them extensively throughout this book. Most deep learning frameworks do the same exact thing-they perform tensor computations on GPUs.\n",
    "\n",
    "What differentiates them is the way they implement the various high-level features and abstractions as well as how they manage the less obvious backend implementation that governs the actual performance of your code.\n",
    "\n",
    "Over the last decade, multiple frameworks have phased in and out of existence. Older and increasingly less popular ones that you might have heard of in passing are Theano, Chainer, Lua, Torch, and Cafee. As of 2020, we think these smaller frameworks are, for the most part, obsolete and not worth exploring in great detail. \n",
    "\n",
    "The big ones that you're familiar with and perhaps have used already are PyTorch and TensorFlow. These two frameworks were launched by two of the most successful technology companies today-Facebook and Google, respectively. Partly because of developer communities to adopt and support their deep learning frameworks. Both frameworks have several things in common: they are both open source and interface with Python as the primary programming language. But there are a few differences between the two, which we will highlight in detail.\n",
    "\n",
    "> PyTorch is based on the Torch framework, and TensorFlow is based on the Theano framework. Even though Torch and Theano have declined in popularity, their derivatives are now the most dominant in the deep learning space.\n",
    "\n",
    "However, there are also a few new kids on the block, which you're probably less familiar with. Jax, Julia, and Swift for TensorFlow all promise killer new features, far better performance/speed, and are fairly drastic departures from what we've seen so far. They are still not as fleshed out as PyTorch and TensorFlow in terms of stability, comunity, and hardware support, but they show a lot of potential and have good development momentum, so be prepared to dip your toes in those as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with PyTorch, the fastest growing deep learning framework over the past several years. It was developed by Facebook' AI Research lab(FAIR) and released publicly in October 2016. The consensus is that PyTorch is now more popular among researchers.\n",
    "\n",
    "At the core of PyTorch lies the torch.tensor object. It's a type of multidimensional array, almost identical to a numpy.ndarray, that can live in GPU memory and be used for fast parallel computation. Almost all of PyTorch is built for manipulating these tensors with operations such as matrix multiplicaiton, convolution, etc.\n",
    "\n",
    "The other big component of PyTorch is autograd. This feature automatically calculates a quantity called the gradient whenever you use PyTorch tensor operations, which is extremely useful for training neural networks.\n",
    "\n",
    "Beyond this, the easiest way to describe PyTorch would be to call it \"NumPy on the GPU\" with added convenience functions for deep learning. Typically, deep learning involves repeatedly performing similar computations on large tensors, which is where GPUs excel. NumPy performs computation on the CPU, which, in most cases, is much slower than running lower-precision computations in parallel on the GPU.\n",
    "\n",
    "For most Python programmers, PyTorch will feel natural and \"Pythonic\" since its interface is very similar to NumPy. This is one of the main reasons that PyTorch has continued rising in popularity over the last few years, despite the fact taht it was released after TensorFlow.\n",
    "\n",
    "Both PyTorch and TensorFlow offer distributed computation features, but PyTorch has better optimization for training because it has native support for asynchronous execution.\n",
    "\n",
    "The job of deep learning frameworks can be described as executing a \"graph\" of computations on tensor data structures. In PyTorch you deine the graph at runtime, which allows you to easily go back and forth between planning and execution. the ability to evaluate operations immediately, without compiling graphs explicitly, is known as eager execution.\n",
    "\n",
    "Eager execution allows you to prototype faster and create new types of architectures, but at the cost of speed. Think of this as the difference between compiled and interpreted languages.\n",
    "\n",
    "This used to be a big deal a few years ago since TensorFlow used static graphs back then, requiring you to define the entire graph first before pushing data through. However, both frameworks now support eager execution by default, and this has since been adopted as the go-to industry standard.\n",
    "\n",
    "Following are itemized lists of things to consider before you start using PyTorch. First the pros:  \n",
    "\n",
    "- Easier to learn and more intuitive; Python-like coding  \n",
    "- Dynamic graph  \n",
    "- Exellent for fast experimentation and prototyping  \n",
    "- Requires less reading through documentation  \n",
    "- Better integration with other Python packages  \n",
    "- Rapidly gaining popularity among researchers  \n",
    "\n",
    "Here are some of the cons of using PyTorch:  \n",
    "\n",
    "- Relies on third party for visualization (e.g., Visdom)  \n",
    "- Has a less-robust native system for edge device deployments (requires API server)\n",
    "\n",
    "Now, let's compare PyTorch with TensorFlow, the framework that remains the most popular in the industry today despite PyTorch's rapid ascent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Developed by the Google Brain team for internal Google use, TensorFlow 1.x was released in late 2015. It has a larger user base in industry, though this is likely due to the fact that it was released earlier and many companies have existing TensorFlow experience and legacy code. For the same reasons, TensorFlow has a larger community base overall. \n",
    "\n",
    "In general, we would not recommend the 1.x version of TensorFlow, since it has a very bloated API and is generally more verbose and less user friendly than PyTorch (and actually slower in some cases due to problems with the backend).\n",
    "\n",
    "However, with TensorFlow 2.0, the differencies between TensorFlow and PyTorch have narrowed. TensorFlow now offers the ability to build dynamic graphs, instead of static graphs. TensorFlow 2.0 also fully integrates Keras, a ery popular high-level API for TensorFlow. While TensorFlow 2.0 has resolved a lot of its issues with a complete redesign of the framework, it has also faced criticism for the drastic changes it introduced, which breaks nearly all 1.x code.\n",
    "\n",
    "Compared to PyTorch, TensorFlow has excellent built-in visualization capabilities (e.g., TensorBoard) and has better support for mobile platforms with TensorFlow Lite (though this is changing with PyTorch Mobile). Because of this, TensorFlow can be easier to deploy in a production setting thanks to tools like TensorFlow Serving, which uses REST client APIs.\n",
    "\n",
    "TensorFlow, in general, consists of a lot more than the Python framework, Though. There are now more variants than we can count, including TensorFlow Lite, TensorFlow Extended, TensorFlow Serving, TensorFlow.js, TensorFlow.jl, TensorFlow Probability, and many more. This could be a helpful ecosystem or a confusing nuisance to deal with, depending on your perspective.\n",
    "\n",
    "We recommend TensorFlow to developers that are ready to build production-ready applications and who may have existing code/infrastructure built on the TensorFlow ecosystem.\n",
    "\n",
    "Following are itemized lists of things to consider before you start using TensorFlow. First, the pros:  \n",
    "\n",
    "- With Keras in TensorFlow 2.0, has a simple built-in high-level API  \n",
    "- Now supports eager mode  \n",
    "- Excellent visualization (TensorBoard)  \n",
    "- Production-ready (TensorFlow Serving)  \n",
    "- Great mobile support  \n",
    "- Large developer communit and comprehensive documentation  \n",
    "- Has better performance at very large scale  \n",
    "- The dominant framework in industry  \n",
    "\n",
    "Following are some cons in using TensorFlow:  \n",
    "\n",
    "- Many people complain that TensorFlow still carrier the baggage from its 1.x version, which was completely different from the TensorFlow we have today and was generally much harder to use.  \n",
    "- It has a steeper learning curve, and can feel at tims like a new language.  \n",
    "\n",
    "While PyTorch and TensorFlow are the two most popular deep learning frameworks available today, let's explore some of the fast-rising newcomers that may eventually challenge the incumbents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Julia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization and Experiment Tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights & Biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neptune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H2O.ai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataiku"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataRobot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Infrastructure and Compute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PaperSpace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FloydHub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kaggle Kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lambda GPU Cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our Pick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edge / On-Device Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ONNX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Core ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edge Accelerators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cloud Inference & Machine Learning as a Service (MLaaS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AWS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Microsoft Azure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Cloud Platform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CI/CD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
