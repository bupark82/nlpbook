{
 "cells": [
  {
   "cell_type": "raw",
   "id": "rational-truck",
   "metadata": {},
   "source": [
    "[[ch08]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minute-saint",
   "metadata": {},
   "source": [
    "# BERTology: Putting it all Together"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ce66001a",
   "metadata": {},
   "source": [
    "함께, 1장에서 `spacy`를 만지작거리기 시작한 이래로 먼 길을 왔습니다. 우리는 딥 러닝 라이브러리와 동등한 전자레인지 식사를 사용하여 가장 일반적인 NLP 문제를 해결하는 것으로 시작한 다음 토큰화 및 임베딩을 포함한 낮은 수준의 세부 사항으로 진행했습니다. 그 과정에서 우리는 RNN, LSTM 및 GRU를 포함한 순환 네트워크와 Transformer 아키텍처 및 어텐션 메커니즘을 다루었습니다.\n",
    "\n",
    "여러 면에서 이 장은 그랜드 피날레입니다. 우리는 모든 조각을 하나로 묶고 2018년 소위 ImageNet 순간으로 이끈 단계를 역추적할 것입니다. 그 이후로 NLP에서 이러한 발전의 잠재적인 상업적 응용 프로그램에 대한 흥분으로 이어졌습니다. 이러한 가능성 중 일부에 대해서도 다룰 것입니다. 시작하자."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advance-western",
   "metadata": {},
   "source": [
    "## ImageNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "delayed-friendly",
   "metadata": {},
   "source": [
    "### The Power of Pretrained Models "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nominated-league",
   "metadata": {},
   "source": [
    "## The Path to NLP’s ImageNet Moment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alpha-armstrong",
   "metadata": {},
   "source": [
    "## Pretrained Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unique-primary",
   "metadata": {},
   "source": [
    "### The Limitations of One-Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "familiar-indie",
   "metadata": {},
   "source": [
    "### word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simplified-amber",
   "metadata": {},
   "source": [
    "### GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cooked-syndrome",
   "metadata": {},
   "source": [
    "### fastText"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocal-phase",
   "metadata": {},
   "source": [
    "### Context-aware Pretrained Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demanding-reverse",
   "metadata": {},
   "source": [
    "## Sequential Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minor-holly",
   "metadata": {},
   "source": [
    "### Sequential Data and the Importance of Sequential Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distinguished-favorite",
   "metadata": {},
   "source": [
    "## RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experienced-remains",
   "metadata": {},
   "source": [
    "### Vanilla RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "permanent-divorce",
   "metadata": {},
   "source": [
    "### LSTMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mechanical-inventory",
   "metadata": {},
   "source": [
    "### GRUs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exclusive-manner",
   "metadata": {},
   "source": [
    "## Attention Mechanisms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sensitive-deadline",
   "metadata": {},
   "source": [
    "## Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sudden-academy",
   "metadata": {},
   "source": [
    "### Transformer-XL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finnish-railway",
   "metadata": {},
   "source": [
    "## NLP’s ImageNet Moment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "absolute-sphere",
   "metadata": {},
   "source": [
    "### ULMFiT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rotary-compilation",
   "metadata": {},
   "source": [
    "### ELMo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hairy-speed",
   "metadata": {},
   "source": [
    "### BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "breathing-avenue",
   "metadata": {},
   "source": [
    "### BERTology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "measured-feedback",
   "metadata": {},
   "source": [
    "### GPT-1, GPT-2, GPT-3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "74589d21",
   "metadata": {},
   "source": [
    "OpenAI는 또한 자체 Transformer 기반 모델을 설계하여 NLP 경쟁에 뛰어들었습니다. 이러한 모델은 Generative Pretrained Transformer의 줄임말인 GPT 모델로 알려져 있습니다. 첫 번째 GPT 모델인 GPT-1은 2018년에 출시되었으며 ULMFiT과 유사한 감독되지 않은 사전 훈련 및 감독된 미세 조정 프로세스를 사용했습니다. GPT-2는 2019년에 출시되었습니다. 이전 버전에 비해 더 많은 데이터와 더 많은 매개변수로 훈련하여 제로 샷 설정에서 많은 작업에서 최첨단 성능을 달성하는 데 도움이 되었습니다.\n",
    "\n",
    "> 제로 샷 학습에서 모델은 학습할 예제가 제공되지 않지만 주어진 지침에 따라 수행할 작업을 이해해야 합니다. 예를 들어 제로 샷 학습 작업에는 영어 문장을 독일어로 번역하는 모델이 필요할 수 있지만 모델에는 학습할 영어-독일어 문장이 제공되지 않습니다. 퓨샷 학습에서는 모델에 학습할 몇 가지 예가 제공되지만 일반적으로 많지는 않습니다.\n",
    "\n",
    "OpenAI는 2020년에 GPT-3를 출시했습니다. GPT-2와 비교할 때 GPT-3는 훨씬 더 큰 데이터 세트에서 학습했으며 더 많은 수의 매개 변수를 가졌습니다. GPT-3는 이전 모델을 능가하고 zero-shot 및 few-shot 학습의 새로운 표준을 세웠습니다. 현재까지 가장 성능이 좋은 생성적인 NLP 모델로 간주됩니다.\n",
    "\n",
    "> 아시다시피 모델은 점점 더 커지고 수년에 걸쳐 점점 더 많은 데이터에 대해 학습했습니다. 이것은 설계 변경, 더 큰 모델 및 더 많은 데이터와 함께 NLP에서 최첨단 성능을 추진하는 데 도움이 되었습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fresh-running",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e8b850e1",
   "metadata": {},
   "source": [
    "이 장에서는 단어 임베딩, RNN, 어텐션 메커니즘, Transformer 아키텍처 및 문맥화된 단어 표현을 포함하여 책의 모든 주요 개념을 함께 묶었습니다. 종합적으로 이러한 발전은 2018년에 NLP의 ImageNet 순간을 가져오는 데 도움이 되었으며, 사전 훈련된 대규모 언어 모델이 대중에게 공개되고 NLP 벤치마크에서 새로운 성능 기록을 세웠습니다.\n",
    "\n",
    "사전 훈련된 언어 모델의 등장으로 응용 NLP 엔지니어는 도메인별 NLP 작업에서 대규모 모델을 미세 조정하고 놀라운 성능을 달성할 수 있게 되었습니다. 이제 NLP 모델을 개발하기 위해 알아야 할 주요 NLP 개념을 다루었으므로 NLP 모델을 개발한 후 프로덕션화하는 방법에 대해 논의해 보겠습니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
