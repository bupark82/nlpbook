{
 "cells": [
  {
   "cell_type": "raw",
   "id": "rational-truck",
   "metadata": {},
   "source": [
    "[[ch08]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minute-saint",
   "metadata": {},
   "source": [
    "# BERTology: Putting it all Together"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ce66001a",
   "metadata": {},
   "source": [
    "함께, 1장에서 `spacy`를 만지작거리기 시작한 이래로 먼 길을 왔습니다. 우리는 딥 러닝 라이브러리와 동등한 전자레인지 식사를 사용하여 가장 일반적인 NLP 문제를 해결하는 것으로 시작한 다음 토큰화 및 임베딩을 포함한 낮은 수준의 세부 사항으로 진행했습니다. 그 과정에서 우리는 RNN, LSTM 및 GRU를 포함한 순환 네트워크와 Transformer 아키텍처 및 어텐션 메커니즘을 다루었습니다.\n",
    "\n",
    "여러 면에서 이 장은 그랜드 피날레입니다. 우리는 모든 조각을 하나로 묶고 2018년 소위 ImageNet 순간으로 이끈 단계를 역추적할 것입니다. 그 이후로 NLP에서 이러한 발전의 잠재적인 상업적 응용 프로그램에 대한 흥분으로 이어졌습니다. 이러한 가능성 중 일부에 대해서도 다룰 것입니다. 시작하자."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advance-western",
   "metadata": {},
   "source": [
    "## ImageNet"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "79735dd5",
   "metadata": {},
   "source": [
    "\"ImageNet 순간\"이 의미하는 바를 명확히 하기 위해 잠시 시간을 할애할 가치가 있습니다. ImageNet은 2009년에 처음 발표된 컴퓨터 비전 데이터 세트입니다. 그것은 핵심 컴퓨터 비전 작업인 이미지 분류의 발전에 대한 벤치마크가 되었으며, 어떤 연구팀이 가장 낮은 오류율로 데이터 세트의 이미지에서 물체를 가장 잘 식별할 수 있는지 알아보기 위해 매년 컴퓨터 비전 대회를 열었습니다. \n",
    "\n",
    "경쟁의 높은 가시성은 2010년부터 컴퓨터 비전 분야의 상당한 발전에 박차를 가하는 데 도움이 되었습니다. 2009년부터 2017년까지 우승 정확도는 71.8%에서 97.3%로 뛰어올라 인간의 능력(초인적 능력 달성)을 능가하고 기계 학습이 무엇을 할 수 있는지에 대한 세계의 상상력을 사로잡았습니다.\n",
    "\n",
    "돌이켜보면 2012년은 컴퓨터 비전과 소위 ImageNet의 원조라는 획기적인 해였습니다. 2012년 토론토 대학의 Geoffrey Hinton, Ilya Sutskever, Alex Krizhevsky가 이끄는 팀은 10.8% 차이로 나머지 필드를 이겼습니다.\n",
    "\n",
    "이 성능은 AI 연구 커뮤니티에 충격을 주었고 더 상업적인 기업이 컴퓨터 비전에 더 많은 관심을 갖도록 설득하기 시작했습니다. 향후 몇 년 동안 기업은 모델이 명시적으로 훈련되지 않은 작업을 포함하여 다양한 컴퓨터 비전 작업을 해결하기 위해 사전 훈련된 ImageNet 모델을 사용했습니다. 즉, ImageNet은 컴퓨터 비전이 나머지 세계의 관심을 끌도록 도와주는 성능 및 적용 용이성 장벽을 돌파했을 때였습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "delayed-friendly",
   "metadata": {},
   "source": [
    "### The Power of Pretrained Models "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b4d01b2d",
   "metadata": {},
   "source": [
    "2012년이 컴퓨터 비전의 돌파구였던 것과 마찬가지로 2018년은 NLP의 돌파구였습니다. 올해는 NLP가 기업에서 달성할 수 있는 것에 대해 세계가 훨씬 더 진지한 관심을 기울이기 시작한 해였으며 그 이후로 상용 응용 프로그램을 위한 NLP에 대한 매우 적극적인 관심이 있었습니다. 소위 ImageNet 순간이라고 하는 NLP의 획기적인 순간으로 이어진 사건을 되짚어 봅시다.\n",
    "\n",
    "2018년 이전에는 특정 NLP 작업을 해결하기 위해 NLP 모델을 대부분 처음부터 훈련해야 한다는 것이 주류였습니다. 특정 작업에 대한 모델을 개발하는 데 도움이 되도록 다른 언어 모델에서 재사용할 항목이 거의 없었습니다. 다른 언어 모델에서 이전할 가치가 있었던 유일한 것은 모델을 시작하는 데 도움이 될 수 있지만 제한된 가치를 제공하는 사전 훈련된 단어 임베딩이었습니다.\n",
    "\n",
    "이것은 기업에서 특정 NLP 작업을 해결하는 데 주요 문제를 제시했습니다. 대부분 처음부터 모델을 교육하려면 당면한 특정 작업에 대해 주석이 달린 많은 데이터가 필요했기 때문입니다. 이 대용량 주석 데이터가 없으면 모델을 처음부터 충분히 좋은 수준의 성능으로 교육할 수 없습니다. 그러나 그렇게 많은 양의 주석이 달린 데이터를 얻는 것은 많은 회사에서 시작도 할 수 없는 일이었고 기업에서 NLP의 적용 가능성을 제한했습니다.\n",
    "\n",
    "NLP의 이 처음부터 훈련하는 패러다임은 2017년 후반에 교리를 깨뜨린 컴퓨터 비전의 레버리지 사전 훈련 모델 패러다임과 극명하게 대조되었습니다. 컴퓨터 비전에서 컴퓨터 비전 모델을 처음부터 훈련시키는 것은 어리석은 것으로 간주되었습니다. 대신, 응용 기계 학습 엔지니어는 특정 작업을 위한 컴퓨터 비전 모델을 개발하기 위해 가장자리 및 모양 식별과 같은 컴퓨터 비전의 기본 요소 중 일부를 이미 학습한 사전 훈련된 대규모 컴퓨터 비전 모델의 처음 여러 계층을 활용합니다.\n",
    "\n",
    "이렇게 미리 훈련된 모델에서 새로운 모델로 \"지식\"의 일부를 이전하는 데 필요한 것보다 주석이 적은 데이터가 필요했고 기업에서 컴퓨터 비전의 채택을 개선했습니다. 불행하게도 2017년 말에는 NLP에서는 사전 훈련된 모델에서 이러한 지식을 이전할 수 없었기 때문에 팀이 특정 모델을 처음부터 훈련하기 위해 주석이 달린 많은 데이터를 수집해야 했습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nominated-league",
   "metadata": {},
   "source": [
    "## The Path to NLP’s ImageNet Moment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "73ca99d1",
   "metadata": {},
   "source": [
    "2018년에 NLP 연구자들이 사전 훈련된 언어 모델을 사용하여 광범위한 NLP 작업에서 최첨단 결과를 달성할 수 있음을 보여주면서 주류 관점이 극적으로 바뀌었습니다. 특정 NLP 문제를 해결하기 위해 언어 모델을 처음부터 훈련할 필요가 없었습니다. 이는 컴퓨터 비전 엔지니어가 사전 훈련된 ImageNet 모델을 활용하여 다양한 컴퓨터 비전 작업을 해결하는 것처럼 이제 응용 기계 학습 팀이 사전 훈련된 언어 모델을 활용하여 다양한 NLP 작업을 해결할 수 있기 때문에 NLP에 분수령이 되었습니다. 사전 훈련된 언어 모델의 여러 계층을 재사용함으로써 응용 NLP 과학자와 엔지니어는 특정 NLP 문제를 해결하기 위해 훨씬 적은 주석 데이터가 필요했습니다. NLP에서 이전에 다루기 어려웠던 문제는 해결하기에 무르익었습니다.\n",
    "NLP에서 이 획기적인 순간으로 이어진 이유를 이해하기 위해 지난 몇 년 동안 NLP의 발전을 되짚어 보겠습니다. 이렇게 하면 이 책의 주요 개념을 함께 연결하여 해당 분야에 대한 이해를 심화할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alpha-armstrong",
   "metadata": {},
   "source": [
    "## Pretrained Word Embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "255e96e1",
   "metadata": {},
   "source": [
    "NLP의 첫 번째 단계 중 하나는 5장에서 다룬 토큰화입니다. 토큰화는 텍스트를 개별 단위(예: 단어, 구두점 등)로 분해한 후 NLP 알고리즘을 적용하여 텍스트의 구조를 학습할 수 있습니다. 각 토큰을 나타내는 방법을 포함합니다.\n",
    "\n",
    "각 토큰을 나타내는 방법을 배우는 것은 일반적으로 NLP의 두 번째 단계입니다. 이 과정을 6장에서 다룬 '단어 임베딩 학습'(즉, 단어 벡터)이라고 합니다. 단어 임베딩은 단어 간의 관계를 포착하기 때문에 NLP에서 매우 중요합니다. 모델이 단어 간의 관계를 학습하지 않으면 텍스트 분류와 같은 더 복잡한 NLP 작업을 수행할 수 없습니다.\n",
    "\n",
    "2013년 이전에 NLP 연구자들은 그들이 수행한 많은 작업을 위해 처음부터 자신의 단어 임베딩을 훈련해야 했습니다. 2013년부터 사전 훈련된 단어 임베딩이 눈에 띄기 시작하여 NLP 연구원이 모델 개발에 이를 활용하여 훈련 프로세스를 가속화할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unique-primary",
   "metadata": {},
   "source": [
    "### The Limitations of One-Hot Encoding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "692c8789",
   "metadata": {},
   "source": [
    "이러한 사전 훈련된 단어 임베딩에 대해 알아보기 전에 단어의 간단한 원-핫 벡터 인코딩이 의미 있는 단어 벡터를 생성하는 데 최적의 접근 방식이 아닌 이유를 살펴보겠습니다. 대규모 코퍼스의 모든 단어에 대해 원-핫 인코딩을 적용해야 하는 경우 인코딩 매트릭스의 차원은 고유한 단어의 수와 같을 것이며 작업하기에 상당히 방대하고 비실용적일 것입니다.\n",
    "\n",
    "예를 들어 코퍼스에 400,000개의 고유 단어가 있는 경우 원-핫 인코딩 매트릭스의 차원은 400,000개로 매우 큽니다. 이 행렬은 희소 행렬(대부분 제로)이며 차원의 저주에 시달릴 것입니다.(예를 들어, 이 행렬이 크고 희소하여 매개변수 추정을 더 어렵게 만들기 때문에 잘 일반화되는 모델을 훈련하려면 많은 데이터가 필요합니다.)\n",
    "\n",
    "높은 차원 외에도 원-핫 인코딩 매트릭스는 단어의 의미론적 속성을 캡처하지 않습니다. 예를 들어, \"queen\"과 \"king\"은 직교하는 벡터를 가지므로 실제로 관련되어 있을 때 완전히 다르다는 것을 의미합니다.\n",
    "\n",
    "대조적으로 Word2Vec, GloVe 및 fastText와 같은 알고리즘으로 훈련된 단어 임베딩은 컨텍스트 정보를 훨씬 낮은 차원 공간에 저장합니다. 400,000개의 고유 단어로 구성된 동일한 어휘에 대해 원-핫 인코딩에 필요한 400,000차원보다 훨씬 적은 수백 차원만 사용하여 각 단어에 대한 컨텍스트 정보를 저장할 수 있습니다.\n",
    "\n",
    "또한 Word2Vec, GloVe 및 fastText로 훈련된 단어 임베딩은 원-핫 인코딩과 달리 각 단어에 대한 의미론적 정보를 저장합니다. \"queen\" 및 \"king\"과 같은 단어는 공간에서 서로 더 가까운 벡터를 가지며, 둘 사이에 의미론적 관계/유사성이 있음을 암시합니다. 이 의미론적 속성을 캡처함으로써 Word2Vec, GloVe 및 fastText로 훈련된 단어 임베딩은 원-핫 인코딩에 비해 언어의 구조를 더 많이 캡처합니다. 이것이 이러한 단어 임베딩이 2013년부터 NLP 분야를 실질적으로 발전시키는 데 도움이 된 방식입니다. 의미론적 정보를 캡처하는 미리 훈련된 단어 임베딩이 NLP 커뮤니티의 연구원들에게 널리 사용되기 시작했습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "familiar-indie",
   "metadata": {},
   "source": [
    "### word2vec"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "37a1d5d7",
   "metadata": {},
   "source": [
    "> 단어는 그것이 유지하는 회사에 의해 특징지어집니다.\n",
    "> - John Rupert Firth\n",
    "\n",
    "2013년에는 최초의 주요 단어 임베딩 알고리즘인 Word2Vec이 등장하면서 사전 훈련된 단어 임베딩이 대중화되었습니다. 6장에서 기억할 수 있듯이 Word2Vec은 대규모 텍스트 코퍼스에서 단어 연관성을 학습하는 데 사용되는 매우 효율적인 알고리즘입니다. 각각의 고유한 단어는 벡터로 표시됩니다(따라서 Word2Vec은 \"word to vector\"의 줄임말입니다). Word2Vec 및 기타 단어 임베딩 알고리즘을 단어에 대한 감독되지 않은 기능 추출기로 생각할 수 있습니다.\n",
    "\n",
    "Word2Vec은 각 단어의 주변 컨텍스트를 기반으로 벡터로 각 단어를 나타내는 방법을 학습합니다. 즉, 대상 단어 주변의 단어는 대상 단어에 대한 벡터 표현을 정의하는 데 도움이 됩니다. 이를 수행하기 위한 두 가지 접근 방식이 있습니다. CBOW(continuous bag of words)는 신경망을 사용하여 대상 단어(CBOW의 반대)가 주어진 단어를 예측합니다.\n",
    "\n",
    "Word2Vec의 마법은 의미론적으로 유사한 단어가 유사한 문맥에 나타나기 때문에 유사한 벡터(예: 숫자 표현)를 갖는다는 것입니다. 즉, 고차원 공간에서 \"queen\" 및 \"king\"과 같이 유사한 의미를 갖는 단어는 유사한 표현(즉, 벡터)을 가지므로 서로 더 가깝게 위치합니다.\n",
    "\n",
    "단어 임베딩을 처음부터 배우는 대신 ML 엔지니어는 모델 개발에서 Word2Vec에서 훈련된 사전 훈련된 단어 임베딩을 사용하여 사전에 수행된 일부 \"학습\"을 활용할 수 있습니다. 이러한 사전 훈련된 단어 임베딩의 출현은 모델 개발을 처음부터 완전히 시작할 필요가 없기 때문에 ML 엔지니어에게 도움이 되었습니다.\n",
    "\n",
    "성공에도 불구하고 Word2Vec에는 단점이 있습니다. 첫째, 전체 문서의 맥락에서 매우 작은 창 기반 모델에 의존합니다. 둘째, 하위 단어 정보를 고려하지 않는데, 이는 예를 들어 동일한 하위 단어에서 파생된 명사와 형용사가 어떻게 연관되는지 등을 효율적으로 학습할 수 없음을 의미합니다. 예를 들어 \"intelligent\"와 \"intelligence\"는 하위 단어 \"intellgen\"을 공유하고 결과적으로 관련되어 유사한 의미 정보를 공유합니다.\n",
    "\n",
    "셋째, Word2Vec은 OOV(Out of Vocabulary) 단어를 처리할 수 없으며 훈련에서 본 단어만 벡터화할 수 있습니다. 마지막으로 Word2Vec은 단어의 컨텍스트별 의미 속성을 명확하게 할 수 없습니다. 예를 들어 Word2Vec을 사용하면 \"bank\"라는 단어는 금융 설정(\"은행에 수표를 입금했습니다\")에 표시되는지 강 설정(\"나는 낚시를 마치고 강둑에 앉았다\")에 표시되는지에 관계없이 동일한 단어 벡터를 갖습니다.\n",
    "\n",
    "> 일반적으로 사전 훈련된 단어 임베딩에는 수백 차원(일반적으로 300차원)이 있습니다. 차원이 많을수록 단어 임베딩 알고리즘으로 더 미묘한 표현을 임베드할 수 있지만 계산 속도와 복잡성이 증가합니다. 더 나은 성능의 모델을 원한다면 선택한 단어 임베딩 알고리즘에 대해 더 적은 차원보다 더 많은 차원을 가진 단어 임베딩 매트릭스를 사용하는 것이 좋습니다. 더 빠르고 더 계산적으로 효율적인 모델을 원하는 경우 차원이 더 적은 행렬을 사용하는 것이 좋습니다. 다른 모든 것은 동일합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simplified-amber",
   "metadata": {},
   "source": [
    "### GloVe"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "336a042e",
   "metadata": {},
   "source": [
    "Global Vectors의 줄임말인 GloVe는 '장면에 등장할 다음 주요 단어 임베딩'이었습니다. Word2Vec 이후 1년 후인 2014년에 출시되었습니다. GloVe는 word2Vec의 첫 번째 주요 단점을 해결했습니다. GloVe는 Word2Vec과 같은 작은 창 기반 모델에 의존하는 대신 각 단어에 대한 단어 임베딩을 학습할 때 전체 말뭉치의 단어 통계를 고려했습니다.\n",
    "\n",
    "GloVe는 Word2Vec과 유사하게 작동하지만 단어의 벡터 표현을 학습하기 위해 다른 접근 방식을 사용합니다. 더 구체적으로, gloVe는 비지도 학습을 사용하여 전역 동시 발생 행렬을 생성하여 대상 단어가 나타나는 전체 말뭉치가 주어진 대상 단어의 의미 속성을 학습합니다.\n",
    "\n",
    "GloVe는 Word2Vec의 단점 중 하나를 해결했지만 여전히 하위 단어 정보를 고려하지 않습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cooked-syndrome",
   "metadata": {},
   "source": [
    "### fastText"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c33f524e",
   "metadata": {},
   "source": [
    "2016년에 Facebook은 최근 몇 년간 세 번째 주요 단어 임베딩 접근 방식인 fastText를 출시했습니다. fastText는 Word2Vec 및 GloVe와 다릅니다. 각 단어를 최소 단위로 간주하는 대신 fastText는 n-gram 문자를 최소 단위로 사용합니다. 즉, fastText는 하위 단어 정보를 사용하여 단어 임베딩을 생성합니다. 예를 들어 벡터 \"kingdom\"이라는 단어는 \"ki\",\"kin,\"\"ing,\"\"ngd,\"\"gdo\" 및 \"dom\"과 같은 n-gram 문자로 분해될 수 있습니다.\n",
    "\n",
    "다른 단어를 문맥으로 사용하여 단어에 대한 벡터 표현을 학습하는 대신 fastText는 다른 n-gram 문자를 문맥으로 사용하여 n-gram 문자에 대한 벡터 표현을 학습합니다. 단위를 Word2Vec 또는 GloVe보다 더 세분화된 수준으로 나누기 때문에 fastText는 더 다양하고 미묘한 단어 임베딩 세트를 달성합니다.\n",
    "\n",
    "가장 작은 단위로 단어 대신 n-gram 문자를 사용하는 것은 여러 가지 이유로 Word2Vec 및 GloVe보다 실질적으로 개선된 것입니다. 첫째, fastText는 동일한 단어 세트에 대해 Word2Vec 또는 GloVe가 할 수 있는 것보다 단어 세트의 다양한 n-gram 문자에서 더 많은 것을 배울 수 있기 때문에 훈련 데이터가 덜 필요합니다.\n",
    "\n",
    "Sencond, fastText는 하위 단어 정보를 사용하므로 fastText가 훈련한 새 단어 때문에 더 잘 일반화됩니다. 예를 들어 fastText가 \"fastest\"에 대해 훈련했지만 \"biggest\"에 대해 훈련하지 않은 경우 \"fastes\"의 \"est\"에서 \"biggest\"의 \"est\"의 의미를 추론할 수 있지만 Word2Vec 및 GloVe는 그렇지 않습니다. 셋째, fastText는 fastText가 임베딩한 OOV 단어에 대한 임베딩을 생성할 수 있습니다. 이것은 Word2Vec이나 GloVe가 지원하지 않는 하위 단어 정보의 사용과 관련이 있습니다.\n",
    "\n",
    "fastText의 유일한 주요 단점은 컨텍스트에 따라 각 단어에 대해 여러 벡터를 생성할 수 없다는 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocal-phase",
   "metadata": {},
   "source": [
    "### Context-aware Pretrained Word Embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9f048407",
   "metadata": {},
   "source": [
    "Word2Vec, GloVe 및 fastText로 훈련된 단어 임베딩은 아무리 훌륭하더라도 컨텍스트를 인식하지 못합니다. 단어의 다양한 문맥별 의미론적 속성을 캡처하지 않습니다. 예를 들어 \"bank\"라는 단어는 \"I deposited a check at the bank\"라는 문장에서 사용되든 \"I sat on the river bank after fishing.\"라는 문장에서 사용되든 상관없이 동일한 단어 벡터(따라서 동일한 의미 속성)를 가집니다.\n",
    "\"은행에 수표를 입금했습니다\", \"나는 낚시를 마치고 강둑에 앉았다.\"\n",
    "\n",
    "2018년부터 등장한 ELMo 및 BERT와 같은 Transformer 아키텍처를 기반으로 한 사전 훈련된 대규모 언어 모델은 상황을 인식하는 단어 표현을 도입하여 이를 변경했습니다. 컨텍스트 인식 단어 표현을 사용하면 금융 설정의 \"은행\"은 강 설정의 \"은행\"과 다른 단어 벡터를 갖습니다. 이것은 직관적으로 느껴져야 합니다. 다른 맥락에서 같은 단어는 다른 것을 의미하므로, 우리는 맥락에 따라 단어의 다른 의미를 나타내기 위해 다른 단어 벡터를 가져야 합니다. 곧 이에 대해 자세히 알아보십시오.\n",
    "\n",
    "이 섹션에서는 수년 동안 단어 임베딩의 발전을 다루었습니다. 다음 섹션에서는 순차 모델부터 시작하여 최근 몇 년간 모델링 접근 방식의 발전에 대해 살펴보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demanding-reverse",
   "metadata": {},
   "source": [
    "## Sequential Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9202accf",
   "metadata": {},
   "source": [
    "2016년부터 순차적 모델이 NLP 분야에서 두각을 나타내기 시작하여 기계 번역, 텍스트 요약, 대화형 봇 및 이미지 캡션과 같은 작업에서 성공을 거두었습니다. 순차 모델은 또한 Google의 새로운 기계 번역 기반 Google 번역(https://oreil.ly/M0Vjb)에 대한 New York Times 기사를 통해 주류의 주목을 받았습니다. \"The Great AI Awakening\"이라고 불리는 이 기사는 복잡한 언어 작업을 해결하는 NLP 모델의 힘을 처음으로 세계 무대에 선보였습니다.\n",
    "\n",
    "순차 모델은 텍스트, 오디오 및 시계열 데이터와 같은 데이터 시퀀스를 입력하거나 출력하는 기계 학습 모델입니다. 순차 모델은 단순한 단일 접근 방식이 아니라 모델링 접근 방식의 한 종류이며 여기에는 RNN, LSTM 및 GRU가 포함되며 이 모든 내용은 이 책의 앞부분에서 자세히 다루었습니다. 이러한 모든 순차 모델은 일련의 데이터를 받아 단일 결과를 출력하거나(예: 영화 리뷰를 긍정적 또는 부정적 감정으로 분류), 단일 입력을 받아 데이터 시퀀스를 출력하거나(예: 이미지를 가져오고 이미지를 설명하는 캡션 반환), 또는 데이터 시퀀스(예: 텍스트 또는 오디오)를 다른 시퀀스(seq2seq 모델링이라고 함)로 켭니다. 예를 들어 신경망 기계 번역 모델은 한 언어로 된 텍스트를 입력 시퀀스(예: 영어)로 사용합니다. 즉, 모델은 입력 시퀀스를 받아 출력 시퀀스를 출력합니다.\n",
    "\n",
    "> 요약하자면 순차 모델은 여러 유형의 시나리오를 처리합니다. (a) 감정 분석과 같은 시나리오의 경우 단일 출력에 대한 순차 입력; (b) 이미지 캡션을 위한 단일 입력에서 순차 출력으로; 및 (c) 기계 번역을 위한 순차 입력에서 순차 출력으로.\n",
    "\n",
    "순차 모델은 일반적으로 인코드와 디코더로 구성됩니다. 인코더는 항목별로 입력 시퀀스를 받아 표현을 생성합니다. 이것을 기계가 처리할 수 있는 숫자 벡터로 텍스트(예: 문장)를 변환하는 것으로 생각하십시오. 기계 번역 작업에서 인코더는 표현을 단어별로 \"인코딩\"하여 표현을 형성합니다.\n",
    "\n",
    "인코더가 전체 입력 시퀀스를 처리하면 표현을 디코더로 전달하고 디코더는 이를 항목별로 출력 시퀀스로 풀어냅니다. 예를 들어, 기계 번역 작업에서 디코더는 표현을 단어별로 \"디코더\"하여 출력 세트를 형성합니다.\n",
    "\n",
    "지난 몇 년 동안 순차 모델은 이전 모델의 결함을 해결하면서 점점 더 좋아졌습니다. 순차 데이터의 특성을 좀 더 살펴보고 최신 순차 모델 중 가장 초기인 RNN 구성부터 시작하겠습니다.\n",
    "\n",
    "> 이 책에서는 NLP 기반 순차 모델링 응용 프로그램에 중점을 두지만 순차 모델링에는 NLP 이외의 응용 프로그램도 있다는 것을 아는 것이 중요합니다. Whitin NLP, 순차 모델링은 텍스트(예: 기계 번역, 텍스트 요약, 질문 답변 등), 오디오(예: 챗봇) 및 음성(예: 음성 인식)과 관련이 있습니다. NLP 외부에서 순차 모델링은 이미지(예: 이미지 캡션), 비디오(예: 비디오 캡션), 시계열 데이터에 대한 이상 탐지, 센서 데이터, 주식 시장 데이터, 게놈 데이터 및 날씨 데이터와 관련된 시계열 예측과 관련이 있습니다. 순차 모델링은 NLP뿐만 아니라 기업에서 가장 관련성이 높고 번창하는 기계 학습 영역 중 하나입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minor-holly",
   "metadata": {},
   "source": [
    "### Sequential Data and the Importance of Sequential Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "53285264",
   "metadata": {},
   "source": [
    "RNN에 대해 알아보기 전에 순차 데이터의 특성과 이를 사용하기 위해 특별한 클래스의 모델(즉, 순차 모델)이 필요한 이유를 살펴보겠습니다. 순차적 데이터는 순차적으로 상호 의존적/관련된 일련의 항목을 포함합니다. 예를 들어, 한 문장의 단어는 다른 단어와 순차적인 방식으로 관련되어 있으므로 상호 의존적입니다. 문장의 단어는 서로 순차적으로 독립적이지 않습니다.\n",
    "\n",
    "이것은 다른 순차 데이터도 마찬가지입니다. 예를 들어, 구어 문장의 음소(음소는 \"고양이\"의 \"c\"와 같은 가장 작은 음성 발화임)도 서로 순차적으로 의존합니다. 우리가 말을 할 때 내는 소리는 각 발화의 앞과 뒤의 소리와 관련이 있습니다. 오디오를 모델링하려면 데이터의 순차적 연결성을 캡처하는 방법이 필요합니다.\n",
    "\n",
    "순차적 데이터의 세 번째 예는 주식 시장 가격입니다. 주가의 각 1초 틱은 이전의 일련의 틱과 이후의 일련의 틱과 관련이 있습니다. 데이터 데이터에는 각 틱을 나머지와 연결하는 패턴이 있습니다. 주가를 잘 예측하기 위해서는 주식 시장의 가격 데이터에서 학습하는 기계 학습 모델이 주가의 순차적 특성을 잘 표현하고 처리할 수 있어야 합니다. 이는 의료, 산업용 로봇 및 기타 여러 분야의 센서 유형 시계열 데이터에 해당됩니다.\n",
    "\n",
    "기존의 피드포워드 신경망은 각 입력/관측을 이전 항목과 이후 항목과 독립적으로 처리합니다. 예를 들어, 이미지를 \"고양이\" 또는 \"개\"로 분류하는 컴퓨터 비전 모델은 현재 이미지를 성공적으로 분류하기 위해 이전 또는 후속 이미지를 고려할 필요가 없습니다. 모델은 현재 이미지에만 초점을 맞추면 됩니다.\n",
    "\n",
    "물론 단일 입력에 대한 이러한 단일 초점은 순차 데이터 문제에 최적이 아닙니다. 프랑스어 문장을 영어로 번역하는 모델을 구축해야 한다면 프랑스어 문장의 각 단어를 영어 단어로 한 단어씩 번역하는 것이 최적이 아닐 것입니다. 이것은 프랑스어를 영어로 문자 그대로 번역하는 것이지만 영어로 된 출력 문장은 프랑스어 문법 규칙이 영어 문법 규칙과 다르기 때문에 문법적으로 그다지 의미가 없을 것입니다.\n",
    "\n",
    "보다 최적의 접근 방식은 모델이 전체 입력(프랑스어) 문장을 출력(영어) 문장으로 번역하기 전에 먼저 전체 입력(프랑스어) 문장의 표현을 생성하는 것입니다. 모델은 이전 단어를 고려하면서 각 단어를 단어별로 처리하여 이를 수행해야 합니다. 모델이 순차 패턴을 모두 무시하는 경우보다 모델이 프랑스어 문장을 더 잘 표현하는 데 도움이 되는 순차 패턴을 언어가 나타내기 때문에 이는 중요합니다.\n",
    "\n",
    "모델은 입력 문장을 처리할 때 프랑스어의 순차적 패턴을 고려하여 문장을 영어로 보다 정확하게 번역할 수 있습니다. 이것이 순차 모델링의 핵심입니다. 텍스트, 오디오 및 시계열 데이터와 같은 데이터의 순차적 패턴을 고려하여 순차적 모델은 기존의 피드포워드 신경망보다 작업에서 더 나은 성능을 생성합니다.\n",
    "\n",
    "이러한 맥락에서 최근 몇 년간 성공적인 순차 모델 중 첫 번째인 RNN을 자세히 살펴보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distinguished-favorite",
   "metadata": {},
   "source": [
    "## RNNs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0f2e168a",
   "metadata": {},
   "source": [
    "순차 모델은 한 번에 한 단계씩 데이터의 시간적 특성에 대해 학습합니다. 이를 증명하기 위해 예를 들어 보겠습니다. 텍스트를 처리하는 순차 모델은 각 단어를 한 번에 한 단어씩 \"읽습니다\". 모델이 단어를 읽는 매 순간이 시간 단계입니다. 모델이 전체 문장을 처리할 때 시간 단계 0에서 시간 단계 'x'로 이동합니다. 여기서 'x'는 문장의 길이입니다. 시간 단계가 증가할 때마다 모델은 현재 단어를 고려하면서 그 앞에 오는 일련의 단어를 고려합니다.\n",
    "\n",
    "> 다음은 RNN을 생각하는 간단하고 직관적인 방법입니다. RNN은 내부에 루프가 있는 네트워크로, 과거 정보를 \"메모리\"로 유지하여 다음 입력을 처리하는 데 사용할 수 있습니다.\n",
    "\n",
    "모델의 기억력이 좋을수록 모델은 문장을 다른 언어로 번역하거나 질문에 답하는 것과 같은 작업을 더 잘 수행할 수 있습니다.\n",
    "\n",
    "RNN은 현재 데이터를 처리할 때 이전 순차 데이터의 메모리를 저장하고 사용할 수 있는 기계 학습 모델 제품군입니다. 예를 들어, RNN은 이전 단어의 메모리를 가지고 있으며 이 메모리를 사용하여 문장의 현재 단어를 처리합니다. RNN의 주요 과제는 긴 시간 프레임에 걸쳐 있는 순차 데이터의 큰 메모리를 갖는 것입니다. 예를 들어, RNN이 몇 문장 전 문장의 단어를 기억하는 것보다 처리한 가장 최근 몇 단어를 기억하는 것이 더 쉽습니다.\n",
    "\n",
    "단기 메모리뿐만 아니라 장기 메모리도 모델링하고 더 나은 성능을 제공하는 게이트(예: LSTM 및 GRU)가 있는 RNN으로 전환하기 전에 이 섹션에서 단기 메모리가 우수한 바닐라 RNN부터 시작하겠습니다. 질문 답변과 같은 보다 복잡한 작업을 해결하는 데 필요한 순차적 데이터의 장기 종속성을 캡처합니다.\n",
    "\n",
    "> RNN 이전에는 CNN이 NLP 문제를 해결하는 데 사용되었습니다. CNN은 컴퓨터 비전 작업에 대한 성능으로 기계 학습에서 유명해졌지만 자연어 작업과도 관련이 있습니다. CNN에서 신경망은 고정 길이 창을 사용하여 데이터를 나타냅니다. 예를 들어 텍스트 기반 문제에서 신경망은 기계 번역과 같은 작업을 수행하기 위해 작고 제한된 단어 컨텍스트를 사용합니다. CNN 기반 언어 모델은 매우 빠르지만 단어의 컨텍스트가 거의 없습니다. 그들은 RNN의 단기 기억보다 훨씬 적은 컨텍스트를 가지고 있습니다. 이는 CNN의 성능을 제한하며, RNN이 사용 가능해지면 연구원들이 RNN으로 전환한 이유입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experienced-remains",
   "metadata": {},
   "source": [
    "### Vanilla RNNs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "06e2c77c",
   "metadata": {},
   "source": [
    "기존의 피드포워드 네트워크와 달리 순환 신경망은 시간적 차원을 가집니다. 즉, RNN은 시간을 고려하지만 기존의 피드포워드 네트워크는 그렇지 않습니다. 기존의 피드포워드 네트워크는 정보를 한 방향으로 피드(따라서 피드포워드)하지만 RNN은 데이터를 전달한 다음 루프를 통해 데이터를 다시 순환시킵니다.\n",
    "\n",
    "이 \"반복적인\" 순환을 통해 RNN은 시간 감각을 갖게 되어 네트워크가 이전 입력의 일부 컨텍스트를 재학습하면서 현재 입력을 처리할 수 있습니다. RNN이 시간 단계 t에서 이벤트를 처리할 때 최근 과거도 고려합니다(예: 시간 단계 t-1, t-2 등에서 발생한 일). 즉, RNN은 시간이 지남에 따라 가중치를 공유합니다. 기존의 피드포워드 네트워크는 잊어버린 기억이 있는 반면 RNN은 최근 이벤트에 대한 더 나은 기억을 가지고 있다고 생각할 수 있습니다. RNN의 정보는 지속되지만 기존의 피드포워드 네트워크에서는 그렇지 않습니다.\n",
    "\n",
    "순차 모델에서 봇 인코더와 디코더는 RNN이 될 수 있습니다. 인코더와 디코더 RNN 모두 각 타임스탬프에서 두 개의 입력을 받습니다. 예를 들어 기계 번역의 경우 두 입력은 (a) 단어와 (b) 숨겨진 상태입니다. 이 은닉 상태 벡터는 순환 네트워크가 이전 시간 단계에서 보존한 순차적 메모리입니다.\n",
    "\n",
    "> 각 단어는 이전 장에서 검토한 단어 임베딩으로 표현됩니다.\n",
    "\n",
    "각 타임스탬프에서 인코더 RNN은 입력(즉, 단어 벡터)과 숨겨진 상태(또한 벡터)를 처리합니다. 다음 타임스탬프에서 RNN은 이전 타임스탬프의 다음 입력(즉, 다음 단어 벡터)과 (출력) 은닉 상태를 처리하고 또 다른 출력 벡터와 새로운 출력 은닉 상태를 생성합니다. 이는 인코더 RNN이 전체 입력 시퀀스 처리를 완료할 때까지 계속됩니다.\n",
    "\n",
    "인코더가 완료되면 생성한 마지막 출력 숨겨진 상태만 디코더에 전달합니다. 이 마지막 출력 숨겨진 상태는 앞에서 언급한 \"표현\"입니다. 디코더가 출력 시퀀스로 처리할 준비가 된 기계 처리 가능한 형식으로 표현된 입력 시퀀스로 생각할 수 있습니다.\n",
    "\n",
    "디코더 RNN이 인코더로부터 \"표현\"을 수신하면 이를 단어별로 출력 시퀀스로 풀어냅니다. 즉, 디코더 RNN은 숨겨진 상태를 단어별로 \"번역\"합니다. 이것은 인코더가 수행한 작업을 해석하는 것으로 생각할 수 있지만 그 반대입니다. 이것은 RNN이 어떻게 작동하는지에 대한 매우 간단한 설명이지만 자세한 내용은 6장을 확인하십시오.\n",
    "\n",
    "> RNN은 텍스트뿐만 아니라 모든 종류의 순차 데이터를 처리할 수 있습니다. 예를 들어 여기에는 시계열 데이터가 포함됩니다.\n",
    "\n",
    "RNN은 내부 상태를 사용하여 시퀀스를 처리하기 때문에 텍스트와 같은 순차적 데이터를 모델링하는 데 탁월한 선택입니다. 즉, RNN은 시퀀스를 통해 항목별로 작업하므로 각 항목을 처리하기 위해 내부 상태/메모리에 의존합니다. 시퀀스의 항목(예: 문장의 단어)이 서로 독립적이지 않기 때문에 이것은 매우 중요합니다. 그들은 관련이 있습니다. 입력이 서로 어떻게 관련되어 있는지에 대한 내부 상태/메모리를 갖는 것은 데이터를 효과적으로 모델링하는 데 중요합니다.\n",
    "\n",
    "직관적이어야 합니다. 한 문장을 한 언어에서 다른 언어로 번역하려면 먼저 입력 문장을 적절하게 표현해야 합니다. 입력 문장의 각 단어는 이전 단어에 종속/관련됩니다. 숨겨진 상태를 관리하는 방식으로 RNN은 내부 상태/메모리(처리된 이전 단어 기반)를 사용하여 각 후속 단어를 처리합니다.\n",
    "\n",
    "그러나 RNN에는 큰 결함도 있습니다. RNN은 매우 긴 시퀀스를 잘 처리할 수 없습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "permanent-divorce",
   "metadata": {},
   "source": [
    "### LSTMs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e69e72bd",
   "metadata": {},
   "source": [
    "Vanilla RNN에는 메모리가 있지만 이 메모리는 대부분 단기적입니다. 바닐라 RNN은 데이터의 장기 종속성을 캡처하고 저장하는 데 정말 어려움을 겪고 있습니다. 따라서 질문 답변과 같은 보다 복잡한 NLP 작업을 해결하는 데 성능이 제한됩니다.\n",
    "\n",
    "LSTM 네트워크는 또한 순환 신경망 아키텍처를 사용하지만 바닐라 RNN이 긴 시퀀스를 매우 잘 처리할 수 없는 문제를 해결하는 데 도움이 됩니다. LSTM은 더 긴 시퀀스에 걸쳐 데이터 메모리를 보유할 수 있습니다. 그들은 신경망 아키텍처의 일부로 게이트로 알려진 메커니즘을 사용하여 훨씬 더 오랫동안 순차 데이터의 컨텍스트를 염두에 둘 수 있습니다(게이트에 대해서는 곧 자세히 설명).\n",
    "\n",
    "NLP 작업을 해결하려면 이전의 여러 단계에서 항목을 기억해야 할 수 있기 때문에 장기 기억을 갖는 것이 매우 중요합니다. George Washington에 관한 Wikipedia 기사(https://oreil.ly/V7Sdh)에서 가져온 다음 구절을 생각해 보십시오.\n",
    "\n",
    "> George Washington은 1789년부터 1797년까지 미국의 초대 대통령을 역임한 미국의 정치 지도자, 군 장군, 정치가이자 건국의 아버지였습니다. 이전에 그는 미국의 독립 전쟁에서 패트리어트 군대를 승리로 이끌었습니다. 그는 미국 헌법과 연방 정부를 수립한 1787년 제헌 회의를 주재했습니다. 워싱턴은 새 국가의 형성기에 다양한 리더십을 발휘하여 \"조국의 아버지\"로 불려 왔습니다.\n",
    "\n",
    "\"미국의 초대 대통령은 누구입니까?\" 바닐라 RNN을 사용하여 훈련된 NLP 모델은 질문(\"George Washington\")에 올바르게 대답할 수 있습니다. 낮습니다(20단계 미만).\n",
    "\n",
    "그러나 일반 RNN을 사용하여 훈련된 NLP 모델은 \"1787년 제헌 회의에서 대통령은 누구였습니까?\"라는 질문에 훨씬 더 어려움을 겪을 것입니다. NLP 모델이 어떤 형태의 장기 기억을 보유할 수 없다면 \"조지 워싱턴\"이라는 언급과 \"1787년 헌법 제정 회의\"라는 언급 사이의 시간 간격이 많기 때문입니다.\n",
    "\n",
    "> RNN은 관련 정보와 정보가 필요한 지점 사이에 약간의 간격이 있을 때 완벽하게 작동하지만 간격이 커지고 RNN의 단기 기억이 멀리 떨어진 정보와 질문을 연결할 수 없으면 어려움을 겪기 시작합니다. 과거. 즉, RNN은 많은 시간 단계에 걸쳐 정보를 보존할 수 없기 때문에 단기 메모리가 필요한 작업만 처리할 수 있습니다. NLP 모델은 조지 워싱턴에 대한 텍스트의 전체 단락을 처리할 때 \"그\"라는 대명사를 \"조지 워싱턴\"으로 성공적으로 명확하게 할 수 있다면 \"1787년 헌법 제정 회의에서 주재한 사람\"이라는 질문에 더 큰 성공을 거둘 것입니다. 보다 현대적인 NLP 모델은 어텐션 메커니즘을 사용하여 이를 매우 잘 수행합니다. 188페이지의 \"Attention Mechanisms\"에서 이에 대해 자세히 설명합니다.\n",
    "\n",
    "LSTM은 LSTM의 메모리로 들어오고 나가는 정보의 양을 제어하는 ​​3개의 세심하게 조절된 일련의 게이트를 사용하여 장기 메모리를 보유합니다. 이러한 게이트를 통해 LSTM은 임의의 시간 간격 동안 값을 기억할 수 있습니다. 세 개의 게이트는 네트워크가 항목이 얼마나 관련성이 있는지에 따라 메모리에 항목을 추가하거나 제거할 수 있는 메커니즘으로 생각할 수 있습니다.\n",
    "\n",
    "즉, LSTM 네트워크는 (a) 현재 입력에서 추가하려는 정보, (b) 더 이상 관련이 없다고 간주하고 잊고 싶은 정보, (c)에 따라 각 시간 단계에서 메모리 벡터를 업데이트합니다. 유지하려는 정보. 게이트는 순차 데이터의 어떤 항목이 LSTM 메모리에 저장하는 데 중요하고 어떤 항목이 중요하지 않은지 학습하는 메커니즘입니다. 게이트는 그 자체로 신경망이며 각각의 특수한 역할을 가장 잘 수행하는 방법을 학습합니다. 이 세 가지 유형의 게이트는 다음과 같습니다.\n",
    "\n",
    "`입력 게이트`  \n",
    "    이는 메모리를 수정하는 데 사용해야 하는 현재 입력의 정보를 결정합니다.\n",
    "\n",
    "`게이트를 잊어라`  \n",
    "    이 유형의 게이트는 정보가 더 이상 관련이 없기 때문에 메모리에서 잊어버릴 정보를 결정합니다.\n",
    "\n",
    "`출력 게이트`  \n",
    "    이것은 현재 입력과 네트워크가 잊기로 선택한 것을 메모리에 보관할(그리고 다음 단계로 전달할) 정보를 결정합니다.\n",
    "\n",
    "이러한 게이트를 사용하여 LSTM을 사용하면 신경망이 한 번에 다른 시간 척도에서 작동할 수 있으므로 RNN보다 훨씬 더 긴 종속성을 캡처할 수 있습니다.\n",
    "\n",
    "> TL;DR은 LSTM이 RNN의 개선된 버전이라는 것입니다. LSTM은 RNN에 비해 장기 기억을 가지고 있습니다. LSTM은 순차 데이터에서 가장 중요한 정보와 중요하지 않은 정보를 학습하는 게이트라는 메커니즘을 사용하여 이를 달성합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mechanical-inventory",
   "metadata": {},
   "source": [
    "### GRUs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "65802561",
   "metadata": {},
   "source": [
    "GRU(Gated Recurrent Unit)는 게이트가 있는 RNN의 또 다른 형태입니다. LSTM과 유사하지만 3개가 아닌 2개의 게이트를 사용하는 더 간단한 구조를 가지고 있습니다. 이 두 종류의 게이트는 다음과 같습니다.\n",
    "\n",
    "`업데이트 게이트`  \n",
    "    업데이트 게이트는 메모리가 현재 입력의 새로운 정보로 업데이트되어야 하는지 여부를 결정합니다.\n",
    "\n",
    "`리셋 게이트`  \n",
    "    이것은 얼마나 많은 새 메모리가 중요한지(그리고 유지되고 전달되어야 하는지) 그렇지 않은지(따라서 재설정되는지)를 결정합니다.\n",
    "\n",
    "즉, 업데이트 게이트는 메모리로 유입되는 정보를 제어하고 재설정 게이트는 메모리 밖으로 유출되는 정보를 제어합니다. GRU의 업데이트 게이트는 LSTM의 입력 게이트와 망각 게이트의 결합과 유사하지만 GRU의 재설정 게이트는 LSTM의 출력 게이트와 유사합니다.\n",
    "\n",
    "GRU의 성능은 LSTM의 성능과 유사하지만(일반적으로 그다지 좋지는 않음) 구조가 단순하기 때문에 GRU가 계산적으로 더 효율적입니다. GRU는 훈련 중에 업데이트할 가중치와 매개변수가 적기 때문에 훈련 데이터가 제한되어 있을 때 더 빠르게 훈련하고 LSTM보다 더 나은 선택이기도 합니다.\n",
    "\n",
    "요약하면 RNN은 특히 2015년부터 NLP 분야를 발전시키는 데 도움이 된 순차 모델 제품군입니다. Vanilla RNN은 간단한 단기 메모리를 가지고 있지만 게이트 변형(LSTM 및 GRU)은 더 긴 단기 메모리 및 캡처를 가지고 있습니다. 순차 데이터의 장기 종속성이 더 좋습니다. LSTM과 GRU는 오늘날 NLP 분야에서 가장 성능이 좋은 RNN입니다. 둘 다 비슷한 성능을 가지고 있지만 GRU는 LSTM보다 간단하고(GRU는 LSTM에서 3개 대신 2개의 게이트를 사용함) 학습 속도가 더 빠릅니다.\n",
    "\n",
    "이러한 RNN이 2016년 말에 성공적이었던 만큼 장기 종속성을 제대로 처리하지 못했으며, 이는 NLP 주의 메커니즘의 다음 돌파구에서 해결하고자 하는 문제입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exclusive-manner",
   "metadata": {},
   "source": [
    "## Attention Mechanisms"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bab1d011",
   "metadata": {},
   "source": [
    "LSTM 및 GRU는 RNN보다 장기 기억을 가지고 있지만 여전히 NLP 작업 수행을 방해하는 단점이 있습니다. 이것은 기계 번역과 같은 시퀀스 간 NLP 작업에서 가장 분명합니다. LSTM과 GRU 모두 시간 단계에서 텍스트로 전달되는 숨겨진 상태(\"메모리\")가 있습니다. 기계 번역에서는 입력 문장이 먼저 인코딩되고 최종 숨겨진 상태(\"표현\")가 디코더에 전달되어 문장을 출력 언어로 디코딩/번역합니다.\n",
    "\n",
    "디코더는 이 최종 숨겨진 상태만 해결해야 합니다. 인코더의 중간 숨겨진 상태에 액세스할 수 없습니다. 이는 디코더가 변환을 개선하는 데 사용할 수 있지만 액세스할 수 없는 정보가 테이블에 남아 있음을 의미합니다(인코더의 중간 숨겨진 상태 형식). 이러한 제한 때문에 예를 들어 LSTM은 기계 번역에서 20단어 이상의 정보를 보존할 수 없습니다. 이를 해결하기 위해 양방향 버전의 LSTM(Bi-LSTM이라고 함)이 발명되었지만 Bi-LSTM도 40단어 이상의 정보를 보존할 수 없었습니다.\n",
    "\n",
    "전체 문장의 의미를 하나의 벡터(인코더의 최종 숨겨진 상태)로 압축하고 이를 디코더에 전달하여 입력 문장을 번역하는 것은 차선책이라는 것이 분명해야 합니다. 디코더가 번역 작업을 할 때마다 모든 시간 단계에서 (인코더의 중간 은닉 상태를 통해) 입력 문장의 관련 위치에 초점을 맞추는 것이 더 좋을 것입니다.\n",
    "\n",
    "즉, 디코더는 번역을 수행하기 위해 하나의 벡터(인코더의 최종 숨겨진 상태)만 사용하는 대신 모든 시간 단계에서 인코더의 관련 숨겨진 상태에 주의를 집중해야 합니다. 직관적으로 이것은 디코더가 입력 문장의 번역을 통해 작업할 때 더 많은 관련 정보를 갖게 되므로 번역 품질을 향상시켜야 합니다.\n",
    "\n",
    "LSTM이 주의를 집중할 수 있도록 하는 메커니즘을 주의 메커니즘이라고 합니다. 그들은 2017년부터 최근 몇 년 동안 NLP의 주요 혁신을 이끌어내는 데 도움을 주었습니다. LSTM의 주의 메커니즘을 통해 디코더는 최종 숨겨진 상태뿐만 아니라 인코더의 모든 숨겨진 상태에 액세스할 수 있습니다. 이 외에도 디코더가 특정 숨겨진 상태에 주의를 집중하고 입력 문장을 출력 문장으로 변환할 때 다른 상태는 무시하도록 돕습니다.\n",
    "\n",
    "전체 소스 입력에 액세스할 수 있으므로 어텐션 메커니즘이 있는 LSTM은 일반 LSTM 및 GRU보다 더 긴 입력 시퀀스를 처리할 수 있습니다(그리고 확실히 RNN보다 훨씬 낫습니다). 간단히 말해서, 어텐션 메커니즘이 있는 LSTM은 더 우수하고 더 많은 강제 기억을 가질 수 있기 때문에 덜 잊습니다.\n",
    "\n",
    "> 어텐션이 있는 LSTM에서 인코더는 어텐션 없는 LSTM의 경우인 마지막 은닉 상태 대신 모든 은닉 상태를 디코더로 전달합니다.\n",
    "\n",
    "이것은 직관적으로 느껴져야 합니다. 신경망의 주의는 인간의 인지 주의를 모방합니다. 문장을 읽든 차를 운전하든 인간으로서 우리는 항상 우리 주변의 모든 것에 동일한 주의를 기울이지 않습니다. 그렇게 하는 것은 정신적으로 지칠 뿐만 아니라 우리에게 불가능할 것입니다. 우리는 그 선천적 부담을 한 번에 지탱할 수 없었습니다.\n",
    "\n",
    "대신 수행 중인 작업에서 가장 중요한 항목에 주의를 집중합니다. 문장을 읽을 때 관사, 전치사, 여러 문장의 단어와 같은 필러 단어보다 주인공의 이름, 위치, 그들이 수행하는 활동과 같은 일부 단어에 더 주의를 기울입니다. 전에. 필러 단어는 관련성이 낮으므로 주의를 기울일 가치가 없습니다.\n",
    "\n",
    "이는 자동차 운전과 같은 복잡한 작업에도 해당됩니다. 우리는 정지 표지판, 신호등, 횡단보도, 기타 주변 차량, 보행자, 자전거 타는 사람 및 부딪히지 않으려는 기타 물체를 포함하여 우리 앞에 있는 도로에 주의를 집중합니다. 우리는 특히 우리 앞에 교통량이 많을 때 주변 시야나 지평선의 풍경에서 일어나는 일에 훨씬 덜 집중합니다.\n",
    "\n",
    "거의 같은 방식으로 기계 학습의 주의 메커니즘은 신경망이 당면한 작업에 가장 중요한 것에 주의를 집중하고 관련성이 덜한 다른 모든 것을 무시하도록 도와줍니다. 이를 통해 집중할 수 있는 사람(약물, 알코올 또는 기타 산만함과 같은 인지 장애가 없음)이 작업을 더 잘 수행할 수 있는 것과 거의 같은 방식으로 신경망이 작업에서 더 나은 성능을 가질 수 있습니다.\n",
    "\n",
    "기계 번역 개선에 초기에 성공한 후 어텐션 메커니즘이 크게 유행했습니다. 셀프 어텐션, 글로벌/소프트 어텐션, 로컬/하드 어텐션을 포함하여 수많은 어텐션 메커니즘이 시장에 등장했습니다. 또한 어텐션 메커니즘은 NLP 애플리케이션 이상에 사용되었습니다. 그들은 컴퓨터 비전에서도 인기를 얻었습니다. 주의 메커니즘의 이러한 변형과 ​​작동 방식에 대한 자세한 내용은 7장을 참조하십시오.\n",
    "\n",
    "> 어텐션 메커니즘은 또한 모델을 보다 쉽게 ​​해석하고 설명할 수 있도록 도와줍니다. 이를 통해 예를 들어 단어를 번역하거나 이미지에 대한 캡션을 생성할 때 모델이 초점을 맞추는 것이 무엇인지 알 수 있습니다.\n",
    "\n",
    "이것이 대중화됨에 따라 연구자들은 주의 메커니즘에 점점 더 의존하고 RNN의 반복적인 신경 구조에 덜 의존하는 신경망 구조를 탐구하기 시작했습니다. 그 결과 NLP의 차세대 혁신인 Transformer 아키텍처가 탄생했습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sensitive-deadline",
   "metadata": {},
   "source": [
    "## Transformers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8c8111c4",
   "metadata": {},
   "source": [
    "어텐션 메커니즘이 있는 LSTM은 일반 LSTM 및 GRU에 비해 ​​크게 개선되었지만 몇 가지 단점이 있었습니다. 특히 주의 메커니즘이 있는 LSTM은 훈련에 계산 집약적이며 병렬화하기 어렵습니다.\n",
    "\n",
    "2017년 어텐션 메커니즘이 있는 LSTM이 인기를 얻은 직후, 연구자들은 훨씬 더 나은 아키텍처를 설계했습니다. 이 아키텍처는 학습 속도가 더 빠르고 순환 네트워크를 완전히 제거하고 어텐션 메커니즘에만 의존합니다. 이 새로운 아키텍처는 Transformer 아키텍처(줄여서 Transformer)로 알려져 있습니다.\n",
    "\n",
    "반복적인 네트워크 기반 인코더-디코더를 사용하는 대신 트랜스포머는 주의를 기울여 피드포워드 인코더-디코더를 사용합니다.\n",
    "\n",
    "Transformer는 ULMFiT, ELMo, BERT, GPT-2 및 GPT-3과 같은 사전 훈련된 대규모 언어 모델의 출현을 예고하는 NLP의 ImageNet 순간의 촉매제였습니다. 트랜스포머는 매우 메모리 집약적이지만 매우 잘 병렬화됩니다. 병렬화가 주어지면 많은 GPU에서 매우 빠른 속도로 많은 데이터로 트랜스포머를 교육할 수 있습니다.\n",
    "\n",
    "왜 Transformer가 Attention이 있는 LSTM에 비해 그토록 중요한 돌파구였는지 설명하는 데 도움이 되므로 병렬화를 좀 더 파헤쳐 보겠습니다. 모든 RNN과 마찬가지로 LSTM은 LSTM이 두 번째 단어, 세 번째 단어 등을 처리하기 전에 처리해야 합니다. 변환기에는 이 요구 사항이 없습니다. 데이터를 처음부터 끝까지 순서대로 처리할 필요가 없습니다.\n",
    "\n",
    "반복 처리를 제거하고 어텐션 메커니즘에만 의존함으로써 트랜스포머는 이전의 기존 순차 모델처럼 순차적으로가 아니라 전체 데이터 시퀀스를 한 번에 디코더에 전달합니다. 한 번에 네트워크를 통해 데이터 블록(예: 여러 문장)을 전달하는 이 혁신은 게임 체인저였습니다.\n",
    "\n",
    "기존의 순차 모델과 비교할 때 Transformer는 주어진 시간에 더 많은 데이터에서 학습하고 결과적으로 훨씬 더 많은 병렬화를 수행하여 교육 시간을 줄입니다. 더 큰 병렬화와 더 빠른 훈련 시간으로 연구자들은 기존 순차 모델이 훈련할 수 있는 데이터 세트보다 훨씬 더 큰 대규모 데이터 세트에서 훈련할 수 있었습니다.\n",
    "\n",
    "이를 통해 Google, Facebook 및 기타 회사의 연구팀은 LSTM으로 가능했던 것보다 훨씬 더 큰 매우 큰 데이터 세트에서 훈련할 수 있었습니다. 훈련 중 병렬화의 이러한 돌파구는 매우 크고 사전 훈련된 언어 모델의 출현으로 이어졌습니다.\n",
    "\n",
    "이를 염두에 두고 Transformer 아키텍처를 더 자세히 살펴보겠습니다. 트랜스포머는 작업을 수행하기 위해 반복되는 순차 처리 없이 어텐션 메커니즘에만 의존합니다. Attention 메커니즘의 발명으로 Transformer가 가능해졌습니다.\n",
    "\n",
    "LSTM과 마찬가지로 Transformer는 인코더-디코더 아키텍처에 의존합니다. 특히 인코딩 구성 요소는 입력을 처리하는 인코더 스택이고 디코딩 구성 요소는 인코더가 전달한 인코딩을 처리하는 디코더 스택입니다. 인코더의 수는 디코더의 수와 같습니다. 또한 모든 인코더는 구조가 동일하고 모든 디코더는 구조가 동일합니다.\n",
    "\n",
    "먼저 인코더 스택을 살펴보겠습니다. 각 인코더에는 셀프 어텐션 메커니즘과 피드 포워드 신경망이라는 두 가지 구성 요소(또는 하위 계층)가 있습니다. self-atteion 메커니즘은 이전 인코더에서 입력 인코딩을 가져오고 인코딩의 관련성을 서로 비교하여 출력 인코딩 세트를 생성합니다. 출력 인코딩은 다음 인코더 계층과 디코더 계층으로 전달하기 전에 인코딩을 개별적으로 처리하는 피드포워드 신경망에 공급됩니다.\n",
    "\n",
    "셀프 어텐션 메커니즘은 인코더가 처리 중인 단어에 대한 입력 문장의 단어 관련성을 평가하는 데 도움이 됩니다. 예를 들어, \"Washington was the first President of the United States, While Adams was the first Vice President\"라는 문장에서 self-attention 메커니즘은 단어를 처리할 때 \"first\" 및 \"President\"라는 단어에 더 큰 관련성을 할당합니다. \"Washington\"보다 다른 단어, 특히 \"Adams\"와 관련된 단어에 할당합니다. 이는 인코더가 당면한 단어를 처리할 때 더 관련성 높은 정보에 집중하는 데 도움이 되며 주의 메커니즘의 아름다움을 다시 한 번 강조합니다.\n",
    "\n",
    "> 첫 번째 인코더는 인코딩이 아닌 입력 문장의 단어 임베딩과 토큰의 위치 정보를 입력으로 받습니다. 다른 모든 인코더는 스택의 이전 인코더에서 생성된 인코딩을 사용합니다.\n",
    "\n",
    "디코더 스택은 인코더가 전달한 인코딩을 처리하고 출력을 생성합니다. 각 디코더에는 셀프 어텐션 메커니즘, 어텐션 메커니즘 및 피드 포워드 신경망의 세 가지 구성 요소가 있습니다. 디코더는 인코더에서 생성된 인코딩에서 관련 정보의 가중치를 지정하는 데 도움이 되는 추가 어텐션 메커니즘을 제외하고 유사한 구조를 가지고 있습니다. 이 어텐션 메커니즘은 디코더가 LSTM에서 어텐션 메커니즘이 수행하는 것과 유사하게 입력 문장의 관련 부분에 집중하는 데 도움이 됩니다.\n",
    "\n",
    "디코더의 self-attention 레이어도 약간 다르게 작동합니다. self-attention 메커니즘은 출력 문장의 이전 위치에만 주의를 기울일 수 있습니다. 모든 미래 위치는 마스킹되어 변압기가 현재 또는 미래 출력을 사용하여 출력을 생성하지 않습니다.\n",
    "\n",
    "디코더 스택 후 출력 벡터는 최종 선형 변환 및 소프트맥스 레이어에 공급되어 어휘에 대한 출력 확률을 생성합니다. 확률이 가장 높은 어휘의 단어가 시간 단계의 최종 출력으로 생성됩니다.\n",
    "\n",
    "2017년 말에 도입된 후 Transformer는 많은 NLP 문제를 해결하기 위한 확실한 뛰어난 아키텍처가 되었습니다. 연구원들은 NLP의 ImageNet 모멘트의 해인 2018년에 폭발적인 활동을 시작으로 지난 3년 동안 이 분야를 극적으로 발전시키기 위해 그것을 사용했습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sudden-academy",
   "metadata": {},
   "source": [
    "### Transformer-XL"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a6893728",
   "metadata": {},
   "source": [
    "원래 2017 Transformer 아키텍처의 주요 제한 사항 중 하나는 Transformer에 입력으로 제공되는 텍스트 문자열에 대한 고정 길이 요구 사항이었습니다. 이러한 제한으로 인해 Transformer에 입력된 텍스트는 종종 단편화되었습니다. 예를 들어, 문장이 중간에서 분할되어 Transformer에 공급되었습니다. 즉, Transformer는 분할된 문장을 처리하는 데 부분적인 컨텍스트만 있음을 의미합니다. 문장 경계 또는 텍스트의 다른 의미론적 속성을 인식하지 못한 채 텍스트가 분할되어 트랜스포머의 성능이 제한되었습니다.\n",
    "\n",
    "이 단점을 해결하기 위해 2019년 연구원들은 Transformer의 어텐션 메커니즘에 공급되는 고정 길이 텍스트의 연속 세그먼트 간의 종속성을 학습하기 위해 반복 메커니즘을 도입한 Transformer-XL을 발명했습니다. 반복 메커니즘을 통해 Transformer는 Transformer가 이미 처리한 이전 세그먼트의 정보를 유지하면서 데이터의 장기 종속성에 액세스할 수 있습니다.\n",
    "\n",
    "Transformer-XL의 흥미로운 점은 딥 러닝의 두 가지 주요 개념인 반복과 주의를 결합한다는 것입니다. 주의 메커니즘(트랜스포머 형태)이 순환 신경망을 대체했지만, 트랜스포머에 반복이 추가되면서 반복이 다시 대중화되었습니다. 이것은 아이디어가 어떻게 인기를 얻고, 인기가 떨어지고, 더 일반적으로 NLP 및 기계 학습 분야를 발전시키기 위해 혁신적인 방법으로 재발견되고 재활용되는지를 강조합니다.\n",
    "\n",
    "이것은 단어 임베딩(Word2Vec 및 GloVe에서 fastText와 같은 서브워드 임베딩으로)과 모델링 아키텍처(바닐라 RNN, LSTM 및 GRU에서 어텐션 메커니즘 및 트랜스포머로)의 발전으로 NLP(소위 ImageNet 순간이라고 함)의 분기점에 도달했습니다. in 2018), 지금부터 살펴보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finnish-railway",
   "metadata": {},
   "source": [
    "## NLP’s ImageNet Moment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "12c18ff7",
   "metadata": {},
   "source": [
    "2017년 Transformer의 발명은 NLP의 ImageNet 순간으로 직접 이어졌습니다. 2018년에는 많은 데이터에 대해 훈련된 대규모의 사전 훈련된 언어 모델이 시장에 출시되었으며 이러한 모델은 다양한 NLP 작업에서 놀라운 성능을 보여주었습니다. 이전에는 모든 NLP 작업을 수행하기 위해 NLP 모델을 교육해야 했습니다. 한 번에 모든 작업을 잘 수행할 수 있는 단일 모델이 없었습니다.\n",
    "\n",
    "이것은 NLP의 분수령이었습니다. 이제 연구원과 응용 NLP 엔지니어는 Google 및 OpenAI와 같은 회사에서 출시한 사전 훈련된 대규모 언어 모델을 활용하여 모든 종류의 도메인별 NLP 작업에 적용할 수 있습니다. 즉, 미리 훈련된 대규모 언어 모델에서 전이 학습이 가능해졌고, 이로 인해 거의 동일한 방식으로 기업 내 활동이 급증했습니다. 그 컴퓨터 비전의 ImageNet 순간은 이미지 태깅, 물체 감지, 자율 주행 등과 관련된 상업적 활동의 폭발적인 증가로 이어졌습니다.\n",
    "\n",
    "수많은 릴리스가 이 순간을 가능하게 했고 NLP 모델의 최신 성능을 빠르게 발전시키는 데 도움이 되었습니다. 이러한 릴리스에는 ULMFiT, ELMo, BERT 및 GPT-1이 포함됩니다. 그 이후로 XLNet 및 RoBERTa와 같은 BERT 변형과 GPT-2 및 GPT-3과 같은 GPT 변형을 포함하여 Transformer 기반 모델의 수가 폭발적으로 증가했습니다.\n",
    "\n",
    "각 릴리스가 NLP 분야에 어떤 기여를 했는지 자세히 살펴보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "absolute-sphere",
   "metadata": {},
   "source": [
    "### ULMFiT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "58208e39",
   "metadata": {},
   "source": [
    "2018년 5월, ULMFiT(Universal Language Model Fine-Tuning)에 관한 Jeremy Howard와 Sebastian Ruder의 논문은 모델이 훈련된 원래 NLP 작업뿐만 아니라 사전 훈련된 언어에서도 가능하다는 것을 보여주었습니다. 이것은 Wikipeidia 기사와 같은 일반 도메인 말뭉치에서 언어 mdoel을 먼저 사전 훈련함으로써 가능합니다. 이 사전 훈련을 통해 언어 모델은 구문 및 의미 체계와 같은 언어의 주요 속성을 학습할 수 있습니다.\n",
    "\n",
    "사전 훈련이 완료되면 다음 단계는 특정 작업을 수행하도록 모델을 미세 조정하는 것입니다. 사전 훈련으로 인해 모델은 처음부터 당면한 특정 작업을 수행하도록 훈련된 경우보다 훨씬 빠르게 수렴할 수 있습니다. 특정 작업.\n",
    "\n",
    "> 언어 모델링은 일련의 단어에서 다음 단어를 예측하도록 모델을 훈련하는 NLP 작업입니다. 연구자들은 언어 모델링에 레이블이 필요하지 않기 때문에 모델을 사전 훈련하기 위해 이 NLP 작업을 선택했습니다. 감독되지 않은 NLP의 한 형태입니다. 레이블이 필요하지 않기 때문에 연구원은 방대한 양의 텍스트 데이터에 대해 언어 모델을 사전 훈련할 수 있으며, 이를 통해 모델은 언어의 속성을 매우 빠르고 강력하게 학습할 수 있습니다. 연구자가 모델을 사전 훈련하기 위해 기계 번역을 선택했다면 비용과 시간이 많이 드는 방대한 데이터 세트를 조립하고 주석을 달아야 했을 것입니다.\n",
    "\n",
    "즉, 사전 훈련된 ULMFiT 기반 모델은 처음부터 훈련해야 하는 모델에 비해 중소형 데이터 세트에서 매우 잘 수행됩니다. ULMFiT은 기업을 위한 NLP 애플리케이션과 많은 양의 데이터를 모으기 어려운 사용 사례를 열었습니다(모델이 훨씬 빠르게 훈련된다는 것은 말할 것도 없습니다)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rotary-compilation",
   "metadata": {},
   "source": [
    "### ELMo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4d72edce",
   "metadata": {},
   "source": [
    "ULMFiT의 뒤를 이어 AllenNLP는 문맥화된 단어 표현을 처음으로 도입한 ELMo를 출시했습니다. 이것은 Word2Vec, GloVec 및 fastText와 같은 이전 단어 임베딩 알고리즘에 의해 생성된 단어 임베딩을 개선했습니다. ELMo를 사용하면 \"bank\"와 같은 동일한 단어에 대해 문맥(금융 은행 대 강둑)에 따라 다른 단어 표현을 생성하는 것이 가능해졌습니다.\n",
    "\n",
    "또한 이러한 단어 표현은 fastText 단어 임베딩과 같은 문자 기반이므로 ELMo 기반 모델은 교육 중에 표시되지 않는 OOV 토큰을 처리할 수 있습니다. 당연하게도 ELMo의 문맥화된 단어 표현을 기존 NLP 시스템에 추가하면 모든 작업에 대한 최첨단 성능이 향상되었습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hairy-speed",
   "metadata": {},
   "source": [
    "### BERT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "91d071ee",
   "metadata": {},
   "source": [
    "ULMFiT(전이 학습) 및 ELMo(문맥화된 단어 표현)의 혁신은 2018년 Google이 많은 NLP 작업에서 성능 기록을 깨뜨린 대규모 사전 훈련된 언어 모델인 BERT의 오픈 소싱에서 NLP의 폭탄선언을 했습니다. BERT(Bidirectional Encoder Representaions from Transformers)는 NLP(Transformers + transfer learning + contextualized word representations)의 최근 여러 발전의 정점이었습니다. BERT는 다음과 같은 이유로 NLP 연구자에게 \"와우\" 순간이었습니다.\n",
    "\n",
    "- 많은 데이터에서 언어의 속성을 학습하면서 주석이 달린 수십억 개의 훈련 예제에 대해 사전 훈련되었습니다.\n",
    "- 특정 작업에 대한 최신 결과를 달성하기 위해 전 세계 누구라도 미세 조정할 수 있습니다.\n",
    "\n",
    "BERT는 semi-supervised pretraining(ULMFiT와 유사) 및 문맥화된 단어 표현(ELMo와 유사)을 활용했을 뿐만 아니라 문장의 일부 단어를 영리하게 마스킹한 다음 네트워크가 마스킹된 단어를 예측하도록 하여 네트워크에 양방향 구성 요소를 도입했습니다. 단어. BERT는 심층 신경망을 사용하여 이 마스킹된 양방향 사전 훈련을 수행할 수 있었고 문장 간의 관계를 모델링하는 방법도 배웠습니다.\n",
    "\n",
    "BERT는 NLP의 이러한 다양한 발전을 활용하여 사전 유지된 대규모 언어 모델을 개발하기 위한 표준 프로세스가 될 접근 방식을 개발했습니다. 2018년에 Google은 기본 모델과 대형 버전의 두 가지 버전을 출시했습니다. 대형 버전은 기본 모델과 비슷하지만 인코더 레이어가 더 많았습니다(대형 버전은 24개, 기본 모델은 12개). 그 이후로 여러 다른 회사에서 각각 이전 버전보다 더 큰 버전 또는 미리 훈련된 대규모 언어 모델을 출시했습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "breathing-avenue",
   "metadata": {},
   "source": [
    "### BERTology"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "faa313ca",
   "metadata": {},
   "source": [
    "Google의 BERT를 능가하는 최초의 BERT 유사 모델은 2019년 초에 출시된 XLNet이었습니다. XLNet은 BERT보다 더 많은 데이터에 대해 더 오랫동안 학습했지만 무작위 순서로 토큰을 예측하여 학습 방법론을 개선했습니다(BERT와 비교하여 마스킹된 토큰이 예측됨). XLNet은 또한 이전에 참조한 Transformer-XL 아키텍처(기본적으로 반복되는 원래 Transformer 아키텍처임)를 활용했습니다.\n",
    "\n",
    "XLNet이 나온 지 몇 달 후, Google이 BERT를 출시한 지 몇 달 후, Facebook은 아키텍처 미래를 최적화하고 RoBERTa라는 자체 BERT 기반 버전을 출시했습니다. 특히 Facebook은 BERT에서 키 하이퍼파라미터를 수정했습니다. BERT의 다음 문장 사전 교육을 제거하고 훨씬 더 많은 시간으로 모델을 교육했습니다. 설계 변경으로 RoBERTa는 BERT가 기록을 세웠던 많은 작업에서 최첨단 성능을 달성했습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "measured-feedback",
   "metadata": {},
   "source": [
    "### GPT-1, GPT-2, GPT-3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "74589d21",
   "metadata": {},
   "source": [
    "OpenAI는 또한 자체 Transformer 기반 모델을 설계하여 NLP 경쟁에 뛰어들었습니다. 이러한 모델은 Generative Pretrained Transformer의 줄임말인 GPT 모델로 알려져 있습니다. 첫 번째 GPT 모델인 GPT-1은 2018년에 출시되었으며 ULMFiT과 유사한 감독되지 않은 사전 훈련 및 감독된 미세 조정 프로세스를 사용했습니다. GPT-2는 2019년에 출시되었습니다. 이전 버전에 비해 더 많은 데이터와 더 많은 매개변수로 훈련하여 제로 샷 설정에서 많은 작업에서 최첨단 성능을 달성하는 데 도움이 되었습니다.\n",
    "\n",
    "> 제로 샷 학습에서 모델은 학습할 예제가 제공되지 않지만 주어진 지침에 따라 수행할 작업을 이해해야 합니다. 예를 들어 제로 샷 학습 작업에는 영어 문장을 독일어로 번역하는 모델이 필요할 수 있지만 모델에는 학습할 영어-독일어 문장이 제공되지 않습니다. 퓨샷 학습에서는 모델에 학습할 몇 가지 예가 제공되지만 일반적으로 많지는 않습니다.\n",
    "\n",
    "OpenAI는 2020년에 GPT-3를 출시했습니다. GPT-2와 비교할 때 GPT-3는 훨씬 더 큰 데이터 세트에서 학습했으며 더 많은 수의 매개 변수를 가졌습니다. GPT-3는 이전 모델을 능가하고 zero-shot 및 few-shot 학습의 새로운 표준을 세웠습니다. 현재까지 가장 성능이 좋은 생성적인 NLP 모델로 간주됩니다.\n",
    "\n",
    "> 아시다시피 모델은 점점 더 커지고 수년에 걸쳐 점점 더 많은 데이터에 대해 학습했습니다. 이것은 설계 변경, 더 큰 모델 및 더 많은 데이터와 함께 NLP에서 최첨단 성능을 추진하는 데 도움이 되었습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fresh-running",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e8b850e1",
   "metadata": {},
   "source": [
    "이 장에서는 단어 임베딩, RNN, 어텐션 메커니즘, Transformer 아키텍처 및 문맥화된 단어 표현을 포함하여 책의 모든 주요 개념을 함께 묶었습니다. 종합적으로 이러한 발전은 2018년에 NLP의 ImageNet 순간을 가져오는 데 도움이 되었으며, 사전 훈련된 대규모 언어 모델이 대중에게 공개되고 NLP 벤치마크에서 새로운 성능 기록을 세웠습니다.\n",
    "\n",
    "사전 훈련된 언어 모델의 등장으로 응용 NLP 엔지니어는 도메인별 NLP 작업에서 대규모 모델을 미세 조정하고 놀라운 성능을 달성할 수 있게 되었습니다. 이제 NLP 모델을 개발하기 위해 알아야 할 주요 NLP 개념을 다루었으므로 NLP 모델을 개발한 후 프로덕션화하는 방법에 대해 논의해 보겠습니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
