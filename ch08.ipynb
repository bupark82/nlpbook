{
 "cells": [
  {
   "cell_type": "raw",
   "id": "rational-truck",
   "metadata": {},
   "source": [
    "[[ch08]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minute-saint",
   "metadata": {},
   "source": [
    "# BERTology: Putting it all Together"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ce66001a",
   "metadata": {},
   "source": [
    "Together, we've come a long way since we started with fiddling with `spacy` in Chapter 1. We started with solving the most common NLP problems using the microwave-meal equivalent of deep learning libraries, and then we proceeded to the low-level details, including tokenization and embeddings. Along the way, we covered reccurent networks, including RNNs, LSTMs, and GRUs, as well as the Transformer architecture and attention mechanisms.\n",
    "\n",
    "This chapter, in many ways, is the grand finale. We will tie all the pieces together and trace back the steps that led to the so-called ImageNet moment in 2018, which has since led to a flurry of exitement regarding the potential commercial applications of these advances in NLP. We will touch on some of these possibilities, too. Let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advance-western",
   "metadata": {},
   "source": [
    "## ImageNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "delayed-friendly",
   "metadata": {},
   "source": [
    "### The Power of Pretrained Models "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nominated-league",
   "metadata": {},
   "source": [
    "## The Path to NLP’s ImageNet Moment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alpha-armstrong",
   "metadata": {},
   "source": [
    "## Pretrained Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unique-primary",
   "metadata": {},
   "source": [
    "### The Limitations of One-Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "familiar-indie",
   "metadata": {},
   "source": [
    "### word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simplified-amber",
   "metadata": {},
   "source": [
    "### GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cooked-syndrome",
   "metadata": {},
   "source": [
    "### fastText"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocal-phase",
   "metadata": {},
   "source": [
    "### Context-aware Pretrained Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demanding-reverse",
   "metadata": {},
   "source": [
    "## Sequential Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minor-holly",
   "metadata": {},
   "source": [
    "### Sequential Data and the Importance of Sequential Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distinguished-favorite",
   "metadata": {},
   "source": [
    "## RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experienced-remains",
   "metadata": {},
   "source": [
    "### Vanilla RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "permanent-divorce",
   "metadata": {},
   "source": [
    "### LSTMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mechanical-inventory",
   "metadata": {},
   "source": [
    "### GRUs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exclusive-manner",
   "metadata": {},
   "source": [
    "## Attention Mechanisms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sensitive-deadline",
   "metadata": {},
   "source": [
    "## Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sudden-academy",
   "metadata": {},
   "source": [
    "### Transformer-XL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finnish-railway",
   "metadata": {},
   "source": [
    "## NLP’s ImageNet Moment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "absolute-sphere",
   "metadata": {},
   "source": [
    "### ULMFiT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rotary-compilation",
   "metadata": {},
   "source": [
    "### ELMo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hairy-speed",
   "metadata": {},
   "source": [
    "### BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "breathing-avenue",
   "metadata": {},
   "source": [
    "### BERTology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "measured-feedback",
   "metadata": {},
   "source": [
    "### GPT-1, GPT-2, GPT-3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "74589d21",
   "metadata": {},
   "source": [
    "OpenAI also entered the NLP race by desining its own Transformer-based models. These models are known as GPT models, short for Generative Pretrained Transformer. The first of the GPT models, GPT-1, was released in 2018 and used an unsupervised pretraining and supervised fine-tuning process similar to ULMFiT. GPT-2 was released in 2019; compared to its predecessor, it trained on more data and with more parameters, helping it achieve state-of-the-art performance on many tasks in zero-shot settings.\n",
    "\n",
    "> In zero-shot learning, a model is given no examples to train on but must understand the task to perform given the instruction provided. For example, a zero-shot learning task may require a model to translate an English sentence into German, but the model would not be given English-to-German sentences to learn from. In few-shot learning, a model is given a few examples to learn from, but usually not many.\n",
    "\n",
    "OpenAI released GPT-3 in 2020. Compared to GPT-2, GPT-3 trained on an even larger dataset and had an even larger number of parmeters. GPT-3 bested its predecessor and set a new standard for zero-shot and few-shot learning. It is considered to be the most performance-generative NLP model to date.\n",
    "\n",
    "> As you may have noticed, models have gotten bigger and bigger and have trained on more and more data over the years. This, along with design changes, larger models, and more data, has helped push the state-of-the-art performance in NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fresh-running",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e8b850e1",
   "metadata": {},
   "source": [
    "In this chapter, we tied all the major concepts in the book together, including word embeddings, RNNs, attention mechanisms, the Transformer architecture, and contextualized word representations. Collectively these advances helped bring about NLP's ImageNet moment in 2018, the year when large, pretrained language models became available to the public and set new performance records on NLP benchmarks.\n",
    "\n",
    "With the rise of the pretrained language models, it has become possible for applied NLP engineers to fine-tune large models on their domain-specific NLP tasks and achieve remarkable performance. Now that we've covered the major NLP concepts you need to know to develop NLP models, let's discuss how to productionize NLP models once you've developed them."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
